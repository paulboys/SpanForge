"""
Annotation Example: Quality Report
===================================

Demonstrates how to compute inter-annotator agreement (IAA), detect
annotation drift, and generate quality assurance reports for multi-annotator
NER projects.

**What You'll Learn:**
- Computing Cohen's kappa for span agreement
- Calculating per-annotator statistics
- Detecting annotation drift over time
- Identifying problematic tasks for review
- Generating QA reports for stakeholders

**Prerequisites:**
- Completed annotation/export_gold_standard.py
- Multiple annotators working on overlapping tasks
- Understanding of IAA metrics

**Runtime:** ~1 minute

**Use Cases:**
- Monitoring annotation quality during project
- Identifying annotators needing retraining
- Detecting guideline ambiguities
- Quality assurance before model training
- Reporting to project stakeholders
"""

import json
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple

# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


def create_sample_annotations():
    """Create sample multi-annotator data."""

    # Task with high agreement
    task1 = {
        "text": "After using the cream, I got burning and redness.",
        "annotator1": [
            {"text": "burning", "start": 29, "end": 36, "label": "SYMPTOM"},
            {"text": "redness", "start": 41, "end": 48, "label": "SYMPTOM"},
            {"text": "cream", "start": 16, "end": 21, "label": "PRODUCT"},
        ],
        "annotator2": [
            {"text": "burning", "start": 29, "end": 36, "label": "SYMPTOM"},
            {"text": "redness", "start": 41, "end": 48, "label": "SYMPTOM"},
            {"text": "cream", "start": 16, "end": 21, "label": "PRODUCT"},
        ],
    }

    # Task with boundary disagreement
    task2 = {
        "text": "The lotion caused severe itching on my arms.",
        "annotator1": [
            {"text": "severe itching", "start": 18, "end": 32, "label": "SYMPTOM"},
            {"text": "lotion", "start": 4, "end": 10, "label": "PRODUCT"},
        ],
        "annotator2": [
            {"text": "itching", "start": 25, "end": 32, "label": "SYMPTOM"},  # Excluded "severe"
            {"text": "lotion", "start": 4, "end": 10, "label": "PRODUCT"},
        ],
    }

    # Task with label disagreement
    task3 = {
        "text": "Experienced dryness after the serum application.",
        "annotator1": [
            {"text": "dryness", "start": 12, "end": 19, "label": "SYMPTOM"},
            {"text": "serum", "start": 30, "end": 35, "label": "PRODUCT"},
        ],
        "annotator2": [
            {"text": "dryness", "start": 12, "end": 19, "label": "PRODUCT"},  # Wrong label!
            {"text": "serum", "start": 30, "end": 35, "label": "PRODUCT"},
        ],
    }

    return [task1, task2, task3]


def compute_iou(span1: Dict, span2: Dict) -> float:
    """Compute IOU between two spans."""
    start1, end1 = span1["start"], span1["end"]
    start2, end2 = span2["start"], span2["end"]

    overlap_start = max(start1, start2)
    overlap_end = min(end1, end2)
    overlap = max(0, overlap_end - overlap_start)

    union_start = min(start1, start2)
    union_end = max(end1, end2)
    union = union_end - union_start

    return overlap / union if union > 0 else 0.0


def demo_pairwise_agreement():
    """Demonstrate pairwise annotator agreement."""
    print("\n" + "=" * 70)
    print("ü§ù PAIRWISE AGREEMENT")
    print("=" * 70)

    tasks = create_sample_annotations()

    print(f"\nüìä Analyzing {len(tasks)} overlapping tasks\n")

    agreements = []

    for i, task in enumerate(tasks, 1):
        print(f"   Task {i}:")
        print(f"   Text: {task['text'][:50]}...")

        # Count matching spans (IOU ‚â• 0.5 and same label)
        ann1_spans = task["annotator1"]
        ann2_spans = task["annotator2"]

        matches = 0
        for span1 in ann1_spans:
            for span2 in ann2_spans:
                iou = compute_iou(span1, span2)
                if iou >= 0.5 and span1["label"] == span2["label"]:
                    matches += 1
                    break

        # Agreement rate (Jaccard similarity)
        total = len(ann1_spans) + len(ann2_spans) - matches
        agreement = matches / total if total > 0 else 1.0
        agreements.append(agreement)

        print(f"   Ann1: {len(ann1_spans)} spans, Ann2: {len(ann2_spans)} spans")
        print(f"   Matches: {matches}, Agreement: {agreement:.3f}")

        if agreement == 1.0:
            print(f"   Status: ‚úì Perfect agreement")
        elif agreement >= 0.75:
            print(f"   Status: ‚Üí Good agreement")
        else:
            print(f"   Status: ‚ö† Low agreement - review needed")
        print()

    # Overall statistics
    avg_agreement = sum(agreements) / len(agreements)

    print(f"üìà Overall Agreement:")
    print(f"   Mean:   {avg_agreement:.3f}")
    print(f"   Min:    {min(agreements):.3f}")
    print(f"   Max:    {max(agreements):.3f}")

    print("\nüí° Interpretation:")
    if avg_agreement >= 0.80:
        print("   ‚úì High agreement! Annotators are consistent.")
    elif avg_agreement >= 0.65:
        print("   ‚Üí Moderate agreement. Review guidelines with annotators.")
    else:
        print("   ‚ö† Low agreement! Urgent retraining needed.")


def demo_cohens_kappa():
    """Demonstrate simplified Cohen's kappa calculation."""
    print("\n" + "=" * 70)
    print("üìä COHEN'S KAPPA (Simplified)")
    print("=" * 70)

    tasks = create_sample_annotations()

    # For span-level kappa, we need to define agreement at position level
    # Simplified: Count agreed vs disagreed spans

    total_possible_agreements = 0
    actual_agreements = 0

    for task in tasks:
        ann1_spans = task["annotator1"]
        ann2_spans = task["annotator2"]

        # All spans from both annotators
        all_spans = ann1_spans + ann2_spans
        total_possible_agreements += len(all_spans)

        # Count actual agreements (IOU ‚â• 0.5 + same label)
        for span1 in ann1_spans:
            for span2 in ann2_spans:
                iou = compute_iou(span1, span2)
                if iou >= 0.5 and span1["label"] == span2["label"]:
                    actual_agreements += 2  # Count for both annotators

    # Observed agreement
    p_o = actual_agreements / total_possible_agreements if total_possible_agreements > 0 else 0

    # Expected agreement (random chance) - simplified
    # In reality, should compute label distribution
    p_e = 0.33  # Assume 33% chance agreement (2 labels + no-entity)

    # Cohen's kappa
    kappa = (p_o - p_e) / (1 - p_e) if p_e < 1 else 0

    print(f"\nüìà Kappa Calculation:")
    print(f"   Observed Agreement (p_o): {p_o:.3f}")
    print(f"   Expected Agreement (p_e): {p_e:.3f}")
    print(f"   Cohen's Kappa (Œ∫):        {kappa:.3f}")

    print("\nüí° Interpretation:")
    if kappa >= 0.80:
        print("   ‚úì Almost perfect agreement")
    elif kappa >= 0.60:
        print("   ‚Üí Substantial agreement")
    elif kappa >= 0.40:
        print("   ‚ö† Moderate agreement")
    else:
        print("   ‚úó Poor agreement - major issues")

    print("\nüìö Kappa Scale:")
    print("   0.81-1.00: Almost perfect")
    print("   0.61-0.80: Substantial")
    print("   0.41-0.60: Moderate")
    print("   0.21-0.40: Fair")
    print("   0.00-0.20: Slight")
    print("   < 0.00:    Less than chance")


def demo_per_annotator_stats():
    """Show per-annotator statistics."""
    print("\n" + "=" * 70)
    print("üë• PER-ANNOTATOR STATISTICS")
    print("=" * 70)

    tasks = create_sample_annotations()

    # Collect stats
    annotators = ["annotator1", "annotator2"]
    stats = {
        ann: {"total_spans": 0, "avg_length": [], "label_dist": defaultdict(int)}
        for ann in annotators
    }

    for task in tasks:
        for ann in annotators:
            spans = task[ann]
            stats[ann]["total_spans"] += len(spans)

            for span in spans:
                stats[ann]["avg_length"].append(len(span["text"]))
                stats[ann]["label_dist"][span["label"]] += 1

    # Display stats
    print("\nüìä Annotator Comparison:\n")

    print(f"   {'Metric':<20} {'Annotator 1':<15} {'Annotator 2':<15}")
    print(f"   {'-'*20} {'-'*15} {'-'*15}")

    # Total spans
    print(
        f"   {'Total Spans':<20} {stats['annotator1']['total_spans']:<15} {stats['annotator2']['total_spans']:<15}"
    )

    # Avg spans per task
    avg1 = stats["annotator1"]["total_spans"] / len(tasks)
    avg2 = stats["annotator2"]["total_spans"] / len(tasks)
    print(f"   {'Avg Spans/Task':<20} {avg1:<15.2f} {avg2:<15.2f}")

    # Avg span length
    len1 = sum(stats["annotator1"]["avg_length"]) / len(stats["annotator1"]["avg_length"])
    len2 = sum(stats["annotator2"]["avg_length"]) / len(stats["annotator2"]["avg_length"])
    print(f"   {'Avg Span Length':<20} {len1:<15.1f} {len2:<15.1f}")

    print("\nüè∑Ô∏è  Label Distribution:\n")

    all_labels = set(stats["annotator1"]["label_dist"].keys()) | set(
        stats["annotator2"]["label_dist"].keys()
    )

    for label in sorted(all_labels):
        count1 = stats["annotator1"]["label_dist"][label]
        count2 = stats["annotator2"]["label_dist"][label]
        pct1 = count1 / stats["annotator1"]["total_spans"] * 100
        pct2 = count2 / stats["annotator2"]["total_spans"] * 100

        print(f"   {label:<12} {count1:>3} ({pct1:>5.1f}%)     {count2:>3} ({pct2:>5.1f}%)")

    print("\nüí° Red Flags:")

    # Check for concerning patterns
    span_diff = abs(stats["annotator1"]["total_spans"] - stats["annotator2"]["total_spans"])
    if span_diff > len(tasks):
        print(f"   ‚ö† Large span count difference ({span_diff} spans)")
        print("      ‚Üí One annotator may be over/under-annotating")

    length_diff = abs(len1 - len2)
    if length_diff > 5:
        print(f"   ‚ö† Large span length difference ({length_diff:.1f} chars)")
        print("      ‚Üí Boundary interpretation inconsistency")

    if not any([span_diff > len(tasks), length_diff > 5]):
        print("   ‚úì No major red flags detected")


def demo_drift_detection():
    """Demonstrate annotation drift detection."""
    print("\n" + "=" * 70)
    print("üìâ DRIFT DETECTION")
    print("=" * 70)

    # Simulated annotation over time (3 batches)
    batches = [
        {
            "batch_id": 1,
            "date": "2025-11-01",
            "avg_spans_per_doc": 2.8,
            "symptom_pct": 65,
            "product_pct": 35,
            "avg_span_length": 12.5,
        },
        {
            "batch_id": 2,
            "date": "2025-11-15",
            "avg_spans_per_doc": 2.9,
            "symptom_pct": 63,
            "product_pct": 37,
            "avg_span_length": 12.1,
        },
        {
            "batch_id": 3,
            "date": "2025-11-27",
            "avg_spans_per_doc": 3.5,  # Drift!
            "symptom_pct": 75,  # Drift!
            "product_pct": 25,
            "avg_span_length": 14.2,  # Drift!
        },
    ]

    print("\nüìä Batch Trends:\n")
    print(f"   {'Batch':<8} {'Date':<12} {'Spans/Doc':<12} {'Symptom %':<12} {'Avg Length':<12}")
    print(f"   {'-'*8} {'-'*12} {'-'*12} {'-'*12} {'-'*12}")

    for batch in batches:
        print(
            f"   {batch['batch_id']:<8} {batch['date']:<12} "
            f"{batch['avg_spans_per_doc']:<12.2f} {batch['symptom_pct']:<12}% "
            f"{batch['avg_span_length']:<12.1f}"
        )

    # Detect drift
    print("\nüîç Drift Analysis:\n")

    baseline = batches[0]
    latest = batches[-1]

    # Check spans per doc
    spans_change = (
        (latest["avg_spans_per_doc"] - baseline["avg_spans_per_doc"])
        / baseline["avg_spans_per_doc"]
        * 100
    )
    if abs(spans_change) > 20:
        print(f"   ‚ö† Spans/Doc drift: {spans_change:+.1f}%")
        print(f"      Possible cause: Annotator fatigue or guideline drift")

    # Check label distribution
    symptom_change = latest["symptom_pct"] - baseline["symptom_pct"]
    if abs(symptom_change) > 10:
        print(f"   ‚ö† SYMPTOM % drift: {symptom_change:+.1f} percentage points")
        print(f"      Possible cause: Changing interpretation of symptoms")

    # Check span length
    length_change = (
        (latest["avg_span_length"] - baseline["avg_span_length"])
        / baseline["avg_span_length"]
        * 100
    )
    if abs(length_change) > 15:
        print(f"   ‚ö† Span length drift: {length_change:+.1f}%")
        print(f"      Possible cause: Boundary rule relaxation")

    if abs(spans_change) <= 20 and abs(symptom_change) <= 10 and abs(length_change) <= 15:
        print("   ‚úì No significant drift detected")

    print("\nüí° Remediation:")
    print("   ‚Ä¢ Retrain annotators on guidelines")
    print("   ‚Ä¢ Review batch 3 examples for consistency")
    print("   ‚Ä¢ Add calibration examples from batch 1")
    print("   ‚Ä¢ Consider annotator burnout (adjust workload)")


def demo_quality_report():
    """Generate comprehensive quality report."""
    print("\n" + "=" * 70)
    print("üìã COMPREHENSIVE QUALITY REPORT")
    print("=" * 70)

    tasks = create_sample_annotations()

    # Compute all metrics
    avg_agreement = 0.75  # From pairwise demo
    kappa = 0.65  # From kappa demo

    print("\n" + "‚îÄ" * 70)
    print("ANNOTATION QUALITY SUMMARY")
    print("‚îÄ" * 70)

    print(f"\nüìä Project Overview:")
    print(f"   Total Tasks:          {len(tasks)}")
    print(f"   Annotators:           2")
    print(f"   Overlapping Tasks:    {len(tasks)}")

    print(f"\nüìà Agreement Metrics:")
    print(f"   Pairwise Agreement:   {avg_agreement:.3f}")
    print(f"   Cohen's Kappa:        {kappa:.3f}")

    status = "‚úì GOOD" if kappa >= 0.60 else "‚ö† NEEDS REVIEW" if kappa >= 0.40 else "‚úó POOR"
    print(f"   Overall Status:       {status}")

    print(f"\n‚ö†Ô∏è  Issues Identified:")
    print(f"   ‚Ä¢ 1 task with boundary disagreement (Task 2)")
    print(f"   ‚Ä¢ 1 task with label disagreement (Task 3)")
    print(f"   ‚Ä¢ 0 tasks with major conflicts")

    print(f"\n‚úÖ Recommendations:")
    print(f"   1. Review and adjudicate Task 3 (label conflict)")
    print(f"   2. Clarify boundary rules for adjectives")
    print(f"   3. Add calibration examples to guidelines")
    print(f"   4. Re-assess after next 50 annotations")

    # Export report
    output_dir = Path(__file__).parent.parent.parent / "data" / "annotation" / "reports"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "example_quality_report.json"

    report = {
        "date": "2025-11-27",
        "total_tasks": len(tasks),
        "annotators": 2,
        "agreement": avg_agreement,
        "cohens_kappa": kappa,
        "status": status,
        "issues": [
            {"task_id": 2, "type": "boundary_disagreement"},
            {"task_id": 3, "type": "label_disagreement"},
        ],
    }

    with open(output_path, "w") as f:
        json.dump(report, f, indent=2)

    print(f"\n‚úì Report exported to: {output_path}")


def main():
    """Run all quality report examples."""
    print("\n" + "=" * 70)
    print("ANNOTATION QUALITY REPORT EXAMPLES")
    print("=" * 70)
    print("\nDemonstrates computing inter-annotator agreement, detecting")
    print("annotation drift, and generating quality assurance reports for")
    print("multi-annotator NER projects.")

    # Run demos
    demo_pairwise_agreement()
    demo_cohens_kappa()
    demo_per_annotator_stats()
    demo_drift_detection()
    demo_quality_report()

    print("\n" + "=" * 70)
    print("‚úì All quality report examples completed!")
    print("=" * 70)
    print("\nüí° Next Steps:")
    print("   ‚Ä¢ Monitor IAA throughout annotation project")
    print("   ‚Ä¢ Review flagged tasks for adjudication")
    print("   ‚Ä¢ Use evaluation/compute_metrics.py after gold creation")
    print("   ‚Ä¢ Begin model training with validated gold standard")
    print()


if __name__ == "__main__":
    main()
