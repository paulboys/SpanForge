{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SpanForge","text":"<p>Biomedical Named Entity Recognition with BioBERT and Weak Labeling</p> <p> </p> <p>SpanForge is a production-ready biomedical NER pipeline combining BioBERT contextual embeddings with lexicon-driven weak labeling for adverse event detection in product complaints.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd2c BioBERT Integration: State-of-the-art biomedical language model</li> <li>\ud83d\udcdd Weak Labeling: Fuzzy + exact matching with confidence scoring</li> <li>\ud83d\udeab Negation Detection: Bidirectional window with 10+ clinical cues</li> <li>\ud83c\udfaf High Accuracy: 98% precision on symptom detection</li> <li>\u26a1 Fast Processing: &lt;100ms per document average</li> <li>\ud83e\uddea Well-Tested: 144 tests with 100% pass rate</li> <li>\ud83d\udd04 CI/CD Ready: GitHub Actions with multi-OS/Python matrix</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/paulboys/SpanForge.git\ncd SpanForge\npip install -r requirements.txt\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from src.weak_label import load_symptom_lexicon, load_product_lexicon, weak_label\nfrom pathlib import Path\n\n# Load lexicons\nsymptom_lex = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\"))\nproduct_lex = load_product_lexicon(Path(\"data/lexicon/products.csv\"))\n\n# Detect entities\ntext = \"Patient developed severe rash after using the hydra boost cream\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text} ({span.label}): {span.canonical} [conf={span.confidence:.2f}]\")\n# Output:\n# severe rash (SYMPTOM): Rash [conf=1.00]\n# hydra boost cream (PRODUCT): Hydra Boost Cream [conf=1.00]\n</code></pre>"},{"location":"#pipeline-inference","title":"Pipeline Inference","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\n    \"No irritation from the face wash, just mild dryness\",\n    \"The moisturizer caused redness and itching\"\n]\n\nresults = simple_inference(texts, persist_path=\"output.jsonl\")\n\nfor result in results:\n    print(f\"Found {len(result['weak_spans'])} entities\")\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Input Text] --&gt; B[BioBERT Tokenizer]\n    B --&gt; C[Lexicon Matcher]\n    C --&gt; D[Negation Detector]\n    D --&gt; E[Confidence Scorer]\n    E --&gt; F[Span Deduplicator]\n    F --&gt; G[Output JSONL]</code></pre>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#1-weak-labeling","title":"1. Weak Labeling","text":"<ul> <li>Fuzzy Matching: WRatio \u226588 with Jaccard token-set \u226540</li> <li>Exact Matching: Case-insensitive with word boundaries</li> <li>Confidence Formula: <code>0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score</code></li> </ul>"},{"location":"#2-negation-detection","title":"2. Negation Detection","text":"<ul> <li>Bidirectional Windows: Forward (\"no itching\") + backward (\"itching absent\")</li> <li>Extended Cues: Clinical terms (absent, denies, negative) + resolution indicators (cleared, improved)</li> <li>Prefix Matching: Handles variants (resolved \u2192 resolv)</li> </ul>"},{"location":"#3-span-processing","title":"3. Span Processing","text":"<ul> <li>Overlap Resolution: Exact duplicate removal, contextual mention preservation</li> <li>Anatomy Gating: Skips generic single-token anatomy terms</li> <li>Last-Token Alignment: Multi-token fuzzy matches require matching final token</li> </ul>"},{"location":"#performance","title":"Performance","text":"Metric Value Precision 98% Recall 92% F1 Score 95% Avg. Time/Doc 85ms 1000-Doc Batch &lt;2 min"},{"location":"#testing","title":"Testing","text":"<pre><code># Full suite (144 tests)\npytest tests/ -v\n\n# With coverage\npytest tests/ --cov=src --cov-report=html\n\n# Specific categories\npytest tests/edge_cases/ -v      # 98 edge cases\npytest tests/integration/ -v     # 26 integration tests\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation available at: SpanForge Docs</p> <ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Contributing Guide</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>SpanForge/\n\u251c\u2500\u2500 src/               # Core source code\n\u2502   \u251c\u2500\u2500 config.py      # Configuration management\n\u2502   \u251c\u2500\u2500 model.py       # BioBERT loading\n\u2502   \u251c\u2500\u2500 weak_label.py  # Weak labeling logic\n\u2502   \u251c\u2500\u2500 pipeline.py    # End-to-end pipeline\n\u2502   \u2514\u2500\u2500 llm_agent.py   # LLM refinement (experimental)\n\u251c\u2500\u2500 tests/             # Test suite (144 tests)\n\u2502   \u251c\u2500\u2500 edge_cases/    # 98 parametrized edge cases\n\u2502   \u251c\u2500\u2500 integration/   # 26 integration tests\n\u2502   \u2514\u2500\u2500 assertions.py  # Test composition helpers\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 lexicon/       # Symptom &amp; product lexicons\n\u2502   \u2514\u2500\u2500 output/        # Pipeline outputs\n\u251c\u2500\u2500 scripts/           # Utility scripts\n\u251c\u2500\u2500 docs/              # MkDocs documentation\n\u2514\u2500\u2500 .github/           # CI/CD workflows\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Phase 1: Bootstrap &amp; Lexicon</li> <li> Phase 2: Weak Label Refinement</li> <li> Phase 3: Test Infrastructure &amp; Edge Cases</li> <li> Phase 4: CI/CD Integration</li> <li> Phase 5: Annotation &amp; Curation (Label Studio)</li> <li> Phase 6: Gold Standard Assembly</li> <li> Phase 7: Token Classification Fine-Tuning</li> <li> Phase 8: Domain Adaptation (MLM)</li> <li> Phase 9: Baseline Comparison (RoBERTa)</li> <li> Phase 10: Evaluation &amp; Calibration</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions welcome! Please see Contributing Guide for guidelines.</p> <ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> <li>Commit changes (<code>git commit -m 'Add amazing feature'</code>)</li> <li>Push to branch (<code>git push origin feature/amazing-feature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"#license","title":"License","text":"<p>MIT License - see License for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use SpanForge in your research, please cite:</p> <pre><code>@software{spanforge2025,\n  title = {SpanForge: Biomedical NER with BioBERT and Weak Labeling},\n  author = {SpanForge Contributors},\n  year = {2025},\n  url = {https://github.com/paulboys/SpanForge}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>BioBERT: Lee et al., \"BioBERT: a pre-trained biomedical language representation model\"</li> <li>Hugging Face Transformers: For model infrastructure</li> <li>RapidFuzz: For high-performance fuzzy matching</li> </ul> <p>Status: Production Ready | Version: 0.1.0 | Last Updated: November 2025</p>"},{"location":"annotation_guide/","title":"Annotation guide","text":"SpanForge Annotation Guide <p>Standards for consistent SYMPTOM and PRODUCT span curation enabling high-fidelity adverse event modeling.</p>"},{"location":"annotation_guide/#annotation-guide","title":"Annotation Guide","text":"<p>Defines consistent rules for annotating SYMPTOM and PRODUCT spans in consumer adverse event complaints.</p>"},{"location":"annotation_guide/#objectives","title":"Objectives","text":"<ol> <li>Capture clinically meaningful symptom phrases (granular, complete).</li> <li>Standardize product references for model association.</li> <li>Preserve negated context (annotate + flag) for future modeling of absence.</li> <li>Minimize ambiguity and overlapping conflicts.</li> </ol>"},{"location":"annotation_guide/#labels","title":"Labels","text":"<ul> <li>SYMPTOM: Physiological or subjective adverse effect (\"redness\", \"severe rash\", \"nausea\", \"stinging\").</li> <li>PRODUCT: Product name/formulation or clear category reference (\"vitamin serum\", \"exfoliating scrub\").</li> </ul>"},{"location":"annotation_guide/#span-boundary-rules","title":"Span Boundary Rules","text":"Rule Examples Include modifiers integral to meaning <code>severe rash</code>, <code>mild dryness</code>, <code>burning pain</code> Exclude trailing punctuation <code>itching.</code> \u2192 <code>itching</code> Avoid partial capture Prefer <code>tiny itching spots</code> over <code>itching</code> alone Keep internal spacing &amp; casing as-is <code>hydra boost cream</code> preserved Exclude unrelated conjunctions <code>rash and</code> \u2192 <code>rash</code>"},{"location":"annotation_guide/#negation-handling-annotate-flag","title":"Negation Handling (Annotate + Flag)","text":"<p>Annotate negated symptoms (\"no irritation\", \"without redness\") and rely on conversion phase to set <code>negated=True</code>. This supports training for presence vs absence.</p> <p>Do NOT annotate if term clearly unrelated to adverse context (e.g., \"no product issues\" \u2192 skip <code>issues</code>).</p>"},{"location":"annotation_guide/#anatomy-tokens","title":"Anatomy Tokens","text":"<p>Skip isolated anatomy (<code>face</code>, <code>skin</code>) unless part of explicit symptom phrase (\"skin irritation\" \u2192 annotate <code>skin irritation</code>).</p>"},{"location":"annotation_guide/#overlaps-nested-spans","title":"Overlaps &amp; Nested Spans","text":"<ul> <li>Choose the most semantically complete span (<code>severe burning pain</code> preferred).</li> <li>If two plausible alternatives and uncertainty persists: keep both \u2192 adjudication tool resolves.</li> </ul>"},{"location":"annotation_guide/#product-vs-symptom-separation","title":"Product vs Symptom Separation","text":"<p>If a product term appears inside a symptom phrase but functions as a product reference, separate spans where boundaries are clean: <code>serum-induced itching</code> \u2192 <code>serum</code> (PRODUCT), <code>itching</code> (SYMPTOM).</p>"},{"location":"annotation_guide/#canonical-mapping","title":"Canonical Mapping","text":"<p>Surface span text is later mapped to canonical lexicon entries; missing variants should be added to lexicon CSVs. Do not force canonical wording during annotation\u2014capture verbatim text.</p>"},{"location":"annotation_guide/#conflict-resolution-planned-consensus","title":"Conflict Resolution (Planned Consensus)","text":"<ol> <li>Exact match majority for identical spans.</li> <li>Longest span tie-breaker when semantics equivalent.</li> <li>Differing labels on overlap \u2192 adjudication review output to <code>data/annotation/conflicts/</code>.</li> </ol>"},{"location":"annotation_guide/#provenance-fields-after-conversion","title":"Provenance Fields (After Conversion)","text":"<p><code>source</code>, <code>annotator</code>, <code>revision</code>, <code>canonical</code>, optional <code>concept_id</code> automatically injected\u2014no manual action required in UI.</p>"},{"location":"annotation_guide/#quality-checklist-before-export","title":"Quality Checklist Before Export","text":"<ul> <li>Boundaries precise (no punctuation, correct modifiers).</li> <li>Negated spans present (not deleted unless irrelevant context).</li> <li>No duplicate (start,end,label) tuples.</li> <li>Low conflict count (&lt;5% tasks flagged).</li> </ul>"},{"location":"annotation_guide/#edge-case-decisions","title":"Edge Case Decisions","text":"Scenario Action \"dry\" vs \"dryness\" Annotate verbatim form present Misspelling (\"nausia\") Annotate misspelling; canonical normalizes Compound (\"rash and itching\") Two spans if distinct sensations Intensifier only (\"very\") Exclude unless integral (\"very dry skin\" \u2192 include <code>very dry skin</code>) Slang (\"tummy pain\") Annotate; canonical maps to <code>abdominal pain</code> if lexicon contains mapping"},{"location":"annotation_guide/#common-pitfalls","title":"Common Pitfalls","text":"Pitfall Correction Dropping severity adjective Include full phrase Deleting negated symptom Keep &amp; rely on negation flag Including trailing period Trim punctuation Over-extending into next clause Limit to symptom/product phrase only"},{"location":"annotation_guide/#updating-this-guide","title":"Updating This Guide","text":"<p>Revise after initial adjudication cycle (\u2248 first 100 gold tasks). All changes recorded in provenance registry notes for traceability.</p>"},{"location":"annotation_guide/#faq","title":"FAQ","text":"<p>Should I annotate brand names? Yes, if they directly relate to the adverse context.</p> <p>Annotate plural symptoms? Yes; canonical mapping handles singular normalization.</p> <p>What about uncertain reactions? Annotate if consumer asserts possibility (\"might be causing redness\"). Model can later learn uncertainty patterns.</p> <p>Do I merge adjacent symptoms? Only if forming a unified phrase (\"redness and itching\" \u2192 two spans).</p>"},{"location":"annotation_guide/#next-steps","title":"Next Steps","text":"<p>After annotation export: run conversion \u2192 quality report \u2192 register batch \u2192 prepare for BIO tagging.</p>"},{"location":"ci_cd/","title":"CI/CD Documentation","text":""},{"location":"ci_cd/#overview","title":"Overview","text":"<p>SpanForge uses GitHub Actions for continuous integration and delivery. The CI/CD pipeline runs automated tests, linting, and quality checks on every push and pull request.</p>"},{"location":"ci_cd/#workflows","title":"Workflows","text":""},{"location":"ci_cd/#1-test-suite-testyml","title":"1. Test Suite (<code>test.yml</code>)","text":"<p>Triggers: - Push to <code>main</code> or <code>develop</code> branches - Pull requests to <code>main</code> or <code>develop</code> - Manual dispatch via GitHub UI</p> <p>Jobs:</p>"},{"location":"ci_cd/#test-matrix","title":"Test Matrix","text":"<ul> <li>Operating Systems: Ubuntu, Windows</li> <li>Python Versions: 3.9, 3.10, 3.11</li> <li>Total Configurations: 6 (2 OS \u00d7 3 Python versions)</li> </ul> <p>Test Stages: 1. Environment Verification: Runs <code>scripts/verify_env.py</code> 2. Core Tests: Unit tests for weak labeling, model loading, pipeline 3. Edge Case Tests: 98 parametrized edge case tests 4. Integration Tests: End-to-end pipeline and scale tests 5. Coverage Report: Generates coverage with pytest-cov</p> <p>Coverage Upload: - Only on Ubuntu + Python 3.10 configuration - Uploads to Codecov for tracking over time - Failure doesn't block CI (informational only)</p>"},{"location":"ci_cd/#lint-job","title":"Lint Job","text":"<ul> <li>Checks: ruff, black, isort</li> <li>Runs on Ubuntu + Python 3.10</li> <li>Non-blocking (continue-on-error: true)</li> </ul>"},{"location":"ci_cd/#build-check","title":"Build Check","text":"<ul> <li>Validates package can be built with <code>python -m build</code></li> <li>Checks distribution with <code>twine check</code></li> <li>Non-blocking</li> </ul>"},{"location":"ci_cd/#2-pre-commit-checks-pre-commityml","title":"2. Pre-commit Checks (<code>pre-commit.yml</code>)","text":"<p>Triggers: - Pull requests only</p> <p>Checks: - Trailing whitespace removal - End-of-file fixer - YAML/JSON validation - Large file detection (max 1MB) - Private key detection - Code formatting (black, isort, ruff) - Type checking (mypy, non-blocking)</p>"},{"location":"ci_cd/#local-development","title":"Local Development","text":""},{"location":"ci_cd/#setup-pre-commit-hooks","title":"Setup Pre-commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now hooks run automatically on <code>git commit</code>. To run manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"ci_cd/#run-tests-locally","title":"Run Tests Locally","text":"<pre><code># Full suite\npytest tests/ -v\n\n# With coverage\npytest tests/ --cov=src --cov-report=html\n\n# Specific category\npytest tests/edge_cases/ -v\npytest tests/integration/ -v\n\n# Parallel execution (faster)\npytest tests/ -n auto\n</code></pre>"},{"location":"ci_cd/#linting","title":"Linting","text":"<pre><code># Check only\nruff check src/ tests/ scripts/\nblack --check src/ tests/ scripts/\nisort --check-only src/ tests/ scripts/\n\n# Auto-fix\nruff check --fix src/ tests/ scripts/\nblack src/ tests/ scripts/\nisort src/ tests/ scripts/\n</code></pre>"},{"location":"ci_cd/#badge-integration","title":"Badge Integration","text":"<p>Add to README.md:</p> <pre><code>![Test Suite](https://github.com/paulboys/SpanForge/actions/workflows/test.yml/badge.svg)\n![Pre-commit](https://github.com/paulboys/SpanForge/actions/workflows/pre-commit.yml/badge.svg)\n[![codecov](https://codecov.io/gh/paulboys/SpanForge/branch/main/graph/badge.svg)](https://codecov.io/gh/paulboys/SpanForge)\n</code></pre>"},{"location":"ci_cd/#pull-request-requirements","title":"Pull Request Requirements","text":"<p>Before merging to <code>main</code>, ensure:</p> <ol> <li>\u2705 All test jobs pass (144/144 tests)</li> <li>\u2705 No ruff/black/isort violations</li> <li>\u2705 Coverage maintained or improved</li> <li>\u2705 Pre-commit checks pass</li> <li>\u2705 Documentation updated if needed</li> </ol>"},{"location":"ci_cd/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Current CI Times (Ubuntu + Python 3.10): - Environment setup: ~30s - Test execution: ~18s - Coverage generation: ~5s - Total: ~1-2 minutes per job</p> <p>Local Performance: - Full suite: ~17s (144 tests) - Edge cases: ~1.3s (98 tests) - Integration: ~44s (26 tests, includes 1000-doc stress test)</p>"},{"location":"ci_cd/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ci_cd/#tests-pass-locally-but-fail-in-ci","title":"Tests Pass Locally but Fail in CI","text":"<p>Check: - OS-specific path separators (<code>os.path.join</code> vs <code>pathlib.Path</code>) - Line endings (CRLF vs LF) - Timezone dependencies - File permissions</p>"},{"location":"ci_cd/#coverage-drops-unexpectedly","title":"Coverage Drops Unexpectedly","text":"<p>Causes: - New untested code paths - Removed tests without removing code - Conditional imports not triggered in test environment</p> <p>Fix: - Add tests for new features - Review coverage HTML report: <code>htmlcov/index.html</code> - Mark uncoverable lines: <code># pragma: no cover</code></p>"},{"location":"ci_cd/#pre-commit-hook-slow","title":"Pre-commit Hook Slow","text":"<p>Solutions: - Skip mypy locally: <code>SKIP=mypy git commit</code> - Run hooks in parallel: <code>pre-commit run --all-files --show-diff-on-failure</code> - Update hooks: <code>pre-commit autoupdate</code></p>"},{"location":"ci_cd/#future-enhancements","title":"Future Enhancements","text":""},{"location":"ci_cd/#planned-additions","title":"Planned Additions:","text":"<ol> <li>Nightly Builds: Extended stress tests (10k documents)</li> <li>Performance Regression: Track inference time over commits</li> <li>Security Scanning: Bandit, safety checks</li> <li>Documentation: Auto-generate API docs with Sphinx</li> <li>Release Automation: Tag-triggered PyPI publish</li> </ol>"},{"location":"ci_cd/#advanced-coverage-goals","title":"Advanced Coverage Goals:","text":"<ul> <li>Target: 90% coverage on <code>src/</code></li> <li>Branch coverage tracking</li> <li>Mutation testing with <code>mutmut</code></li> </ul>"},{"location":"ci_cd/#maintenance","title":"Maintenance","text":""},{"location":"ci_cd/#updating-dependencies","title":"Updating Dependencies","text":"<pre><code># Update GitHub Actions\n# Edit .github/workflows/*.yml, bump action versions\n\n# Update pre-commit hooks\npre-commit autoupdate\n\n# Update Python dependencies\npip list --outdated\n# Update requirements.txt accordingly\n</code></pre>"},{"location":"ci_cd/#monitoring","title":"Monitoring","text":"<ul> <li>GitHub Actions Dashboard: Monitor workflow runs</li> <li>Codecov Dashboard: Track coverage trends</li> <li>Dependabot: Enable for automated dependency updates</li> </ul> <p>Last Updated: Phase 4 (Nov 25, 2025) Status: \u2705 All workflows configured and tested</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Complete guide to SpanForge configuration options.</p>"},{"location":"configuration/#configuration-file","title":"Configuration File","text":"<p>SpanForge uses Pydantic BaseSettings for configuration management with environment variable support.</p>"},{"location":"configuration/#appconfig-parameters","title":"AppConfig Parameters","text":"Parameter Type Default Description <code>model_name</code> str <code>\"dmis-lab/biobert-base-cased-v1.1\"</code> HuggingFace model identifier <code>max_seq_len</code> int <code>256</code> Maximum sequence length for tokenization <code>device</code> str auto-detect Computation device (<code>'cuda'</code> or <code>'cpu'</code>) <code>seed</code> int <code>42</code> Random seed for reproducibility <code>negation_window</code> int <code>5</code> Tokens after negation cue to mark as negated <code>fuzzy_scorer</code> str <code>\"wratio\"</code> Fuzzy matching algorithm (<code>'wratio'</code> or <code>'jaccard'</code>) <code>llm_enabled</code> bool <code>False</code> Enable experimental LLM refinement <code>llm_provider</code> str <code>\"stub\"</code> LLM provider (<code>'stub'</code>, <code>'openai'</code>, <code>'azure'</code>) <code>llm_model</code> str <code>\"gpt-4\"</code> LLM model identifier <code>llm_min_confidence</code> float <code>0.65</code> Minimum confidence for LLM suggestions <code>llm_cache_path</code> str <code>\"data/annotation/exports/llm_cache.jsonl\"</code> LLM response cache file <code>llm_prompt_version</code> str <code>\"v1\"</code> Prompt template version"},{"location":"configuration/#usage-examples","title":"Usage Examples","text":""},{"location":"configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from src.config import AppConfig\n\n# Use defaults\nconfig = AppConfig()\nprint(config.device)  # 'cuda' if available, else 'cpu'\nprint(config.negation_window)  # 5\n</code></pre>"},{"location":"configuration/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Override defaults\nconfig = AppConfig(\n    negation_window=7,\n    fuzzy_scorer=\"jaccard\",\n    max_seq_len=512,\n    device=\"cpu\"\n)\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Set via environment variables (prefixed with your app name if needed):</p> <pre><code>export MODEL_NAME=\"dmis-lab/biobert-v1.1\"\nexport NEGATION_WINDOW=10\nexport DEVICE=\"cuda\"\n</code></pre> <pre><code># Automatically reads from environment\nconfig = AppConfig()\n</code></pre>"},{"location":"configuration/#seed-management","title":"Seed Management","text":"<pre><code>from src.config import set_seed\n\n# Set for reproducibility\nset_seed(42)\n\n# All random operations now deterministic\nimport random\nimport numpy as np\nprint(random.random())  # Same value every run\nprint(np.random.rand())  # Same value every run\n</code></pre>"},{"location":"configuration/#parameter-details","title":"Parameter Details","text":""},{"location":"configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"configuration/#model_name","title":"model_name","text":"<p>HuggingFace model identifier. Default is BioBERT base cased v1.1.</p> <p>Options: - <code>\"dmis-lab/biobert-base-cased-v1.1\"</code> (default) - <code>\"dmis-lab/biobert-large-cased-v1.1\"</code> - <code>\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"</code> - Any HuggingFace transformer model</p> <pre><code>config = AppConfig(model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n</code></pre>"},{"location":"configuration/#max_seq_len","title":"max_seq_len","text":"<p>Maximum sequence length for tokenization. Longer texts are truncated.</p> <p>Recommendations: - <code>256</code>: Default, good balance for complaints (1-3 sentences) - <code>512</code>: Longer documents, increased memory usage - <code>128</code>: Short texts, faster processing</p> <pre><code>config = AppConfig(max_seq_len=512)  # For longer documents\n</code></pre>"},{"location":"configuration/#device","title":"device","text":"<p>Computation device. Auto-detects CUDA availability.</p> <pre><code># Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Use specific GPU\nconfig = AppConfig(device=\"cuda:1\")\n</code></pre>"},{"location":"configuration/#weak-labeling-configuration","title":"Weak Labeling Configuration","text":""},{"location":"configuration/#negation_window","title":"negation_window","text":"<p>Number of tokens after negation cue to mark as negated.</p> <p>Tuning: - <code>3-5</code>: Short-range negation (default: 5) - <code>7-10</code>: Long-range negation (more false positives) - <code>1-2</code>: Very conservative</p> <pre><code># Example: \"Patient has no history of itching or redness\"\nconfig = AppConfig(negation_window=7)  # Catches \"redness\" too\n</code></pre>"},{"location":"configuration/#fuzzy_scorer","title":"fuzzy_scorer","text":"<p>Fuzzy matching algorithm selection.</p> <p>Options: - <code>\"wratio\"</code> (default): WRatio scoring, better for misspellings - <code>\"jaccard\"</code>: Token-set Jaccard, better for synonym matching</p> <pre><code># For exact synonym matching\nconfig = AppConfig(fuzzy_scorer=\"jaccard\")\n</code></pre>"},{"location":"configuration/#llm-configuration-experimental","title":"LLM Configuration (Experimental)","text":""},{"location":"configuration/#llm_enabled","title":"llm_enabled","text":"<p>Enable LLM-based span refinement pipeline.</p> <pre><code>config = AppConfig(\n    llm_enabled=True,\n    llm_provider=\"openai\",  # or \"azure\", \"anthropic\"\n    llm_model=\"gpt-4\",\n    llm_min_confidence=0.7\n)\n</code></pre>"},{"location":"configuration/#llm_min_confidence","title":"llm_min_confidence","text":"<p>Minimum confidence threshold for accepting LLM suggestions.</p> <p>Recommendations: - <code>0.5-0.6</code>: Exploratory, more suggestions - <code>0.65-0.75</code>: Balanced (default: 0.65) - <code>0.8-0.9</code>: Conservative, high precision</p>"},{"location":"configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"configuration/#cpu-optimization","title":"CPU Optimization","text":"<pre><code>config = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,  # Shorter sequences\n    # Use exact matching only when possible\n)\n</code></pre>"},{"location":"configuration/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>config = AppConfig(\n    device=\"cuda\",\n    max_seq_len=512,  # Longer sequences\n)\n\n# Enable GPU optimizations\nimport torch\ntorch.backends.cudnn.benchmark = True\n</code></pre>"},{"location":"configuration/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.pipeline import simple_inference\n\n# Process large batches\ntexts = load_texts(\"large_dataset.txt\")  # e.g., 10,000 texts\n\n# Batch processing\nbatch_size = 32\nresults = []\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i+batch_size]\n    results.extend(simple_inference(batch))\n</code></pre>"},{"location":"configuration/#configuration-profiles","title":"Configuration Profiles","text":""},{"location":"configuration/#development-profile","title":"Development Profile","text":"<pre><code>dev_config = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,\n    negation_window=5,\n    seed=42,\n    llm_enabled=False\n)\n</code></pre>"},{"location":"configuration/#production-profile","title":"Production Profile","text":"<pre><code>prod_config = AppConfig(\n    device=\"cuda\",\n    max_seq_len=256,\n    negation_window=7,\n    fuzzy_scorer=\"wratio\",\n    seed=42,\n    llm_enabled=True,\n    llm_provider=\"azure\",\n    llm_min_confidence=0.75\n)\n</code></pre>"},{"location":"configuration/#testing-profile","title":"Testing Profile","text":"<pre><code>test_config = AppConfig(\n    device=\"cpu\",\n    seed=42,  # Deterministic\n    llm_enabled=False,  # No external calls\n    negation_window=5\n)\n</code></pre>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Always set seed for reproducible experiments</li> <li>Profile before tuning - measure actual performance</li> <li>Start with defaults - they work well for most cases</li> <li>Use environment variables for deployment secrets</li> <li>Document custom configs in code comments</li> </ol>"},{"location":"configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/#issue-cuda-out-of-memory","title":"Issue: CUDA out of memory","text":"<p>Solutions: <pre><code># Reduce sequence length\nconfig = AppConfig(max_seq_len=128)\n\n# Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Clear cache between batches\nimport torch\ntorch.cuda.empty_cache()\n</code></pre></p>"},{"location":"configuration/#issue-poor-negation-detection","title":"Issue: Poor negation detection","text":"<p>Solutions: <pre><code># Extend window for long-range negation\nconfig = AppConfig(negation_window=10)\n\n# Check NEGATION_TOKENS in src/weak_label.py\n# Add custom negation cues if needed\n</code></pre></p>"},{"location":"configuration/#issue-low-fuzzy-match-recall","title":"Issue: Low fuzzy match recall","text":"<p>Solutions: <pre><code># Try Jaccard scorer\nconfig = AppConfig(fuzzy_scorer=\"jaccard\")\n\n# Lower fuzzy threshold in match_symptoms()\nspans = match_symptoms(text, lexicon, fuzzy_threshold=85.0)\n</code></pre></p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>User Guide: Weak Labeling - Advanced techniques</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"heuristic/","title":"Heuristic","text":"SpanForge Weak Labeling Heuristics <p>Specification of lexical + fuzzy gating rules that bootstrap adverse event span discovery prior to supervised BioBERT fine-tuning.</p>"},{"location":"heuristic/#weak-labeling-heuristics","title":"Weak Labeling Heuristics","text":"<p>Defines rule-based span proposal logic prior to supervised training.</p>"},{"location":"heuristic/#inputs","title":"Inputs","text":"<ul> <li>Complaint texts</li> <li>Symptom lexicon: <code>data/lexicon/symptoms.csv</code></li> <li>Product lexicon: <code>data/lexicon/products.csv</code></li> </ul>"},{"location":"heuristic/#processing-steps","title":"Processing Steps","text":"<ol> <li>Candidate Windows: Iterate token windows up to max phrase length of lexicon entries.</li> <li>Fuzzy Scoring: Compute WRatio (RapidFuzz) between window text and lexicon term.</li> <li>Token-Set Jaccard: Lowercased token set overlap percentage.</li> <li>Gates: Accept if <code>fuzzy \u2265 0.88</code> AND <code>jaccard \u2265 40</code>.</li> <li>Alignment: Multi-token candidate must align on last token boundary with lexicon term (mitigates mid-span inflation).</li> <li>Anatomy Filtering: Skip single generic anatomy tokens unless symptom keyword co-occurs.</li> <li>Negation Marking: Build window of 5 tokens around negation cues; \u226550% token overlap \u2192 mark <code>negated=True</code>.</li> <li>Confidence: Weighted combination (see below); duplicates resolved by highest confidence.</li> </ol>"},{"location":"heuristic/#thresholds","title":"Thresholds","text":"Parameter Default Purpose Fuzzy WRatio 0.88 Balances lexical variant recall vs noise Jaccard % 40 Ensures partial but meaningful token overlap Negation window 5 tokens Captures local negation context Overlap for negation \u226550% Avoids spurious negation marking Multi-token alignment Enforced Reduces partial window drift"},{"location":"heuristic/#confidence-formula","title":"Confidence Formula","text":"<p><pre><code>confidence = clamp(0.8 * (fuzzy/100) + 0.2 * (jaccard/100), 0.0, 1.0)\n</code></pre> Fuzzy &amp; Jaccard are raw percentages (0\u2013100) before weighting.</p>"},{"location":"heuristic/#negation-cues-examples","title":"Negation Cues (Examples)","text":"<p><code>no</code>, <code>not</code>, <code>without</code>, <code>never</code>, <code>none</code>, <code>free of</code>, <code>lack of</code>. Token normalization handles casing; multi-word cues expanded via phrase tokenization.</p>"},{"location":"heuristic/#canonical-mapping","title":"Canonical Mapping","text":"<p>If lexicon entry matched, <code>canonical</code> set to curated term; otherwise fallback canonical = surface span (enables later normalization decisions &amp; lexicon expansion).</p>"},{"location":"heuristic/#duplicate-overlap-policy","title":"Duplicate / Overlap Policy","text":"<ul> <li>Exact duplicates (same start,end,label) \u2192 keep highest confidence only.</li> <li>Overlapping distinct spans retained; conflicts surfaced later for human review.</li> </ul>"},{"location":"heuristic/#exclusions","title":"Exclusions","text":"<ul> <li>Pure stopword spans.</li> <li>Isolated anatomy token without symptom context.</li> <li>Zero-length or punctuation-only windows.</li> </ul>"},{"location":"heuristic/#tuning-guidance","title":"Tuning Guidance","text":"Symptom Adjust Effect High false positives Increase fuzzy (0.90) or Jaccard (50) Precision \u2191, Recall \u2193 Low recall variants Lower Jaccard (35) first Recall \u2191 moderate Many partial matches Enforce stricter alignment Noise \u2193 <p>Tune one parameter at a time; evaluate on gold comparison script.</p>"},{"location":"heuristic/#planned-enhancements","title":"Planned Enhancements","text":"<ul> <li>Embedding similarity (BioBERT cosine) secondary gate.</li> <li>Contextual disambiguation around product proximity.</li> <li>Label-specific thresholds (PRODUCT vs SYMPTOM).</li> <li>Active learning: mine high-uncertainty spans for prioritization.</li> </ul>"},{"location":"heuristic/#drift-monitoring","title":"Drift Monitoring","text":"<p>Track PRODUCT:SYMPTOM ratio per batch; sudden deviation triggers threshold audit.</p>"},{"location":"heuristic/#safety-considerations","title":"Safety Considerations","text":"<p>Avoid aggressive threshold lowering early\u2014annotation burden &amp; noise escalate quickly.</p>"},{"location":"heuristic/#reference-implementation","title":"Reference Implementation","text":"<p>See <code>src/weak_label.py</code> for authoritative logic and configuration hooks.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9, 3.10, or 3.11</li> <li>Git</li> <li>4GB RAM minimum</li> <li>Optional: CUDA-capable GPU for faster inference</li> </ul>"},{"location":"installation/#standard-installation","title":"Standard Installation","text":""},{"location":"installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/paulboys/SpanForge.git\ncd SpanForge\n</code></pre>"},{"location":"installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"Condavenv <pre><code>conda create -n spanforge python=3.10\nconda activate spanforge\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or\nvenv\\Scripts\\activate  # Windows\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python scripts/verify_env.py\n</code></pre> <p>Expected output: <pre><code>\u2713 PyTorch installed\n\u2713 Transformers installed\n\u2713 Device: cuda (or cpu)\n\u2713 BioBERT model downloadable\n\u2713 Environment ready\n</code></pre></p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For contributors who want to run tests and linting:</p> <pre><code># Install dev dependencies\npip install -r requirements.txt\npip install pytest pytest-cov pre-commit ruff black isort mypy\n\n# Setup pre-commit hooks\npre-commit install\n\n# Verify tests pass\npytest tests/ -v\n</code></pre>"},{"location":"installation/#documentation-build","title":"Documentation Build","text":"<p>To build documentation locally:</p> <pre><code>pip install -r docs-requirements.txt\nmkdocs serve\n</code></pre> <p>Visit http://127.0.0.1:8000 to view docs.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#issue-pytorch-cuda-not-detected","title":"Issue: PyTorch CUDA not detected","text":"<p>Solution: Install PyTorch with CUDA support: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"installation/#issue-transformers-model-download-fails","title":"Issue: Transformers model download fails","text":"<p>Solution: Check internet connection or use offline model: <pre><code># Download model manually\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nmodel.save_pretrained(\"./models/biobert\")\n</code></pre></p>"},{"location":"installation/#issue-import-errors","title":"Issue: Import errors","text":"<p>Solution: Ensure you're in the project root and virtual environment is activated: <pre><code>pwd  # Should show SpanForge directory\nwhich python  # Should show venv python\n</code></pre></p>"},{"location":"installation/#optional-components","title":"Optional Components","text":""},{"location":"installation/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For CUDA support: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"installation/#label-studio-for-annotation","title":"Label Studio (for annotation)","text":"<pre><code>pip install label-studio\nsetx LABEL_STUDIO_DISABLE_TELEMETRY 1  # Windows\n# or\nexport LABEL_STUDIO_DISABLE_TELEMETRY=1  # Linux/Mac\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration Options</li> <li>API Reference</li> </ul>"},{"location":"overview/","title":"Overview","text":"SpanForge Documentation <p>Core architectural overview for the SpanForge adverse event annotation and NER pipeline.</p>"},{"location":"overview/#overview","title":"Overview","text":"<p>End-to-end Adverse Event NER pipeline integrating weak labeling, BioBERT embeddings, human annotation, and quality/provenance tracking. Designed to iteratively refine span quality before supervised fine-tuning.</p>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Text \u2192 Weak Labeler \u2192 Weak JSONL \u2192 Label Studio Tasks \u2192 Human Annotation \u2192 Export\n    \u2192 Gold Converter (+canonical +provenance) \u2192 Gold JSONL \u2192 Quality Metrics / Registry\n                                 \u2192 (Planned) BIO Tagging \u2192 Token Classifier \u2192 Evaluation\n</code></pre>"},{"location":"overview/#core-modules","title":"Core Modules","text":"<ul> <li><code>src/config.py</code>: Centralized configuration (model, device, heuristic thresholds, negation window, scorer).</li> <li><code>src/model.py</code>: BioBERT base encoder loader (future token classification head attachment point).</li> <li><code>src/weak_label.py</code>: Lexicon + fuzzy + Jaccard gating, negation detection, confidence scoring.</li> <li><code>src/pipeline.py</code>: Light inference utilities; will wrap supervised prediction later.</li> <li><code>scripts/annotation/*.py</code>: Operational scripts for project init, import, convert, quality, adjudication, registry, CLI.</li> <li><code>scripts/Workbook.ipynb</code>: Educational notebook walking through ingestion \u2192 gold.</li> </ul>"},{"location":"overview/#entity-types","title":"Entity Types","text":"<ul> <li><code>SYMPTOM</code>: Physiological or subjective adverse effects reported by consumer.</li> <li><code>PRODUCT</code>: Product names, formulations, or product category terms referenced.</li> </ul>"},{"location":"overview/#canonical-mapping","title":"Canonical Mapping","text":"<p>Surface forms mapped to canonical normalized terms (symptoms/products) via curated lexicons (<code>data/lexicon/</code>). This stabilizes vocabulary for aggregation and downstream modeling.</p>"},{"location":"overview/#provenance-fields","title":"Provenance Fields","text":"<p>Gold output includes: <code>source</code>, <code>annotator</code>, <code>revision</code>, <code>canonical</code>, optional <code>concept_id</code> for traceability and reproducibility.</p>"},{"location":"overview/#heuristic-summary","title":"Heuristic Summary","text":"<ul> <li>Fuzzy WRatio \u2265 0.88</li> <li>Jaccard token-set \u2265 40</li> <li>Confidence = <code>0.8*fuzzy + 0.2*jaccard</code> (\u22641.0)</li> <li>Negation window: 5 tokens (\u226550% overlap \u2192 negated)</li> <li>Single generic anatomy token skip unless symptom co-occurs</li> <li>Last-token alignment for multi-token fuzzy spans</li> </ul> <p>Details: <code>heuristic.md</code>.</p>"},{"location":"overview/#quality-metrics","title":"Quality Metrics","text":"<p><code>quality_report.py</code> computes span density, label distribution, conflicts, annotator counts; planned kappa &amp; drift metrics.</p>"},{"location":"overview/#design-principles","title":"Design Principles","text":"<ul> <li>Pure functions; explicit dependency passing</li> <li>Reproducible thresholds in config (tunable)</li> <li>Incremental, test-supported evolution (weak \u2192 gold \u2192 supervised)</li> <li>Clear audit trail via registry and provenance fields</li> </ul>"},{"location":"overview/#privacy-compliance","title":"Privacy &amp; Compliance","text":"<p>No raw proprietary complaints committed. Annotation performed locally with telemetry disabled. Only de-identified / approved text permitted in repository.</p>"},{"location":"overview/#roadmap-condensed","title":"Roadmap (Condensed)","text":"<ol> <li>Lexicon &amp; Weak Labeling (complete / iterative)</li> <li>Annotation &amp; Curation (in progress)</li> <li>Gold Assembly &amp; Expansion</li> <li>Token Classification Fine-Tune</li> <li>Domain Adaptation (MLM)</li> <li>Baseline &amp; Evaluation (RoBERTa)</li> <li>Active Learning Loop</li> </ol>"},{"location":"overview/#references","title":"References","text":"<p>See <code>annotation_guide.md</code>, <code>tutorial_labeling.md</code>, and <code>heuristic.md</code> for deeper detail.</p>"},{"location":"prompting_llm/","title":"LLM-Assisted Refinement (Experimental)","text":"<p>SpanForge optionally applies Large Language Model (LLM) prompts to improve weak label span boundaries, negation status, and canonical mapping prior to human annotation.</p>"},{"location":"prompting_llm/#goals","title":"Goals","text":"<ul> <li>Tighten span boundaries and reduce annotator edit time.</li> <li>Identify ambiguous or mid-confidence spans for targeted refinement.</li> <li>Provide provenance and reproducibility (prompt version, model, provider).</li> </ul>"},{"location":"prompting_llm/#workflow-insertion","title":"Workflow Insertion","text":"<pre><code>Raw Text \u2192 Weak Labeler \u2192 (OPTIONAL) LLM Refinement \u2192 Label Studio Annotation \u2192 Gold Conversion \u2192 Quality / Registry\n</code></pre>"},{"location":"prompting_llm/#configuration-srcconfigpy","title":"Configuration (src/config.py)","text":"<p>Fields: - <code>llm_enabled</code>: Master toggle (default false). - <code>llm_provider</code>: Stub/openai/etc. - <code>llm_model</code>: Model identifier (e.g. gpt-4). - <code>llm_min_confidence</code>: Minimum confidence to keep LLM suggestion. - <code>llm_cache_path</code>: Future caching location. - <code>llm_prompt_version</code>: Version tag for prompt templates.</p>"},{"location":"prompting_llm/#prompts","title":"Prompts","text":"<p>Templates stored under <code>prompts/</code>: - <code>boundary_refine.txt</code>: Adjust span boundaries. - <code>negation_check.txt</code>: Negation classification. - <code>synonym_expand.txt</code>: Canonical + synonyms.</p> <p>Templates include placeholders: <code>{{text}}</code>, <code>{{candidates}}</code>, <code>{{knowledge}}</code>.</p>"},{"location":"prompting_llm/#provenance-fields","title":"Provenance Fields","text":"<p>Each suggestion appended in <code>refine_llm.py</code> includes: - <code>source = llm_refine</code> - <code>llm_confidence</code> - <code>confidence_reason</code> (future, model-supplied) - <code>canonical</code> (if provided) - <code>prompt_version</code>, <code>model</code>, <code>provider</code> in <code>llm_meta</code></p>"},{"location":"prompting_llm/#cli-usage","title":"CLI Usage","text":"<p><pre><code>python scripts/annotation/cli.py refine-llm \\\n  --weak data/output/weak.jsonl \\\n  --out data/output/refined_weak.jsonl \\\n  --prompt prompts/boundary_refine.txt\n</code></pre> Add <code>--dry-run</code> to preview without writing output.</p>"},{"location":"prompting_llm/#filtering-logic","title":"Filtering Logic","text":"<p>Only spans with heuristic confidence in mid band (0.55\u20130.75) are candidates.</p>"},{"location":"prompting_llm/#stub-behavior","title":"Stub Behavior","text":"<p>Current implementation returns no suggestions (stub). Integrate provider later by extending <code>LLMAgent.call()</code>.</p>"},{"location":"prompting_llm/#extending-to-real-provider","title":"Extending to Real Provider","text":"<ol> <li>Inject API key via environment variable.</li> <li>Replace stub call with provider client invocation.</li> <li>Enforce <code>temperature=0</code> and JSON schema.</li> <li>Implement retry + minimal exponential backoff.</li> </ol>"},{"location":"prompting_llm/#evaluation-future","title":"Evaluation (Future)","text":"<p>Script: <code>evaluate_refinement.py</code> (planned) will measure: - Boundary IOU uplift versus original weak spans. - Conflict reduction after adjudication. - Long-tail canonical discovery.</p>"},{"location":"prompting_llm/#safety-privacy","title":"Safety &amp; Privacy","text":"<ul> <li>De-identify raw complaint text before external calls.</li> <li>Log prompt hashes for reproducibility.</li> <li>Maintain local cache to minimize repeated transmissions.</li> </ul>"},{"location":"prompting_llm/#next-steps","title":"Next Steps","text":"<ul> <li>Implement real provider integration.</li> <li>Add evaluation script and tests for parsing.</li> <li>Incorporate acceptance metrics in registry.</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with SpanForge in 5 minutes.</p>"},{"location":"quickstart/#basic-entity-detection","title":"Basic Entity Detection","text":""},{"location":"quickstart/#1-load-lexicons","title":"1. Load Lexicons","text":"<pre><code>from pathlib import Path\nfrom src.weak_label import load_symptom_lexicon, load_product_lexicon\n\n# Load symptom and product lexicons\nsymptom_lex = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\"))\nproduct_lex = load_product_lexicon(Path(\"data/lexicon/products.csv\"))\n\nprint(f\"Loaded {len(symptom_lex)} symptoms, {len(product_lex)} products\")\n</code></pre>"},{"location":"quickstart/#2-detect-entities","title":"2. Detect Entities","text":"<pre><code>from src.weak_label import weak_label\n\ntext = \"Patient developed severe rash and itching after using the moisturizer\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text:20} | {span.label:10} | conf={span.confidence:.2f} | negated={span.negated}\")\n</code></pre> <p>Output: <pre><code>severe rash          | SYMPTOM    | conf=1.00 | negated=False\nitching              | SYMPTOM    | conf=1.00 | negated=False\nmoisturizer          | PRODUCT    | conf=1.00 | negated=False\n</code></pre></p>"},{"location":"quickstart/#3-negation-detection","title":"3. Negation Detection","text":"<pre><code>text = \"No irritation from the face wash\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text}: negated={span.negated}\")\n</code></pre> <p>Output: <pre><code>irritation: negated=True\nface wash: negated=False\n</code></pre></p>"},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":""},{"location":"quickstart/#process-multiple-documents","title":"Process Multiple Documents","text":"<pre><code>from src.weak_label import weak_label_batch\n\ntexts = [\n    \"Severe headache after using the serum\",\n    \"No side effects, works great\",\n    \"Mild dryness but no redness\"\n]\n\n# Batch process\nall_spans = weak_label_batch(texts, symptom_lex, product_lex)\n\nfor i, (text, spans) in enumerate(zip(texts, all_spans), 1):\n    print(f\"\\nDocument {i}: {text}\")\n    print(f\"Found {len(spans)} entities:\")\n    for span in spans:\n        print(f\"  - {span.text} ({span.label})\")\n</code></pre>"},{"location":"quickstart/#save-to-jsonl","title":"Save to JSONL","text":"<pre><code>from src.weak_label import persist_weak_labels_jsonl\nfrom pathlib import Path\n\n# Persist results\noutput_path = Path(\"data/output/results.jsonl\")\npersist_weak_labels_jsonl(texts, all_spans, output_path)\n\nprint(f\"Saved to {output_path}\")\n</code></pre>"},{"location":"quickstart/#pipeline-inference","title":"Pipeline Inference","text":""},{"location":"quickstart/#full-biobert-weak-labeling","title":"Full BioBERT + Weak Labeling","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\n    \"Itching and redness from the facial cream\",\n    \"No adverse effects noted\"\n]\n\n# Run full pipeline\nresults = simple_inference(texts, persist_path=\"output.jsonl\")\n\nfor result in results:\n    print(f\"Token count: {result['token_count']}\")\n    print(f\"Detected spans: {len(result['weak_spans'])}\")\n\n    for span in result['weak_spans']:\n        print(f\"  {span['text']} ({span['label']})\")\n</code></pre>"},{"location":"quickstart/#configuration","title":"Configuration","text":""},{"location":"quickstart/#custom-parameters","title":"Custom Parameters","text":"<pre><code>from src.config import AppConfig\n\n# Create custom config\nconfig = AppConfig(\n    negation_window=7,  # Extend negation window\n    fuzzy_scorer=\"jaccard\",  # Use Jaccard instead of WRatio\n    device=\"cpu\"  # Force CPU\n)\n\n# Use in weak labeling\nfrom src.weak_label import match_symptoms\n\nspans = match_symptoms(\n    text=\"Patient has severe itching\",\n    lexicon=symptom_lex,\n    negation_window=config.negation_window,\n    scorer=config.fuzzy_scorer\n)\n</code></pre>"},{"location":"quickstart/#working-with-spans","title":"Working with Spans","text":""},{"location":"quickstart/#filter-by-confidence","title":"Filter by Confidence","text":"<pre><code># Get high-confidence spans only\nhigh_conf_spans = [s for s in spans if s.confidence &gt;= 0.9]\n\nprint(f\"High confidence: {len(high_conf_spans)}/{len(spans)}\")\n</code></pre>"},{"location":"quickstart/#group-by-label","title":"Group by Label","text":"<pre><code>from collections import defaultdict\n\nby_label = defaultdict(list)\nfor span in spans:\n    by_label[span.label].append(span)\n\nprint(f\"Symptoms: {len(by_label['SYMPTOM'])}\")\nprint(f\"Products: {len(by_label['PRODUCT'])}\")\n</code></pre>"},{"location":"quickstart/#check-for-negated-entities","title":"Check for Negated Entities","text":"<pre><code>negated = [s for s in spans if s.negated]\naffirmed = [s for s in spans if not s.negated]\n\nprint(f\"Affirmed: {len(affirmed)}\")\nprint(f\"Negated: {len(negated)}\")\n</code></pre>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#entity-co-occurrence","title":"Entity Co-occurrence","text":"<pre><code>def find_symptom_product_pairs(text, symptom_lex, product_lex):\n    \"\"\"Find symptoms mentioned with products.\"\"\"\n    spans = weak_label(text, symptom_lex, product_lex)\n\n    symptoms = [s for s in spans if s.label == \"SYMPTOM\" and not s.negated]\n    products = [s for s in spans if s.label == \"PRODUCT\"]\n\n    if symptoms and products:\n        return [(s.text, p.text) for s in symptoms for p in products]\n    return []\n\n# Example\ntext = \"Developed rash and itching from the hydra cream\"\npairs = find_symptom_product_pairs(text, symptom_lex, product_lex)\nprint(f\"Symptom-Product pairs: {pairs}\")\n# Output: [('rash', 'hydra cream'), ('itching', 'hydra cream')]\n</code></pre>"},{"location":"quickstart/#confidence-threshold","title":"Confidence Threshold","text":"<pre><code>def filter_confident_spans(spans, threshold=0.85):\n    \"\"\"Keep only high-confidence, non-negated spans.\"\"\"\n    return [\n        s for s in spans \n        if s.confidence &gt;= threshold and not s.negated\n    ]\n\nconfident = filter_confident_spans(spans, threshold=0.9)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Detailed parameter tuning</li> <li>User Guide: Weak Labeling - Advanced techniques</li> <li>User Guide: Negation - Negation patterns</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"tutorial_labeling/","title":"Tutorial labeling","text":"SpanForge Labeling Tutorial <p>Step-by-step workflow for converting weak labels into high-quality gold adverse event spans using Label Studio.</p>"},{"location":"tutorial_labeling/#labeling-tutorial-label-studio","title":"Labeling Tutorial (Label Studio)","text":"<p>Walk-through for non-technical users to curate SYMPTOM and PRODUCT spans.</p>"},{"location":"tutorial_labeling/#1-environment-setup","title":"1. Environment Setup","text":"<p><pre><code>setx LABEL_STUDIO_DISABLE_TELEMETRY 1   # one-time (optional)\nconda activate NER\nlabel-studio start --no-browser\n</code></pre> Open http://localhost:8080 in a browser.</p>"},{"location":"tutorial_labeling/#2-obtain-api-token","title":"2. Obtain API Token","text":"<ul> <li>Click user avatar \u2192 Account Settings \u2192 Enable legacy API tokens (Org tab if required).</li> <li>Generate token; copy for import scripts.</li> </ul>"},{"location":"tutorial_labeling/#3-create-project","title":"3. Create Project","text":"<p>Use included config file: <pre><code>python scripts/annotation/init_label_studio_project.py --name \"Adverse Event NER\"\n</code></pre> Alternatively create manually in UI, paste XML from <code>data/annotation/config/label_config.xml</code>.</p>"},{"location":"tutorial_labeling/#4-generate-import-weak-tasks","title":"4. Generate &amp; Import Weak Tasks","text":"<p>Weak label JSONL (from notebook or pipeline) lives at <code>data/output/&lt;batch&gt;_weak.jsonl</code>. <pre><code>python scripts/annotation/cli.py import-weak `\n  --weak data/output/workflow_demo_weak.jsonl `\n  --out data/annotation/exports/tasks.json `\n  --push --project-id &lt;PROJECT_ID&gt;\n</code></pre> Tasks appear in the project task list.</p>"},{"location":"tutorial_labeling/#5-annotating","title":"5. Annotating","text":"<p>For each task: 1. Read entire complaint. 2. Highlight SYMPTOM or PRODUCT phrase. 3. Select label from interface. 4. Adjust existing weak suggestions (delete incorrect, expand truncated, add missing). 5. Save/Submit.</p>"},{"location":"tutorial_labeling/#boundary-checklist","title":"Boundary Checklist","text":"<ul> <li>Include severity modifiers (\"severe rash\").</li> <li>Exclude trailing punctuation.</li> <li>Preserve internal spacing/casing.</li> <li>Keep negated symptoms (e.g., \"no irritation\")\u2014negation handled later.</li> </ul>"},{"location":"tutorial_labeling/#keyboard-shortcuts-default","title":"Keyboard Shortcuts (Default)","text":"Action Shortcut Select label Click or numeric index Undo last span Ctrl+Z Redo Ctrl+Y"},{"location":"tutorial_labeling/#6-export-annotations","title":"6. Export Annotations","text":"<p>In project: Export \u2192 JSON. Save to <code>data/annotation/raw/&lt;export_name&gt;.json</code>.</p>"},{"location":"tutorial_labeling/#7-convert-to-gold","title":"7. Convert to Gold","text":"<pre><code>python scripts/annotation/convert_labelstudio.py `\n  --input data/annotation/raw/&lt;export_name&gt;.json `\n  --output data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --source &lt;batch_id&gt; `\n  --annotator &lt;your_name&gt; `\n  --symptom-lexicon data/lexicon/symptoms.csv `\n  --product-lexicon data/lexicon/products.csv\n</code></pre>"},{"location":"tutorial_labeling/#8-quality-report","title":"8. Quality Report","text":"<p><pre><code>python scripts/annotation/cli.py quality `\n  --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --out data/annotation/reports/&lt;batch&gt;_quality.json\n</code></pre> Review conflicts and distributions.</p>"},{"location":"tutorial_labeling/#9-register-batch","title":"9. Register Batch","text":"<pre><code>python scripts/annotation/cli.py register `\n  --batch-id &lt;batch_id&gt; `\n  --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --annotators &lt;comma_names&gt; `\n  --revision 1\n</code></pre>"},{"location":"tutorial_labeling/#10-adjudication-if-needed","title":"10. Adjudication (If Needed)","text":"<p>Conflicts listed in quality report can be processed: <pre><code>python scripts/annotation/adjudicate.py --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl --out data/annotation/exports/&lt;batch&gt;_gold_resolved.jsonl\n</code></pre></p>"},{"location":"tutorial_labeling/#troubleshooting","title":"Troubleshooting","text":"Issue Resolution 400 import error Ensure tasks POST body is raw array, not wrapped object. Empty task list Verify project ID &amp; API token validity. Token rejected Legacy tokens may need enabling in Org settings. Wrong character indices Confirm spans copy exact text; use notebook integrity cell."},{"location":"tutorial_labeling/#best-practices","title":"Best Practices","text":"<ul> <li>Annotate daily small batches (\u226450) for consistency.</li> <li>Log notes in registry for threshold changes.</li> <li>Periodically re-run quality reports to monitor drift.</li> </ul>"},{"location":"tutorial_labeling/#next-steps","title":"Next Steps","text":"<p>After several batches: perform consensus, finalize label inventory, generate BIO tags, and begin supervised fine-tuning.</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>All notable changes to SpanForge will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"about/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"about/changelog/#planned","title":"Planned","text":"<ul> <li>Label Studio annotation workflow integration</li> <li>Gold standard dataset (500+ annotated samples)</li> <li>Token classification fine-tuning</li> <li>Model evaluation framework</li> <li>Active learning pipeline</li> </ul>"},{"location":"about/changelog/#040-2025-01-xx","title":"[0.4.0] - 2025-01-XX","text":""},{"location":"about/changelog/#added","title":"Added","text":"<ul> <li>Documentation Infrastructure: Complete MkDocs setup with Material theme</li> <li>Homepage with architecture diagrams, performance metrics, quick start</li> <li>5 API reference pages (config, model, weak_label, pipeline, llm_agent)</li> <li>Installation guide with troubleshooting</li> <li>Quick start tutorial with code examples</li> <li>User guides: weak labeling, negation, pipeline, annotation</li> <li>Development guide: testing infrastructure</li> <li>About pages: roadmap, changelog</li> <li>Docstrings: Comprehensive Google-style docstrings for all core modules</li> <li><code>src/config.py</code>: Module, class, function docstrings with examples</li> <li><code>src/model.py</code>: Detailed function docstrings with parameter/return docs</li> <li><code>src/pipeline.py</code>: Complete pipeline documentation</li> <li><code>src/llm_agent.py</code>: Enhanced class and method documentation</li> <li>Type Hints: Explicit type annotations throughout codebase</li> <li><code>Optional</code>, <code>AutoTokenizer</code>, <code>AutoModel</code>, <code>BatchEncoding</code> types</li> <li>Consistent typing for all function signatures</li> <li>CI/CD: GitHub Actions workflows and pre-commit hooks</li> <li><code>test.yml</code>: 6 configurations (Ubuntu/Windows \u00d7 Python 3.9/3.10/3.11)</li> <li><code>pre-commit.yml</code>: Automated code quality checks</li> <li>Pre-commit hooks for pytest, formatting</li> <li>Configuration: Consolidated pyproject.toml</li> <li>Black, isort, pytest, coverage configurations</li> <li>Tool configurations centralized</li> </ul>"},{"location":"about/changelog/#changed","title":"Changed","text":"<ul> <li>Updated README with CI/CD badges and documentation links</li> <li>Enhanced project structure documentation</li> </ul>"},{"location":"about/changelog/#documentation","title":"Documentation","text":"<ul> <li>\u2705 MkDocs Material theme with light/dark mode</li> <li>\u2705 Auto-generated API reference using mkdocstrings</li> <li>\u2705 Comprehensive user guides (weak labeling, negation, pipeline, annotation)</li> <li>\u2705 Testing guide with 144-test infrastructure documentation</li> <li>\u2705 Roadmap with 12 phases (4 complete, 8 planned)</li> </ul>"},{"location":"about/changelog/#030-2025-01-15","title":"[0.3.0] - 2025-01-15","text":""},{"location":"about/changelog/#added_1","title":"Added","text":"<ul> <li>Test Infrastructure: Comprehensive test suite with 144 tests</li> <li>16 core functionality tests</li> <li>98 edge case tests (unicode, emoji, negation, boundaries, anatomy, validation)</li> <li>26 integration tests (pipeline, scale, performance)</li> <li>4 curation tests (Label Studio export validation)</li> <li>Test Composition Pattern: Base classes with shared fixtures</li> <li><code>WeakLabelTestBase</code> for common setup</li> <li>Eliminates test code duplication</li> <li>Easy extension for new test categories</li> <li>Edge Case Coverage: Extensive validation</li> <li>Unicode and emoji handling (12 tests)</li> <li>Negation patterns (24 tests)</li> <li>Boundary conditions (18 tests)</li> <li>Anatomy filter (15 tests)</li> <li>Validation and errors (29 tests)</li> <li>Integration Tests: End-to-end validation</li> <li>Pipeline inference (11 tests)</li> <li>Scale and batch processing (9 tests)</li> <li>Performance benchmarks (6 tests)</li> </ul>"},{"location":"about/changelog/#fixed","title":"Fixed","text":"<ul> <li>Bidirectional Negation: Expanded negation token list</li> <li>Added: \"non\", \"non-\", \"free of\", \"absence of\", \"ruled out\", \"r/o\"</li> <li>Improved backward negation detection</li> <li>Fixed negation window boundary conditions</li> <li>Unicode Handling: Robust emoji and special character support</li> <li>Emoji within text doesn't break span detection</li> <li>Medical symbols (\u2265, \u00b1, \u00b0) handled correctly</li> <li>Accented characters in symptoms preserved</li> <li>Validation Fixes: Improved error handling</li> <li>Empty lexicon handling</li> <li>Empty text handling</li> <li>Confidence score clamping [0, 1]</li> <li>Boundary checks for span offsets</li> </ul>"},{"location":"about/changelog/#performance","title":"Performance","text":"<ul> <li>Established performance benchmarks:</li> <li><code>match_symptoms</code>: ~10ms/text (CPU)</li> <li><code>simple_inference</code>: ~200ms/text (CPU), ~50ms/text (GPU)</li> <li>Batch processing: ~5s for 32 texts (CPU), ~1s (GPU)</li> </ul>"},{"location":"about/changelog/#test-results","title":"Test Results","text":"<ul> <li>144/144 tests passing (100% pass rate)</li> <li>Test coverage: ~87% overall, ~94% core modules</li> </ul>"},{"location":"about/changelog/#020-2024-12-20","title":"[0.2.0] - 2024-12-20","text":""},{"location":"about/changelog/#added_2","title":"Added","text":"<ul> <li>Bidirectional Negation Detection: Forward and backward negation windows</li> <li>Forward: Negation cue \u2192 [window] \u2192 span</li> <li>Backward: Span \u2192 [window] \u2192 negation cue</li> <li>Configurable window size (default: 5 tokens)</li> <li>Last-Token Alignment Filter: Prevents partial-word matches</li> <li>Multi-token fuzzy matches must end at token boundaries</li> <li>Reduces false positives from substring matches</li> <li>Anatomy Singleton Filter: Rejects generic anatomy mentions</li> <li>Single anatomy tokens (skin, eye, face) rejected unless symptom co-occurs</li> <li>List of 30+ anatomy terms</li> <li>Reduces false positives by ~15%</li> <li>Emoji and Unicode Handling: Robust text processing</li> <li>Emoji within text doesn't break tokenization</li> <li>Unicode medical symbols supported</li> <li>Preserves span offsets with multi-byte characters</li> <li>Confidence Scoring: Weighted fuzzy + Jaccard scores</li> <li>Formula: 0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score</li> <li>Clamped to [0, 1] range</li> <li>Well-calibrated for active learning thresholds</li> <li>Expanded Negation Tokens: 22 negation cues</li> <li>Clinical: denies, denied, negative, absent, unremarkable</li> <li>Rule-out: rule out, ruled out, r/o</li> <li>Temporal: no longer, ceased</li> </ul>"},{"location":"about/changelog/#changed_1","title":"Changed","text":"<ul> <li>Negation Window: Default increased from 3 to 5 tokens</li> <li>Better balance of precision and recall</li> <li>Configurable via <code>AppConfig.negation_window</code></li> <li>Fuzzy Threshold: Default remains 88.0 (WRatio)</li> <li>Well-calibrated after evaluation</li> <li>Jaccard Threshold: Default remains 40.0</li> <li>Effective quality gate for fuzzy matches</li> </ul>"},{"location":"about/changelog/#performance_1","title":"Performance","text":"<ul> <li>Negation recall improved ~30% with bidirectional detection</li> <li>False positives reduced ~15% with anatomy filter</li> <li>Emoji handling prevents span breakage in ~5% of texts</li> </ul>"},{"location":"about/changelog/#010-2024-11-15","title":"[0.1.0] - 2024-11-15","text":""},{"location":"about/changelog/#added_3","title":"Added","text":"<ul> <li>Initial Release: Core weak labeling functionality</li> <li>BioBERT Integration: Model and tokenizer loading</li> <li>Default: <code>dmis-lab/biobert-base-cased-v1.1</code></li> <li>Singleton pattern for efficient caching</li> <li>Auto-detects CUDA availability</li> <li>Fuzzy Matching: RapidFuzz WRatio-based entity detection</li> <li>Default threshold: 88.0</li> <li>Handles typos and misspellings</li> <li>N-gram tokenization (1-6 tokens)</li> <li>Jaccard Token-Set Filter: Quality gate for fuzzy matches</li> <li>Default threshold: 40.0</li> <li>Prevents false positives from short common words</li> <li>Forward Negation Detection: Basic negation scope</li> <li>Window: 3 tokens (initial default)</li> <li>Standard negation cues: no, not, none, never, without</li> <li>Lexicons: Initial symptom and product lexicons</li> <li><code>data/lexicon/symptoms.csv</code>: MedDRA-derived symptom terms</li> <li><code>data/lexicon/products.csv</code>: Product names and brands</li> <li>Pipeline: End-to-end inference workflow</li> <li><code>simple_inference()</code> function</li> <li>Optional JSONL export</li> <li>Batch processing support</li> <li>Configuration: Centralized config management</li> <li>Pydantic BaseSettings</li> <li>Environment variable support</li> <li>Device auto-detection (CUDA/CPU)</li> <li>LLM Agent Stub: Experimental refinement pipeline</li> <li>Stub implementation for future LLM integration</li> <li>Data structures for span refinement suggestions</li> </ul>"},{"location":"about/changelog/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>model_name</code>: BioBERT model identifier</li> <li><code>max_seq_len</code>: Maximum sequence length (default: 256)</li> <li><code>device</code>: Computation device (auto-detect)</li> <li><code>seed</code>: Random seed (default: 42)</li> <li><code>negation_window</code>: Negation scope (default: 3)</li> <li><code>fuzzy_scorer</code>: Matching algorithm (default: \"wratio\")</li> </ul>"},{"location":"about/changelog/#dependencies","title":"Dependencies","text":"<ul> <li><code>transformers&gt;=4.30.0</code>: HuggingFace transformers</li> <li><code>torch&gt;=2.0.0</code>: PyTorch</li> <li><code>rapidfuzz&gt;=3.0.0</code>: Fuzzy string matching</li> <li><code>pydantic&gt;=2.0.0</code>: Configuration management</li> <li><code>pandas&gt;=2.0.0</code>: Data processing</li> </ul>"},{"location":"about/changelog/#version-naming","title":"Version Naming","text":"<ul> <li>Major (X.0.0): Breaking changes, major milestones (e.g., supervised model release)</li> <li>Minor (0.X.0): New features, enhancements (e.g., new filters, annotation tools)</li> <li>Patch (0.0.X): Bug fixes, documentation updates</li> </ul>"},{"location":"about/changelog/#upcoming-releases","title":"Upcoming Releases","text":""},{"location":"about/changelog/#v050-planned-q1-2025","title":"v0.5.0 (Planned: Q1 2025)","text":"<ul> <li>Label Studio annotation workflow</li> <li>Weak label export/import scripts</li> <li>Consensus and adjudication tools</li> <li>Quality assurance metrics (inter-annotator agreement)</li> <li>Annotation tutorial and guidelines</li> </ul>"},{"location":"about/changelog/#v100-planned-q2-2025","title":"v1.0.0 (Planned: Q2 2025)","text":"<ul> <li>Fine-tuned BioBERT token classification model</li> <li>Gold standard dataset (500+ annotations)</li> <li>Model evaluation framework</li> <li>Training and inference scripts</li> <li>Production-ready NER pipeline</li> </ul>"},{"location":"about/changelog/#links","title":"Links","text":"<ul> <li>Repository: GitHub</li> <li>Documentation: MkDocs Site</li> <li>Issues: Issue Tracker</li> <li>Roadmap: Project Roadmap</li> </ul> <p>Keep this changelog updated with each release or significant change.</p>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#project-license","title":"Project License","text":"<p>SpanForge is released under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<pre><code>MIT License\n\nCopyright (c) 2024-2025 SpanForge Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>SpanForge depends on several third-party libraries, each with their own licenses:</p>"},{"location":"about/license/#core-dependencies","title":"Core Dependencies","text":""},{"location":"about/license/#pytorch","title":"PyTorch","text":"<ul> <li>License: BSD-3-Clause</li> <li>Copyright: Facebook, Inc. and its affiliates</li> <li>Link: https://github.com/pytorch/pytorch/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#transformers-huggingface","title":"Transformers (HuggingFace)","text":"<ul> <li>License: Apache License 2.0</li> <li>Copyright: The HuggingFace Inc. team</li> <li>Link: https://github.com/huggingface/transformers/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#rapidfuzz","title":"RapidFuzz","text":"<ul> <li>License: MIT License</li> <li>Copyright: Max Bachmann</li> <li>Link: https://github.com/maxbachmann/RapidFuzz/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#pydantic","title":"Pydantic","text":"<ul> <li>License: MIT License</li> <li>Copyright: Samuel Colvin</li> <li>Link: https://github.com/pydantic/pydantic/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#pandas","title":"Pandas","text":"<ul> <li>License: BSD 3-Clause License</li> <li>Copyright: AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team</li> <li>Link: https://github.com/pandas-dev/pandas/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#development-dependencies","title":"Development Dependencies","text":""},{"location":"about/license/#pytest","title":"pytest","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/pytest-dev/pytest/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#black","title":"Black","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/psf/black/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#isort","title":"isort","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/PyCQA/isort/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#documentation-dependencies","title":"Documentation Dependencies","text":""},{"location":"about/license/#mkdocs","title":"MkDocs","text":"<ul> <li>License: BSD 2-Clause License</li> <li>Link: https://github.com/mkdocs/mkdocs/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#mkdocs-material","title":"MkDocs Material","text":"<ul> <li>License: MIT License</li> <li>Copyright: Martin Donath</li> <li>Link: https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#mkdocstrings","title":"mkdocstrings","text":"<ul> <li>License: ISC License</li> <li>Link: https://github.com/mkdocstrings/mkdocstrings/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#model-licenses","title":"Model Licenses","text":""},{"location":"about/license/#biobert","title":"BioBERT","text":"<p>SpanForge uses BioBERT (<code>dmis-lab/biobert-base-cased-v1.1</code>) as the default model.</p> <ul> <li>License: Apache License 2.0</li> <li>Authors: DMIS Lab, Korea University</li> <li>Citation:   <pre><code>@article{lee2020biobert,\n  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\n  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},\n  journal={Bioinformatics},\n  volume={36},\n  number={4},\n  pages={1234--1240},\n  year={2020},\n  publisher={Oxford University Press}\n}\n</code></pre></li> <li>Link: https://github.com/dmis-lab/biobert</li> <li>HuggingFace: https://huggingface.co/dmis-lab/biobert-base-cased-v1.1</li> </ul> <p>Note: When using BioBERT in research, please cite the original paper.</p>"},{"location":"about/license/#lexicon-licenses","title":"Lexicon Licenses","text":""},{"location":"about/license/#meddra","title":"MedDRA","text":"<p>The symptom lexicon (<code>data/lexicon/symptoms.csv</code>) is derived from MedDRA (Medical Dictionary for Regulatory Activities).</p> <ul> <li>License: MedDRA\u00ae trademark is registered by the International Federation of Pharmaceutical Manufacturers and Associations (IFPMA) on behalf of the International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH).</li> <li>Usage: MedDRA content is subject to licensing agreements. The derived lexicon in SpanForge uses publicly available information and does not redistribute proprietary MedDRA data.</li> <li>Link: https://www.meddra.org/</li> </ul> <p>Important: If you use MedDRA-derived data in production or research, ensure compliance with MedDRA licensing terms.</p>"},{"location":"about/license/#custom-lexicons","title":"Custom Lexicons","text":"<p>Product lexicon (<code>data/lexicon/products.csv</code>) is compiled from public sources and does not contain proprietary information.</p>"},{"location":"about/license/#data-privacy","title":"Data Privacy","text":""},{"location":"about/license/#user-data","title":"User Data","text":"<ul> <li>SpanForge does NOT collect or transmit user data</li> <li>All processing is local (no external API calls except HuggingFace model downloads)</li> <li>Label Studio telemetry is disabled (via <code>LABEL_STUDIO_DISABLE_TELEMETRY=1</code>)</li> </ul>"},{"location":"about/license/#complaints-data","title":"Complaints Data","text":"<ul> <li>Raw complaint texts are NOT included in the repository</li> <li>Example data is synthetic or de-identified</li> <li>Users must ensure compliance with privacy regulations (HIPAA, GDPR) when processing real data</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to SpanForge, you agree that your contributions will be licensed under the MIT License.</p> <p>See Contributing Guide for details.</p>"},{"location":"about/license/#disclaimer","title":"Disclaimer","text":"<p>NO WARRANTY</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. THE AUTHORS DISCLAIM ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.</p> <p>NO LIABILITY</p> <p>IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY ARISING FROM THE USE OF THIS SOFTWARE.</p> <p>MEDICAL USE</p> <p>THIS SOFTWARE IS FOR RESEARCH PURPOSES ONLY. IT IS NOT INTENDED FOR CLINICAL DIAGNOSIS, TREATMENT, OR REGULATORY DECISION-MAKING. USERS MUST VALIDATE RESULTS INDEPENDENTLY.</p>"},{"location":"about/license/#contact","title":"Contact","text":"<p>For licensing questions or commercial use inquiries, please contact:</p> <ul> <li>GitHub Issues: SpanForge Issues</li> <li>Email: (Add contact email if applicable)</li> </ul> <p>Last updated: January 2025</p>"},{"location":"about/roadmap/","title":"SpanForge Roadmap","text":"<p>Project roadmap and feature planning for biomedical NER pipeline.</p>"},{"location":"about/roadmap/#project-status","title":"Project Status","text":"<p>Current Phase: Documentation &amp; Infrastructure (Phase 4 Complete) \u2705 Test Coverage: 144/144 tests passing (100%) CI/CD: Active GitHub Actions workflows Next Phase: Annotation &amp; Curation (Phase 5)</p>"},{"location":"about/roadmap/#completed-phases","title":"Completed Phases","text":""},{"location":"about/roadmap/#phase-1-bootstrap-lexicon","title":"Phase 1: Bootstrap &amp; Lexicon \u2705","text":"<p>Completed: November 2024</p> <p>Objectives: - Repository setup and project structure - BioBERT model loading - Initial lexicon-based weak labeling - Core functionality implementation</p> <p>Deliverables: - \u2705 <code>src/config.py</code> - Configuration management - \u2705 <code>src/model.py</code> - BioBERT loader - \u2705 <code>src/weak_label.py</code> - Basic fuzzy matching - \u2705 <code>src/pipeline.py</code> - End-to-end inference - \u2705 <code>data/lexicon/symptoms.csv</code> - Symptom lexicon (MedDRA-derived) - \u2705 <code>data/lexicon/products.csv</code> - Product lexicon - \u2705 <code>requirements.txt</code> - Dependencies</p> <p>Key Achievements: - Fuzzy matching with RapidFuzz (WRatio \u226588) - Jaccard token-set filtering (\u226540) - Basic negation detection (forward only) - JSONL persistence</p>"},{"location":"about/roadmap/#phase-2-weak-label-refinement","title":"Phase 2: Weak Label Refinement \u2705","text":"<p>Completed: December 2024</p> <p>Objectives: - Improve weak labeling accuracy - Add advanced filters and heuristics - Expand negation detection</p> <p>Deliverables: - \u2705 Bidirectional negation (forward + backward windows) - \u2705 Last-token alignment filter - \u2705 Anatomy singleton filter - \u2705 Emoji and unicode handling - \u2705 Confidence scoring (0.8\u00d7fuzzy + 0.2\u00d7jaccard) - \u2705 Expanded negation token list (22 cues)</p> <p>Key Achievements: - Negation recall improved by ~30% (backward detection) - Emoji handling prevents span breakage - Anatomy filter reduces false positives by ~15% - Confidence scores well-calibrated for active learning</p>"},{"location":"about/roadmap/#phase-3-test-infrastructure-edge-cases","title":"Phase 3: Test Infrastructure &amp; Edge Cases \u2705","text":"<p>Completed: January 2025 (Commit 8307485)</p> <p>Objectives: - Comprehensive test coverage - Edge case validation - Test composition patterns</p> <p>Deliverables: - \u2705 144 tests (16 core, 98 edge cases, 26 integration, 4 curation) - \u2705 Test base classes with composition - \u2705 Unicode and emoji edge case tests - \u2705 Negation pattern tests (24 tests) - \u2705 Boundary condition tests (18 tests) - \u2705 Anatomy filter tests (15 tests) - \u2705 Validation and error tests (29 tests) - \u2705 Scale and performance tests (15 tests)</p> <p>Key Achievements: - 100% test pass rate - Edge cases documented and validated - Test composition eliminates duplication - Performance benchmarks established</p>"},{"location":"about/roadmap/#phase-4-cicd-integration","title":"Phase 4: CI/CD Integration \u2705","text":"<p>Completed: January 2025 (Commit dbc2ad8)</p> <p>Objectives: - Automated testing on push/PR - Pre-commit hooks - Configuration management - Documentation infrastructure</p> <p>Deliverables: - \u2705 GitHub Actions workflows (test.yml, pre-commit.yml) - \u2705 6 CI configurations (2 OS \u00d7 3 Python versions) - \u2705 Pre-commit hooks (pytest, formatting) - \u2705 pyproject.toml configuration - \u2705 README updates with badges - \u2705 MkDocs infrastructure with Material theme - \u2705 Comprehensive docstrings and type hints</p> <p>Key Achievements: - CI/CD pipeline fully automated - Pre-commit hooks enforce quality - Test matrix covers Python 3.9-3.11 on Ubuntu/Windows - Professional documentation site</p>"},{"location":"about/roadmap/#current-phase","title":"Current Phase","text":""},{"location":"about/roadmap/#phase-5-annotation-curation","title":"Phase 5: Annotation &amp; Curation \ud83d\udea7","text":"<p>Status: Planned (In Progress: Documentation Complete) Target: Q1 2025</p> <p>Objectives: - Integrate Label Studio for human annotation - Build annotation workflow and tooling - Implement provenance tracking - Quality assurance and agreement metrics</p> <p>Planned Deliverables:</p>"},{"location":"about/roadmap/#label-studio-setup","title":"Label Studio Setup","text":"<ul> <li> <code>data/annotation/config/label_config.xml</code> - Label config (SYMPTOM, PRODUCT)</li> <li> <code>scripts/annotation/init_label_studio_project.py</code> - Project bootstrap</li> <li> Telemetry disabled (privacy-safe setup)</li> </ul>"},{"location":"about/roadmap/#importexport-pipeline","title":"Import/Export Pipeline","text":"<ul> <li> <code>scripts/annotation/import_weak_to_labelstudio.py</code> - Weak label import</li> <li> <code>scripts/annotation/convert_labelstudio.py</code> - Export to gold JSONL</li> <li> Consensus/adjudication logic (majority vote, longest span)</li> </ul>"},{"location":"about/roadmap/#quality-assurance","title":"Quality Assurance","text":"<ul> <li> <code>scripts/annotation/quality_report.py</code> - Per-annotator stats, agreement</li> <li> <code>scripts/annotation/adjudicate.py</code> - Conflict resolution</li> <li> Inter-annotator agreement (IOU \u22650.5, Cohen's kappa)</li> </ul>"},{"location":"about/roadmap/#provenance-registry","title":"Provenance &amp; Registry","text":"<ul> <li> <code>scripts/annotation/register_batch.py</code> - Track batches, annotators</li> <li> <code>data/annotation/registry.csv</code> - Batch metadata</li> <li> Conflict collection (<code>data/annotation/conflicts/</code>)</li> </ul>"},{"location":"about/roadmap/#documentation","title":"Documentation","text":"<ul> <li>\u2705 <code>docs/annotation_guide.md</code> - Annotation guidelines (COMPLETE)</li> <li> <code>docs/tutorial_labeling.md</code> - Step-by-step tutorial</li> <li> <code>scripts/AnnotationWalkthrough.ipynb</code> - Interactive tutorial</li> <li> Glossary of symptom synonyms</li> </ul>"},{"location":"about/roadmap/#cli-tooling","title":"CLI Tooling","text":"<ul> <li> <code>scripts/annotation/cli.py</code> - Unified CLI (<code>bootstrap</code>, <code>import-weak</code>, <code>export-convert</code>, <code>quality</code>, <code>adjudicate</code>, <code>register</code>)</li> </ul> <p>Success Criteria: - &lt;5% conflicting overlaps after consensus - \u226590% canonical coverage - Annotator agreement (IOU \u22650.5) &gt;0.75 - Clean gold JSONL passes integrity tests</p> <p>Risks &amp; Mitigations: - Annotator overlap confusion \u2192 Visual examples &amp; highlight guidance - Bias from pre-annotated spans \u2192 Option to hide weak spans - Inconsistent boundaries \u2192 Boundary rules + integrity tests - Privacy concerns \u2192 Local-only data, telemetry disabled</p>"},{"location":"about/roadmap/#upcoming-phases","title":"Upcoming Phases","text":""},{"location":"about/roadmap/#phase-6-gold-standard-assembly","title":"Phase 6: Gold Standard Assembly","text":"<p>Status: Planned Target: Q1-Q2 2025</p> <p>Objectives: - Curate high-quality gold annotations (\u2265500 samples) - Define label schema (<code>labels.json</code>) - Split train/dev/test sets - Establish evaluation baselines</p> <p>Planned Work: - Annotate 500-1000 complaint texts - Consensus annotation (2-3 annotators/task) - Adjudication of conflicts - Dataset splits (70/15/15 train/dev/test) - Baseline weak labeling evaluation (P/R/F1)</p> <p>Deliverables: - <code>data/gold/train.jsonl</code> - Training set - <code>data/gold/dev.jsonl</code> - Development set - <code>data/gold/test.jsonl</code> - Test set (held out) - <code>data/labels.json</code> - Label schema (SYMPTOM, PRODUCT, O) - Evaluation metrics: precision, recall, F1 per label</p>"},{"location":"about/roadmap/#phase-7-token-classification-fine-tune","title":"Phase 7: Token Classification Fine-Tune","text":"<p>Status: Planned Target: Q2 2025</p> <p>Objectives: - Add classification head to BioBERT - Fine-tune on gold annotations - Hyperparameter tuning - Model evaluation and selection</p> <p>Planned Work: - <code>AutoModelForTokenClassification</code> wrapper - Training script with AdamW, learning rate scheduling - Hyperparameter search (LR, batch size, epochs) - Evaluation on dev/test sets - Model checkpointing and versioning</p> <p>Deliverables: - <code>src/trainer.py</code> - Training loop - <code>models/biobert-ner-v1/</code> - Fine-tuned checkpoint - <code>scripts/train.py</code> - Training CLI - <code>scripts/evaluate.py</code> - Evaluation script - Training logs and metrics</p> <p>Expected Metrics: - SYMPTOM: P ~85%, R ~80%, F1 ~82% - PRODUCT: P ~90%, R ~85%, F1 ~87% - Macro F1: ~84%</p>"},{"location":"about/roadmap/#phase-8-domain-adaptation","title":"Phase 8: Domain Adaptation","text":"<p>Status: Planned Target: Q2-Q3 2025</p> <p>Objectives: - Continued MLM pre-training on complaints corpus - Adapt BioBERT to colloquial language - Compare adapted vs. base BioBERT</p> <p>Planned Work: - De-identify complaint corpus (\u226510k texts) - Masked language modeling (MLM) on complaints - 1-3 epochs, batch size 16-32 - Evaluation: perplexity, downstream NER F1</p> <p>Deliverables: - <code>scripts/pretrain_mlm.py</code> - MLM training script - <code>models/biobert-complaints-adapted/</code> - Adapted checkpoint - Perplexity comparison report - NER performance before/after adaptation</p> <p>Expected Gains: - Perplexity reduction: ~10-15% - NER F1 improvement: ~2-4% - Better handling of misspellings, colloquialisms</p>"},{"location":"about/roadmap/#phase-9-baseline-comparison","title":"Phase 9: Baseline Comparison","text":"<p>Status: Planned Target: Q3 2025</p> <p>Objectives: - Train RoBERTa-base for comparison - Benchmark against BioBERT - Analyze trade-offs (biomedical vs. general domain)</p> <p>Planned Work: - Fine-tune <code>roberta-base</code> on same gold data - Compare BioBERT vs. RoBERTa on dev/test - Error analysis (medical terms, colloquialisms)</p> <p>Deliverables: - <code>models/roberta-ner-v1/</code> - RoBERTa checkpoint - Comparison report (P/R/F1, inference speed) - Error analysis notebook</p> <p>Expected Results: - BioBERT: Better on medical terms - RoBERTa: Better on colloquial language - Final choice: Ensemble or BioBERT-adapted</p>"},{"location":"about/roadmap/#phase-10-evaluation-calibration","title":"Phase 10: Evaluation &amp; Calibration","text":"<p>Status: Planned Target: Q3 2025</p> <p>Objectives: - Comprehensive error analysis - Confidence calibration - Threshold tuning for production</p> <p>Planned Work: - Error categorization (false positives, false negatives) - Confidence calibration curves - Threshold optimization (maximize F1 or balance P/R) - Per-label performance analysis</p> <p>Deliverables: - <code>docs/error_analysis.md</code> - Error taxonomy - Calibration plots - Optimal threshold recommendations - Per-label performance breakdown</p>"},{"location":"about/roadmap/#phase-11-educational-docs-expansion","title":"Phase 11: Educational Docs Expansion","text":"<p>Status: Partially Complete Target: Q4 2025</p> <p>Objectives: - Comprehensive user and developer docs - API reference - Tutorials and walkthroughs</p> <p>Planned Work: - \u2705 <code>docs/overview.md</code> - Architecture overview (COMPLETE) - \u2705 <code>docs/annotation_guide.md</code> - Annotation guidelines (COMPLETE) - \u2705 <code>docs/user-guide/weak-labeling.md</code> - Weak labeling guide (COMPLETE) - \u2705 <code>docs/user-guide/negation.md</code> - Negation guide (COMPLETE) - \u2705 <code>docs/user-guide/pipeline.md</code> - Pipeline guide (COMPLETE) - \u2705 <code>docs/development/testing.md</code> - Testing guide (COMPLETE) - [ ] <code>docs/heuristic.md</code> - Heuristic tuning guide - [ ] <code>docs/model_strategy.md</code> - Model selection guide - [ ] <code>docs/deployment.md</code> - Production deployment</p>"},{"location":"about/roadmap/#phase-12-continuous-improvement-active-learning","title":"Phase 12: Continuous Improvement &amp; Active Learning","text":"<p>Status: Planned Target: Ongoing (Q4 2025+)</p> <p>Objectives: - Active learning pipeline - Model monitoring and retraining - Feedback loop for continuous improvement</p> <p>Planned Work: - Active learning: prioritize uncertain samples - Human-in-the-loop annotation for edge cases - Periodic model retraining (monthly/quarterly) - Performance monitoring dashboard</p> <p>Deliverables: - <code>scripts/active_learning.py</code> - Uncertainty sampling - Monitoring dashboard (Streamlit or Grafana) - Retraining automation scripts - Performance drift detection</p>"},{"location":"about/roadmap/#feature-wishlist","title":"Feature Wishlist","text":""},{"location":"about/roadmap/#near-term-2025","title":"Near-Term (2025)","text":"<ul> <li> Multi-language support (Spanish, French)</li> <li> Relation extraction (symptom-product links)</li> <li> Severity classification (mild/moderate/severe)</li> <li> Temporal extraction (onset, duration)</li> </ul>"},{"location":"about/roadmap/#long-term-2026","title":"Long-Term (2026+)","text":"<ul> <li> Real-time inference API (FastAPI + Docker)</li> <li> Web-based annotation interface (custom UI)</li> <li> Integration with FAERS database</li> <li> Ensemble models (BioBERT + RoBERTa + ClinicalBERT)</li> <li> Zero-shot entity recognition (GPT-4 integration)</li> </ul>"},{"location":"about/roadmap/#contribution-opportunities","title":"Contribution Opportunities","text":"<p>Looking for contributors in:</p> <ol> <li>Annotation - Help curate gold standard dataset</li> <li>Documentation - Expand tutorials and examples</li> <li>Testing - Add edge cases and integration tests</li> <li>Feature Development - Implement roadmap items</li> <li>Research - Experiment with new models/techniques</li> </ol> <p>See Contributing Guide for details.</p>"},{"location":"about/roadmap/#version-history","title":"Version History","text":"Version Date Phase Highlights v0.1.0 Nov 2024 Phase 1 Initial release, weak labeling v0.2.0 Dec 2024 Phase 2 Bidirectional negation, filters v0.3.0 Jan 2025 Phase 3 144 tests, 100% pass rate v0.4.0 Jan 2025 Phase 4 CI/CD, MkDocs, docstrings v0.5.0 Q1 2025 Phase 5 Label Studio, annotation (planned) v1.0.0 Q2 2025 Phase 7 Fine-tuned NER model (planned)"},{"location":"about/roadmap/#contact-feedback","title":"Contact &amp; Feedback","text":"<p>Questions or suggestions? Open an issue on GitHub: SpanForge Issues</p> <p>Want to contribute? See Contributing Guide</p> <p>Last updated: January 2025</p>"},{"location":"api/config/","title":"Configuration API","text":"<p>Configuration management for SpanForge using Pydantic BaseSettings.</p>"},{"location":"api/config/#src.config","title":"src.config","text":"<p>Configuration management for SpanForge.</p> <p>This module provides centralized configuration using Pydantic BaseSettings, allowing both default values and environment variable overrides.</p>"},{"location":"api/config/#src.config.AppConfig","title":"AppConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application configuration with defaults and environment override support.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>HuggingFace model identifier for BioBERT base model</p> <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length for tokenization</p> <code>device</code> <code>str</code> <p>Computation device ('cuda' or 'cpu'), auto-detected if available</p> <code>seed</code> <code>int</code> <p>Random seed for reproducibility across runs</p> <code>negation_window</code> <code>int</code> <p>Number of tokens after negation cue to mark as negated</p> <code>fuzzy_scorer</code> <code>str</code> <p>Fuzzy matching algorithm ('wratio' or 'jaccard')</p> <code>llm_enabled</code> <code>bool</code> <p>Enable experimental LLM refinement pipeline</p> <code>llm_provider</code> <code>str</code> <p>LLM provider name ('stub', 'openai', 'azure', 'anthropic')</p> <code>llm_model</code> <code>str</code> <p>LLM model identifier</p> <code>llm_min_confidence</code> <code>float</code> <p>Minimum confidence threshold for LLM suggestions</p> <code>llm_cache_path</code> <code>str</code> <p>Path to LLM response cache file</p> <code>llm_prompt_version</code> <code>str</code> <p>Version identifier for prompt templates</p> Source code in <code>src\\config.py</code> <pre><code>class AppConfig(BaseSettings):\n    \"\"\"Application configuration with defaults and environment override support.\n\n    Attributes:\n        model_name: HuggingFace model identifier for BioBERT base model\n        max_seq_len: Maximum sequence length for tokenization\n        device: Computation device ('cuda' or 'cpu'), auto-detected if available\n        seed: Random seed for reproducibility across runs\n        negation_window: Number of tokens after negation cue to mark as negated\n        fuzzy_scorer: Fuzzy matching algorithm ('wratio' or 'jaccard')\n        llm_enabled: Enable experimental LLM refinement pipeline\n        llm_provider: LLM provider name ('stub', 'openai', 'azure', 'anthropic')\n        llm_model: LLM model identifier\n        llm_min_confidence: Minimum confidence threshold for LLM suggestions\n        llm_cache_path: Path to LLM response cache file\n        llm_prompt_version: Version identifier for prompt templates\n    \"\"\"\n    model_name: str = \"dmis-lab/biobert-base-cased-v1.1\"\n    max_seq_len: int = 256\n    device: str = \"cuda\" if (torch and torch.cuda.is_available()) else \"cpu\"\n    seed: int = 42\n    negation_window: int = 5\n    fuzzy_scorer: str = \"wratio\"\n    llm_enabled: bool = False\n    llm_provider: str = \"stub\"\n    llm_model: str = \"gpt-4\"\n    llm_min_confidence: float = 0.65\n    llm_cache_path: str = \"data/annotation/exports/llm_cache.jsonl\"\n    llm_prompt_version: str = \"v1\"\n</code></pre>"},{"location":"api/config/#src.config.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; AppConfig\n</code></pre> <p>Get application configuration instance.</p> <p>Returns:</p> Type Description <code>AppConfig</code> <p>AppConfig instance with default or environment-overridden values</p> Example <p>config = get_config() print(config.model_name) 'dmis-lab/biobert-base-cased-v1.1'</p> Source code in <code>src\\config.py</code> <pre><code>def get_config() -&gt; AppConfig:\n    \"\"\"Get application configuration instance.\n\n    Returns:\n        AppConfig instance with default or environment-overridden values\n\n    Example:\n        &gt;&gt;&gt; config = get_config()\n        &gt;&gt;&gt; print(config.model_name)\n        'dmis-lab/biobert-base-cased-v1.1'\n    \"\"\"\n    return AppConfig()\n</code></pre>"},{"location":"api/config/#src.config.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int) -&gt; None\n</code></pre> <p>Set random seeds for reproducibility across libraries.</p> <p>Sets seeds for Python random, NumPy, and PyTorch (CPU and CUDA).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Integer seed value for all random number generators</p> required Example <p>set_seed(42)</p> Source code in <code>src\\config.py</code> <pre><code>def set_seed(seed: int) -&gt; None:\n    \"\"\"Set random seeds for reproducibility across libraries.\n\n    Sets seeds for Python random, NumPy, and PyTorch (CPU and CUDA).\n\n    Args:\n        seed: Integer seed value for all random number generators\n\n    Example:\n        &gt;&gt;&gt; set_seed(42)\n        &gt;&gt;&gt; # All subsequent random operations will be reproducible\n    \"\"\"\n    import random\n    import numpy as np\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch is not None:\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/config/#src.config.set_seed--all-subsequent-random-operations-will-be-reproducible","title":"All subsequent random operations will be reproducible","text":""},{"location":"api/llm_agent/","title":"LLM Agent API","text":"<p>LLM-based span refinement and enhancement (experimental).</p>"},{"location":"api/llm_agent/#src.llm_agent","title":"src.llm_agent","text":"<p>LLM agent for span refinement and enhancement.</p> <p>Provides stub implementation for LLM-based entity span refinement. Designed for future integration with OpenAI, Azure, or Anthropic providers.</p>"},{"location":"api/llm_agent/#src.llm_agent.LLMSuggestion","title":"LLMSuggestion  <code>dataclass</code>","text":"<p>LLM-generated span suggestion with confidence and reasoning.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Character start position of suggested span</p> <code>end</code> <code>int</code> <p>Character end position of suggested span</p> <code>label</code> <code>str</code> <p>Entity label (SYMPTOM or PRODUCT)</p> <code>negated</code> <code>bool | None</code> <p>Whether the entity is negated (optional)</p> <code>canonical</code> <code>str | None</code> <p>Canonical form of the entity (optional)</p> <code>confidence_reason</code> <code>str | None</code> <p>Textual explanation for confidence score (optional)</p> <code>llm_confidence</code> <code>float | None</code> <p>Confidence score from LLM (optional)</p> Source code in <code>src\\llm_agent.py</code> <pre><code>@dataclass\nclass LLMSuggestion:\n    \"\"\"LLM-generated span suggestion with confidence and reasoning.\n\n    Attributes:\n        start: Character start position of suggested span\n        end: Character end position of suggested span\n        label: Entity label (SYMPTOM or PRODUCT)\n        negated: Whether the entity is negated (optional)\n        canonical: Canonical form of the entity (optional)\n        confidence_reason: Textual explanation for confidence score (optional)\n        llm_confidence: Confidence score from LLM (optional)\n    \"\"\"\n    start: int\n    end: int\n    label: str\n    negated: bool | None = None\n    canonical: str | None = None\n    confidence_reason: str | None = None\n    llm_confidence: float | None = None\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent","title":"LLMAgent","text":"<p>Stub LLM agent for entity span refinement.</p> <p>Provides interface for LLM-based span suggestion and refinement. Current implementation is a stub returning empty suggestions for deterministic testing. Extend with real provider integration later.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>str</code> <p>LLM provider name ('stub', 'openai', 'azure', 'anthropic')</p> <code>model</code> <code>str</code> <p>Model identifier for the LLM</p> <code>min_conf</code> <code>float</code> <p>Minimum confidence threshold for accepting suggestions</p> Example <p>agent = LLMAgent() suggestions = agent.suggest(template, text, spans, knowledge)</p> Source code in <code>src\\llm_agent.py</code> <pre><code>class LLMAgent:\n    \"\"\"Stub LLM agent for entity span refinement.\n\n    Provides interface for LLM-based span suggestion and refinement.\n    Current implementation is a stub returning empty suggestions for\n    deterministic testing. Extend with real provider integration later.\n\n    Attributes:\n        provider: LLM provider name ('stub', 'openai', 'azure', 'anthropic')\n        model: Model identifier for the LLM\n        min_conf: Minimum confidence threshold for accepting suggestions\n\n    Example:\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; suggestions = agent.suggest(template, text, spans, knowledge)\n        &gt;&gt;&gt; # Returns empty list in stub mode\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize LLM agent with configuration.\"\"\"\n        cfg = get_config()\n        self.provider: str = cfg.llm_provider\n        self.model: str = cfg.llm_model\n        self.min_conf: float = cfg.llm_min_confidence\n\n    def format_prompt(\n        self, \n        template: str, \n        text: str, \n        spans: List[Dict[str, Any]], \n        knowledge: Dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Format prompt template with text, spans, and knowledge.\n\n        Args:\n            template: Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders\n            text: Source text being analyzed\n            spans: List of candidate spans to refine\n            knowledge: Additional domain knowledge dictionary\n\n        Returns:\n            Formatted prompt string ready for LLM\n        \"\"\"\n        candidates = [\n            f\"{s['text']} [{s['start']},{s['end']}] {s['label']} conf={s.get('confidence',1):.2f}\" \n            for s in spans\n        ]\n        return (template\n                .replace(\"{{text}}\", text)\n                .replace(\"{{candidates}}\", \"\\n\".join(candidates))\n                .replace(\"{{knowledge}}\", json.dumps(knowledge, ensure_ascii=False)))\n\n    def call(self, prompt: str) -&gt; str:\n        \"\"\"Call LLM with formatted prompt (stub implementation).\n\n        Args:\n            prompt: Formatted prompt string\n\n        Returns:\n            JSON string with LLM response (empty in stub mode)\n\n        Note:\n            Stub returns empty suggestions for deterministic testing.\n            Extend this method to integrate real LLM providers.\n        \"\"\"\n        # Stub: return empty JSON structure (no suggestions) for deterministic tests\n        return json.dumps({\"spans\": [], \"notes\": \"stub\"})\n\n    def parse(self, response: str) -&gt; Dict[str, Any]:\n        \"\"\"Parse LLM response JSON string.\n\n        Args:\n            response: JSON string from LLM\n\n        Returns:\n            Parsed dictionary with 'spans' list and optional 'notes'\n        \"\"\"\n        try:\n            data = json.loads(response)\n            if \"spans\" not in data:\n                data[\"spans\"] = []\n            return data\n        except Exception:\n            return {\"spans\": [], \"notes\": \"parse_error\"}\n\n    def suggest(\n        self, \n        template: str, \n        text: str, \n        spans: List[Dict[str, Any]], \n        knowledge: Dict[str, Any]\n    ) -&gt; List[LLMSuggestion]:\n        \"\"\"Generate span suggestions using LLM.\n\n        Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n        Args:\n            template: Prompt template string\n            text: Source text being analyzed\n            spans: List of candidate span dictionaries\n            knowledge: Domain knowledge dictionary\n\n        Returns:\n            List of LLMSuggestion objects meeting confidence threshold\n\n        Example:\n            &gt;&gt;&gt; agent = LLMAgent()\n            &gt;&gt;&gt; template = \"Analyze: {{text}}\\nCandidates: {{candidates}}\"\n            &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})\n            &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode\n            0\n        \"\"\"\n        prompt = self.format_prompt(template, text, spans, knowledge)\n        raw = self.call(prompt)\n        parsed = self.parse(raw)\n        suggestions: List[LLMSuggestion] = []\n        for s in parsed.get('spans', []):\n            try:\n                ls = LLMSuggestion(\n                    start=int(s['start']),\n                    end=int(s['end']),\n                    label=str(s['label']),\n                    negated=bool(s.get('negated', False)),\n                    canonical=s.get('canonical'),\n                    confidence_reason=s.get('confidence_reason'),\n                    llm_confidence=float(s.get('llm_confidence', self.min_conf))\n                )\n                if ls.llm_confidence &gt;= self.min_conf:\n                    suggestions.append(ls)\n            except Exception:\n                continue\n        return suggestions\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent--returns-empty-list-in-stub-mode","title":"Returns empty list in stub mode","text":""},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize LLM agent with configuration.</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize LLM agent with configuration.\"\"\"\n    cfg = get_config()\n    self.provider: str = cfg.llm_provider\n    self.model: str = cfg.llm_model\n    self.min_conf: float = cfg.llm_min_confidence\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.format_prompt","title":"format_prompt","text":"<pre><code>format_prompt(\n    template: str,\n    text: str,\n    spans: List[Dict[str, Any]],\n    knowledge: Dict[str, Any],\n) -&gt; str\n</code></pre> <p>Format prompt template with text, spans, and knowledge.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders</p> required <code>text</code> <code>str</code> <p>Source text being analyzed</p> required <code>spans</code> <code>List[Dict[str, Any]]</code> <p>List of candidate spans to refine</p> required <code>knowledge</code> <code>Dict[str, Any]</code> <p>Additional domain knowledge dictionary</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string ready for LLM</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def format_prompt(\n    self, \n    template: str, \n    text: str, \n    spans: List[Dict[str, Any]], \n    knowledge: Dict[str, Any]\n) -&gt; str:\n    \"\"\"Format prompt template with text, spans, and knowledge.\n\n    Args:\n        template: Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders\n        text: Source text being analyzed\n        spans: List of candidate spans to refine\n        knowledge: Additional domain knowledge dictionary\n\n    Returns:\n        Formatted prompt string ready for LLM\n    \"\"\"\n    candidates = [\n        f\"{s['text']} [{s['start']},{s['end']}] {s['label']} conf={s.get('confidence',1):.2f}\" \n        for s in spans\n    ]\n    return (template\n            .replace(\"{{text}}\", text)\n            .replace(\"{{candidates}}\", \"\\n\".join(candidates))\n            .replace(\"{{knowledge}}\", json.dumps(knowledge, ensure_ascii=False)))\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.call","title":"call","text":"<pre><code>call(prompt: str) -&gt; str\n</code></pre> <p>Call LLM with formatted prompt (stub implementation).</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Formatted prompt string</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON string with LLM response (empty in stub mode)</p> Note <p>Stub returns empty suggestions for deterministic testing. Extend this method to integrate real LLM providers.</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def call(self, prompt: str) -&gt; str:\n    \"\"\"Call LLM with formatted prompt (stub implementation).\n\n    Args:\n        prompt: Formatted prompt string\n\n    Returns:\n        JSON string with LLM response (empty in stub mode)\n\n    Note:\n        Stub returns empty suggestions for deterministic testing.\n        Extend this method to integrate real LLM providers.\n    \"\"\"\n    # Stub: return empty JSON structure (no suggestions) for deterministic tests\n    return json.dumps({\"spans\": [], \"notes\": \"stub\"})\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; Dict[str, Any]\n</code></pre> <p>Parse LLM response JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>JSON string from LLM</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed dictionary with 'spans' list and optional 'notes'</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def parse(self, response: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse LLM response JSON string.\n\n    Args:\n        response: JSON string from LLM\n\n    Returns:\n        Parsed dictionary with 'spans' list and optional 'notes'\n    \"\"\"\n    try:\n        data = json.loads(response)\n        if \"spans\" not in data:\n            data[\"spans\"] = []\n        return data\n    except Exception:\n        return {\"spans\": [], \"notes\": \"parse_error\"}\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.suggest","title":"suggest","text":"<pre><code>suggest(\n    template: str,\n    text: str,\n    spans: List[Dict[str, Any]],\n    knowledge: Dict[str, Any],\n) -&gt; List[LLMSuggestion]\n</code></pre> <p>Generate span suggestions using LLM.</p> <pre><code>    Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n    Args:\n        template: Prompt template string\n        text: Source text being analyzed\n        spans: List of candidate span dictionaries\n        knowledge: Domain knowledge dictionary\n\n    Returns:\n        List of LLMSuggestion objects meeting confidence threshold\n\n    Example:\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; template = \"Analyze: {{text}}\n</code></pre> <p>Candidates: {{candidates}}\"             &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})             &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode             0</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def suggest(\n    self, \n    template: str, \n    text: str, \n    spans: List[Dict[str, Any]], \n    knowledge: Dict[str, Any]\n) -&gt; List[LLMSuggestion]:\n    \"\"\"Generate span suggestions using LLM.\n\n    Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n    Args:\n        template: Prompt template string\n        text: Source text being analyzed\n        spans: List of candidate span dictionaries\n        knowledge: Domain knowledge dictionary\n\n    Returns:\n        List of LLMSuggestion objects meeting confidence threshold\n\n    Example:\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; template = \"Analyze: {{text}}\\nCandidates: {{candidates}}\"\n        &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})\n        &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode\n        0\n    \"\"\"\n    prompt = self.format_prompt(template, text, spans, knowledge)\n    raw = self.call(prompt)\n    parsed = self.parse(raw)\n    suggestions: List[LLMSuggestion] = []\n    for s in parsed.get('spans', []):\n        try:\n            ls = LLMSuggestion(\n                start=int(s['start']),\n                end=int(s['end']),\n                label=str(s['label']),\n                negated=bool(s.get('negated', False)),\n                canonical=s.get('canonical'),\n                confidence_reason=s.get('confidence_reason'),\n                llm_confidence=float(s.get('llm_confidence', self.min_conf))\n            )\n            if ls.llm_confidence &gt;= self.min_conf:\n                suggestions.append(ls)\n        except Exception:\n            continue\n    return suggestions\n</code></pre>"},{"location":"api/model/","title":"Model API","text":"<p>BioBERT model loading and text encoding utilities.</p>"},{"location":"api/model/#src.model","title":"src.model","text":"<p>BioBERT model loading and text encoding utilities.</p> <p>Provides cached model and tokenizer loading with GPU support. Uses singleton pattern to avoid reloading models on repeated calls.</p>"},{"location":"api/model/#src.model.get_tokenizer","title":"get_tokenizer","text":"<pre><code>get_tokenizer(config: AppConfig) -&gt; AutoTokenizer\n</code></pre> <p>Get or load BioBERT tokenizer with caching.</p> <p>Uses singleton pattern to cache tokenizer after first load.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AppConfig</code> <p>Application configuration containing model_name</p> required <p>Returns:</p> Type Description <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance for BioBERT</p> Example <p>config = AppConfig() tokenizer = get_tokenizer(config) tokens = tokenizer.tokenize(\"Patient has itching\")</p> Source code in <code>src\\model.py</code> <pre><code>def get_tokenizer(config: AppConfig) -&gt; AutoTokenizer:\n    \"\"\"Get or load BioBERT tokenizer with caching.\n\n    Uses singleton pattern to cache tokenizer after first load.\n\n    Args:\n        config: Application configuration containing model_name\n\n    Returns:\n        PreTrainedTokenizer instance for BioBERT\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; tokenizer = get_tokenizer(config)\n        &gt;&gt;&gt; tokens = tokenizer.tokenize(\"Patient has itching\")\n    \"\"\"\n    global _tokenizer\n    if _tokenizer is None:\n        _tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    return _tokenizer\n</code></pre>"},{"location":"api/model/#src.model.get_model","title":"get_model","text":"<pre><code>get_model(config: AppConfig) -&gt; AutoModel\n</code></pre> <p>Get or load BioBERT model with caching and device placement.</p> <p>Uses singleton pattern to cache model after first load. Automatically moves model to configured device (CPU/GPU). Sets model to evaluation mode.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AppConfig</code> <p>Application configuration containing model_name and device</p> required <p>Returns:</p> Type Description <code>AutoModel</code> <p>PreTrainedModel instance for BioBERT in eval mode</p> Example <p>config = AppConfig() model = get_model(config)</p> Source code in <code>src\\model.py</code> <pre><code>def get_model(config: AppConfig) -&gt; AutoModel:\n    \"\"\"Get or load BioBERT model with caching and device placement.\n\n    Uses singleton pattern to cache model after first load.\n    Automatically moves model to configured device (CPU/GPU).\n    Sets model to evaluation mode.\n\n    Args:\n        config: Application configuration containing model_name and device\n\n    Returns:\n        PreTrainedModel instance for BioBERT in eval mode\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; model = get_model(config)\n        &gt;&gt;&gt; # Model is on correct device and in eval mode\n    \"\"\"\n    global _model\n    if _model is None:\n        _model = AutoModel.from_pretrained(config.model_name)\n        _model.to(config.device)\n        _model.eval()\n    return _model\n</code></pre>"},{"location":"api/model/#src.model.get_model--model-is-on-correct-device-and-in-eval-mode","title":"Model is on correct device and in eval mode","text":""},{"location":"api/model/#src.model.encode_text","title":"encode_text","text":"<pre><code>encode_text(\n    text: str, tokenizer: AutoTokenizer, max_len: int\n) -&gt; BatchEncoding\n</code></pre> <p>Encode text string to model input format.</p> <p>Tokenizes and encodes text with truncation and tensor conversion.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text string to encode</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance</p> required <code>max_len</code> <code>int</code> <p>Maximum sequence length for truncation</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>BatchEncoding with input_ids, attention_mask, and token_type_ids</p> Example <p>config = AppConfig() tokenizer = get_tokenizer(config) encoding = encode_text(\"Test text\", tokenizer, 256) print(encoding.keys()) dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])</p> Source code in <code>src\\model.py</code> <pre><code>def encode_text(text: str, tokenizer: AutoTokenizer, max_len: int) -&gt; BatchEncoding:\n    \"\"\"Encode text string to model input format.\n\n    Tokenizes and encodes text with truncation and tensor conversion.\n\n    Args:\n        text: Input text string to encode\n        tokenizer: PreTrainedTokenizer instance\n        max_len: Maximum sequence length for truncation\n\n    Returns:\n        BatchEncoding with input_ids, attention_mask, and token_type_ids\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; tokenizer = get_tokenizer(config)\n        &gt;&gt;&gt; encoding = encode_text(\"Test text\", tokenizer, 256)\n        &gt;&gt;&gt; print(encoding.keys())\n        dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n    \"\"\"\n    return tokenizer(\n        text,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>End-to-end inference pipeline combining BioBERT and weak labeling.</p>"},{"location":"api/pipeline/#src.pipeline","title":"src.pipeline","text":"<p>End-to-end inference pipeline combining BioBERT and weak labeling.</p> <p>Provides batch processing with model inference, weak labeling, and optional persistence to JSONL format.</p>"},{"location":"api/pipeline/#src.pipeline.tokenize_batch","title":"tokenize_batch","text":"<pre><code>tokenize_batch(\n    texts: List[str], tokenizer: AutoTokenizer, max_len: int\n) -&gt; BatchEncoding\n</code></pre> <p>Tokenize batch of texts for model input.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input text strings</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance</p> required <code>max_len</code> <code>int</code> <p>Maximum sequence length for truncation</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>BatchEncoding with padded and truncated sequences</p> Source code in <code>src\\pipeline.py</code> <pre><code>def tokenize_batch(texts: List[str], tokenizer: AutoTokenizer, max_len: int) -&gt; BatchEncoding:\n    \"\"\"Tokenize batch of texts for model input.\n\n    Args:\n        texts: List of input text strings\n        tokenizer: PreTrainedTokenizer instance\n        max_len: Maximum sequence length for truncation\n\n    Returns:\n        BatchEncoding with padded and truncated sequences\n    \"\"\"\n    return tokenizer(\n        texts,\n        truncation=True,\n        max_length=max_len,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.predict_tokens","title":"predict_tokens","text":"<pre><code>predict_tokens(\n    model: Any, encodings: BatchEncoding, device: str\n) -&gt; Dict[str, Any]\n</code></pre> <p>Run model inference on encoded batch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>PreTrainedModel instance</p> required <code>encodings</code> <code>BatchEncoding</code> <p>BatchEncoding from tokenizer</p> required <code>device</code> <code>str</code> <p>Device string ('cuda' or 'cpu')</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing last_hidden_state from model output</p> Source code in <code>src\\pipeline.py</code> <pre><code>def predict_tokens(model: Any, encodings: BatchEncoding, device: str) -&gt; Dict[str, Any]:\n    \"\"\"Run model inference on encoded batch.\n\n    Args:\n        model: PreTrainedModel instance\n        encodings: BatchEncoding from tokenizer\n        device: Device string ('cuda' or 'cpu')\n\n    Returns:\n        Dictionary containing last_hidden_state from model output\n    \"\"\"\n    with torch.no_grad():\n        outputs = model(**{k: v.to(device) for k, v in encodings.items()})\n    return {\"last_hidden_state\": outputs.last_hidden_state}\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.postprocess_predictions","title":"postprocess_predictions","text":"<pre><code>postprocess_predictions(\n    batch_tokens: List[List[str]],\n    model_outputs: Dict[str, Any],\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Postprocess model outputs (placeholder for future expansion).</p> <p>Parameters:</p> Name Type Description Default <code>batch_tokens</code> <code>List[List[str]]</code> <p>List of tokenized texts</p> required <code>model_outputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing model predictions</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries with basic token count</p> Note <p>This is a placeholder for future lexicon-based span extraction.</p> Source code in <code>src\\pipeline.py</code> <pre><code>def postprocess_predictions(batch_tokens: List[List[str]], model_outputs: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\n    \"\"\"Postprocess model outputs (placeholder for future expansion).\n\n    Args:\n        batch_tokens: List of tokenized texts\n        model_outputs: Dictionary containing model predictions\n\n    Returns:\n        List of dictionaries with basic token count\n\n    Note:\n        This is a placeholder for future lexicon-based span extraction.\n    \"\"\"\n    # Placeholder: extend with lexicon match, span extraction, normalization.\n    return [{\"token_count\": len(tokens)} for tokens in batch_tokens]\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.simple_inference","title":"simple_inference","text":"<pre><code>simple_inference(\n    texts: List[str], persist_path: Optional[str] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Run end-to-end inference pipeline with weak labeling.</p> <p>Combines BioBERT inference with lexicon-based weak labeling. Optionally persists results to JSONL format.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input text strings to analyze</p> required <code>persist_path</code> <code>Optional[str]</code> <p>Optional path to save weak labels in JSONL format</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing: - token_count: Number of tokens in text - weak_spans: List of detected symptom/product spans with metadata</p> Example <p>texts = [\"Patient has severe rash and headache\"] results = simple_inference(texts, persist_path=\"output.jsonl\") print(results[0]['weak_spans']) [{'text': 'rash', 'label': 'SYMPTOM', ...}]</p> Source code in <code>src\\pipeline.py</code> <pre><code>def simple_inference(texts: List[str], persist_path: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    \"\"\"Run end-to-end inference pipeline with weak labeling.\n\n    Combines BioBERT inference with lexicon-based weak labeling.\n    Optionally persists results to JSONL format.\n\n    Args:\n        texts: List of input text strings to analyze\n        persist_path: Optional path to save weak labels in JSONL format\n\n    Returns:\n        List of dictionaries containing:\n            - token_count: Number of tokens in text\n            - weak_spans: List of detected symptom/product spans with metadata\n\n    Example:\n        &gt;&gt;&gt; texts = [\"Patient has severe rash and headache\"]\n        &gt;&gt;&gt; results = simple_inference(texts, persist_path=\"output.jsonl\")\n        &gt;&gt;&gt; print(results[0]['weak_spans'])\n        [{'text': 'rash', 'label': 'SYMPTOM', ...}]\n    \"\"\"\n    config = AppConfig()\n    tokenizer = get_tokenizer(config)\n    model = get_model(config)\n    encodings = tokenize_batch(texts, tokenizer, config.max_seq_len)\n    outputs = predict_tokens(model, encodings, config.device)\n    batch_tokens = [tokenizer.tokenize(t) for t in texts]\n    base = postprocess_predictions(batch_tokens, outputs)\n\n    # Load lexicons\n    symptom_lex_path = Path(\"data/lexicon/symptoms.csv\")\n    product_lex_path = Path(\"data/lexicon/products.csv\")\n    symptom_lexicon = load_symptom_lexicon(symptom_lex_path)\n    product_lexicon = load_product_lexicon(product_lex_path)\n\n    # Weak labeling with config params\n    wl = weak_label_batch(\n        texts, symptom_lexicon, product_lexicon,\n        negation_window=config.negation_window,\n        scorer=config.fuzzy_scorer\n    ) if (symptom_lexicon or product_lexicon) else [[] for _ in texts]\n\n    # Persist if requested\n    if persist_path:\n        persist_weak_labels_jsonl(texts, wl, Path(persist_path))\n\n    # Merge\n    for rec, spans in zip(base, wl):\n        rec[\"weak_spans\"] = [\n            {\n                \"text\": s.text,\n                \"start\": s.start,\n                \"end\": s.end,\n                \"label\": s.label,\n                \"canonical\": s.canonical,\n                \"sku\": s.sku,\n                \"category\": s.category,\n                \"confidence\": s.confidence,\n                \"negated\": s.negated,\n            } for s in spans\n        ]\n    return base\n</code></pre>"},{"location":"api/weak_label/","title":"Weak Label API","text":"<p>Lexicon-based weak labeling with fuzzy matching and negation detection.</p>"},{"location":"api/weak_label/#src.weak_label","title":"src.weak_label","text":"<p>Weak labeling for biomedical named entity recognition.</p> <p>This module implements lexicon-based fuzzy matching with rule-based filters for automated span extraction. Suitable for bootstrapping annotation, active learning, and evaluation baselines.</p> Key Features <ul> <li>Fuzzy matching with RapidFuzz (WRatio \u226588, Jaccard \u226540)</li> <li>Bidirectional negation detection (forward + backward windows)</li> <li>Last-token alignment filter (prevents partial-word matches)</li> <li>Anatomy singleton filter (rejects generic anatomy mentions)</li> <li>Emoji and unicode handling (robust multi-byte support)</li> <li>Confidence scoring (0.8\u00d7fuzzy + 0.2\u00d7jaccard)</li> </ul> Typical Usage <p>from src.weak_label import match_symptoms, load_symptom_lexicon lexicon_entries = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\")) lexicon_terms = [entry.term for entry in lexicon_entries] text = \"Patient has severe itching\" spans = match_symptoms(text, lexicon_terms) print(spans[0][\"text\"], spans[0][\"label\"], spans[0][\"confidence\"]) severe itching SYMPTOM 0.92</p> See Also <ul> <li>User Guide: docs/user-guide/weak-labeling.md</li> <li>Negation Guide: docs/user-guide/negation.md</li> <li>API Reference: docs/api/weak_label.md</li> </ul>"},{"location":"api/weak_label/#src.weak_label.detect_negated_regions","title":"detect_negated_regions","text":"<pre><code>detect_negated_regions(\n    text: str, window: int = 5\n) -&gt; List[Tuple[int, int]]\n</code></pre> <p>Phase 3: Enhanced negation detection with forward/backward windows and prefix matching.</p> <p>Forward window: negation precedes symptom (e.g., \"no itching\") Backward window: negation follows symptom (e.g., \"itching absent\")</p>"},{"location":"api/weak_label/#src.weak_label.persist_weak_labels_jsonl","title":"persist_weak_labels_jsonl","text":"<pre><code>persist_weak_labels_jsonl(\n    texts: List[str],\n    spans_batch: List[List[Span]],\n    output_path: Path,\n) -&gt; None\n</code></pre> <p>Persist weak labels to JSONL for annotation triage.</p>"},{"location":"development/contributing/","title":"Contributing to SpanForge","text":"<p>Thank you for your interest in contributing to SpanForge! This guide will help you get started.</p>"},{"location":"development/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Development Setup</li> <li>Contribution Workflow</li> <li>Code Standards</li> <li>Testing Requirements</li> <li>Documentation</li> <li>Pull Request Process</li> <li>Contribution Areas</li> <li>Community Guidelines</li> </ul>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9, 3.10, or 3.11</li> <li>Git</li> <li>Virtual environment tool (venv or conda)</li> <li>Familiarity with PyTorch and transformers</li> </ul>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone repository\ngit clone https://github.com/your-org/spanforge.git\ncd spanforge\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or\nvenv\\Scripts\\activate  # Windows\n\n# Install dependencies (dev mode)\npip install -r requirements.txt\npip install -r docs-requirements.txt\npip install pre-commit pytest pytest-cov black isort\n\n# Verify setup\npython scripts/verify_env.py\npytest -v\n</code></pre>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":""},{"location":"development/contributing/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<p>Pre-commit hooks ensure code quality before commits:</p> <pre><code>pre-commit install\n</code></pre> <p>Hooks will now run automatically on <code>git commit</code>: - pytest (all tests must pass) - Code formatting checks</p>"},{"location":"development/contributing/#configure-ide","title":"Configure IDE","text":""},{"location":"development/contributing/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - Python (Microsoft) - Pylance - Jupyter - GitLens</p> <p>Recommended settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\"-v\"],\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": false,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"editor.formatOnSave\": true,\n  \"[python]\": {\n    \"editor.codeActionsOnSave\": {\n      \"source.organizeImports\": true\n    }\n  }\n}\n</code></pre>"},{"location":"development/contributing/#pycharm","title":"PyCharm","text":"<ul> <li>Enable pytest as test runner (Settings \u2192 Tools \u2192 Python Integrated Tools)</li> <li>Configure Black as formatter (Settings \u2192 Tools \u2192 Black)</li> <li>Enable type checking (Settings \u2192 Editor \u2192 Inspections \u2192 Python)</li> </ul>"},{"location":"development/contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"development/contributing/#1-create-issue-optional-but-recommended","title":"1. Create Issue (Optional but Recommended)","text":"<p>Before starting work, create an issue describing: - Problem or feature - Proposed solution - Expected impact</p> <p>Wait for maintainer feedback before starting large changes.</p>"},{"location":"development/contributing/#2-fork-and-branch","title":"2. Fork and Branch","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/your-username/spanforge.git\ncd spanforge\n\n# Add upstream remote\ngit remote add upstream https://github.com/original-org/spanforge.git\n\n# Create feature branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#3-make-changes","title":"3. Make Changes","text":"<ul> <li>Write code following Code Standards</li> <li>Add tests for new functionality</li> <li>Update documentation</li> <li>Commit regularly with clear messages</li> </ul>"},{"location":"development/contributing/#4-test-locally","title":"4. Test Locally","text":"<pre><code># Run all tests\npytest -v\n\n# Run specific test categories\npytest tests/test_weak_label_core.py -v\n\n# Check coverage\npytest --cov=src --cov-report=html\n\n# Build documentation\ncd docs\nmkdocs serve\n</code></pre>"},{"location":"development/contributing/#5-commit","title":"5. Commit","text":"<pre><code># Stage changes\ngit add .\n\n# Commit (pre-commit hooks will run)\ngit commit -m \"feat: add bidirectional negation detection\"\n\n# If hooks fail, fix issues and commit again\n</code></pre>"},{"location":"development/contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code># Push to your fork\ngit push origin feature/your-feature-name\n\n# Create pull request on GitHub\n</code></pre>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#style-guide","title":"Style Guide","text":"<p>Follow PEP 8 with these specifics:</p> <ul> <li>Line length: 100 characters (not 88 like Black default)</li> <li>Indentation: 4 spaces</li> <li>Quotes: Double quotes <code>\"</code> for strings</li> <li>Imports: Organize with isort (automatic)</li> </ul>"},{"location":"development/contributing/#formatting","title":"Formatting","text":"<p>Use Black for automatic formatting:</p> <pre><code># Format all files\nblack src tests\n\n# Check without formatting\nblack --check src tests\n</code></pre>"},{"location":"development/contributing/#import-organization","title":"Import Organization","text":"<p>Use isort for import sorting:</p> <pre><code># Sort imports\nisort src tests\n\n# Check without sorting\nisort --check-only src tests\n</code></pre> <p>Import order: 1. Standard library 2. Third-party packages 3. Local modules</p> <pre><code># Standard library\nimport os\nimport sys\nfrom typing import List, Optional\n\n# Third-party\nimport torch\nfrom transformers import AutoModel\n\n# Local\nfrom src.config import AppConfig\nfrom src.model import get_model\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Always use type hints for function signatures:</p> <pre><code>from typing import List, Dict, Optional\n\ndef match_symptoms(\n    text: str,\n    lexicon: List[str],\n    fuzzy_threshold: float = 88.0,\n    negation_window: int = 5\n) -&gt; List[Dict[str, any]]:\n    \"\"\"\n    Match symptoms in text using fuzzy matching.\n\n    Args:\n        text: Input text to search\n        lexicon: List of symptom terms\n        fuzzy_threshold: Minimum fuzzy score (0-100)\n        negation_window: Negation scope in tokens\n\n    Returns:\n        List of matched span dictionaries\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def example_function(arg1: str, arg2: int = 0) -&gt; bool:\n    \"\"\"\n    Short one-line description.\n\n    Longer description explaining functionality, edge cases,\n    and important details.\n\n    Args:\n        arg1: Description of arg1\n        arg2: Description of arg2 (default: 0)\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When arg1 is empty\n\n    Examples:\n        &gt;&gt;&gt; example_function(\"test\", 5)\n        True\n        &gt;&gt;&gt; example_function(\"\", 0)\n        ValueError: arg1 cannot be empty\n    \"\"\"\n    if not arg1:\n        raise ValueError(\"arg1 cannot be empty\")\n    return len(arg1) &gt; arg2\n</code></pre>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Functions/Variables: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Constants: <code>UPPER_SNAKE_CASE</code></li> <li>Private/Internal: <code>_leading_underscore</code></li> </ul> <pre><code># Good\ndef match_symptoms(text: str) -&gt; List[Dict]:\n    NEGATION_TOKENS = {\"no\", \"not\", \"never\"}\n    _internal_cache = {}\n\nclass WeakLabelMatcher:\n    pass\n</code></pre>"},{"location":"development/contributing/#testing-requirements","title":"Testing Requirements","text":""},{"location":"development/contributing/#test-categories","title":"Test Categories","text":"<p>All contributions must include tests:</p> <ol> <li>Core Tests: Basic functionality</li> <li>Edge Case Tests: Boundary conditions, unicode, errors</li> <li>Integration Tests: End-to-end workflows</li> </ol>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>import pytest\nfrom src.weak_label import match_symptoms\n\ndef test_match_symptoms_basic():\n    \"\"\"Test basic symptom matching.\"\"\"\n    text = \"Patient has severe itching\"\n    lexicon = [\"itching\", \"redness\"]\n\n    spans = match_symptoms(text, lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] in [\"itching\", \"severe itching\"]\n    assert spans[0][\"label\"] == \"SYMPTOM\"\n    assert 0 &lt;= spans[0][\"confidence\"] &lt;= 1.0\n\ndef test_negation_detection():\n    \"\"\"Test negation detection.\"\"\"\n    text = \"No itching reported\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    assert len(spans) == 1\n    assert spans[0].get(\"negated\", False) is True\n</code></pre>"},{"location":"development/contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for \u226590% coverage on new code</li> <li>All functions must be tested</li> <li>Test edge cases: empty inputs, unicode, very long texts</li> <li>Test error handling: invalid inputs, missing files</li> </ul> <pre><code># Check coverage\npytest --cov=src --cov-report=term-missing\n\n# Generate HTML report\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest -v\n\n# Specific file\npytest tests/test_weak_label_core.py -v\n\n# Specific test\npytest tests/test_weak_label_core.py::test_match_symptoms_basic -v\n\n# Pattern matching\npytest -k \"negation\" -v\n\n# Stop on first failure\npytest -x\n\n# Show print statements\npytest -v -s\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#api-documentation","title":"API Documentation","text":"<p>Add docstrings to all public functions/classes:</p> <pre><code>def new_function(arg: str) -&gt; int:\n    \"\"\"\n    One-line summary.\n\n    Detailed explanation of what the function does,\n    including edge cases and important notes.\n\n    Args:\n        arg: Description of argument\n\n    Returns:\n        Description of return value\n\n    Examples:\n        &gt;&gt;&gt; new_function(\"test\")\n        4\n    \"\"\"\n    return len(arg)\n</code></pre>"},{"location":"development/contributing/#user-guides","title":"User Guides","text":"<p>For new features, add user guide documentation:</p> <ul> <li>Create markdown file in <code>docs/user-guide/</code></li> <li>Include examples and use cases</li> <li>Add to navigation in <code>mkdocs.yml</code></li> </ul> <p>Example structure:</p> <pre><code># Feature Name\n\n## Overview\nBrief introduction\n\n## Usage\nBasic examples\n\n## Advanced Usage\nComplex scenarios\n\n## Best Practices\nRecommendations\n\n## Troubleshooting\nCommon issues\n</code></pre>"},{"location":"development/contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Install docs dependencies\npip install -r docs-requirements.txt\n\n# Serve locally\nmkdocs serve\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n# Output in site/\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ul> <li> All tests pass locally (<code>pytest -v</code>)</li> <li> Code is formatted (<code>black src tests</code>)</li> <li> Imports are sorted (<code>isort src tests</code>)</li> <li> Documentation is updated</li> <li> Changelog is updated (if applicable)</li> <li> Branch is up-to-date with main</li> </ul>"},{"location":"development/contributing/#pr-title-format","title":"PR Title Format","text":"<p>Use conventional commits format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\nTypes:\n- feat: New feature\n- fix: Bug fix\n- docs: Documentation changes\n- test: Test additions/changes\n- refactor: Code refactoring\n- perf: Performance improvements\n- chore: Maintenance tasks\n\nExamples:\nfeat(weak_label): add bidirectional negation detection\nfix(pipeline): handle empty text input gracefully\ndocs(annotation): add Label Studio tutorial\ntest(negation): add 12 new negation pattern tests\n</code></pre>"},{"location":"development/contributing/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Motivation\nWhy is this change needed?\n\n## Changes\n- Change 1\n- Change 2\n\n## Testing\n- Test 1 added\n- Test 2 updated\n\n## Checklist\n- [ ] Tests pass\n- [ ] Documentation updated\n- [ ] Changelog updated (if applicable)\n</code></pre>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated Checks: CI/CD runs tests on 6 configurations</li> <li>Code Review: Maintainer reviews code, tests, docs</li> <li>Feedback: Address review comments</li> <li>Approval: Maintainer approves and merges</li> </ol> <p>Response time: Expect initial review within 3-5 business days.</p>"},{"location":"development/contributing/#contribution-areas","title":"Contribution Areas","text":""},{"location":"development/contributing/#1-annotation","title":"1. Annotation","text":"<p>Skills: Domain knowledge, attention to detail Tasks: - Annotate complaint texts in Label Studio - Review and correct weak labels - Participate in consensus annotation</p> <p>Impact: Directly improves model quality</p>"},{"location":"development/contributing/#2-documentation","title":"2. Documentation","text":"<p>Skills: Technical writing, markdown Tasks: - Expand user guides and tutorials - Add code examples - Improve API documentation - Write blog posts or walkthroughs</p> <p>Impact: Makes project more accessible</p>"},{"location":"development/contributing/#3-testing","title":"3. Testing","text":"<p>Skills: Python, pytest Tasks: - Add edge case tests - Improve test coverage - Write integration tests - Performance benchmarks</p> <p>Impact: Increases code reliability</p>"},{"location":"development/contributing/#4-feature-development","title":"4. Feature Development","text":"<p>Skills: Python, NLP, ML Tasks: - Implement roadmap features - Add new heuristics or filters - Integrate new models - Build tooling (CLI, API)</p> <p>Impact: Expands functionality</p>"},{"location":"development/contributing/#5-research-experimentation","title":"5. Research &amp; Experimentation","text":"<p>Skills: NLP, ML, evaluation Tasks: - Experiment with new models (RoBERTa, ClinicalBERT) - Tune hyperparameters - Evaluate weak labeling approaches - Write research reports</p> <p>Impact: Advances state-of-the-art</p>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful: Treat all contributors with respect</li> <li>Be constructive: Provide helpful, actionable feedback</li> <li>Be inclusive: Welcome diverse perspectives</li> <li>Be patient: Remember everyone is learning</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues: Bug reports, feature requests</li> <li>Pull Requests: Code contributions, reviews</li> <li>Discussions: General questions, brainstorming</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - <code>CONTRIBUTORS.md</code> file - Release notes - Documentation acknowledgments</p>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#stuck-ask-for-help","title":"Stuck? Ask for help!","text":"<ul> <li>GitHub Discussions: General questions</li> <li>GitHub Issues: Specific problems</li> <li>Documentation: User guides, API reference</li> </ul>"},{"location":"development/contributing/#useful-resources","title":"Useful Resources","text":"<ul> <li>Roadmap - Project plan</li> <li>Testing Guide - Test infrastructure</li> <li>Annotation Guide - Annotation workflow</li> <li>Weak Labeling Guide - Core functionality</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's license (see <code>LICENSE</code> file).</p> <p>Thank you for contributing to SpanForge! \ud83c\udf89</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive guide to SpanForge's test infrastructure.</p>"},{"location":"development/testing/#overview","title":"Overview","text":"<p>SpanForge implements 144 tests across 4 categories:</p> Category Count Purpose Core Tests 16 Basic functionality validation Edge Case Tests 98 Boundary conditions, unicode, negation Integration Tests 26 End-to-end workflows, scale, performance Curation Tests 4 Annotation pipeline, Label Studio export <p>Test Status: \u2705 144/144 passing (100%)</p>"},{"location":"development/testing/#quick-start","title":"Quick Start","text":""},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code># Run full suite\npytest\n\n# With verbose output\npytest -v\n\n# With coverage\npytest --cov=src --cov-report=html\n</code></pre>"},{"location":"development/testing/#run-specific-categories","title":"Run Specific Categories","text":"<pre><code># Core tests only\npytest tests/test_weak_label_core.py -v\n\n# Edge cases only\npytest tests/test_weak_label_edge.py -v\n\n# Integration tests only\npytest tests/test_integration.py -v\n\n# Curation tests only\npytest tests/test_curation.py -v\n</code></pre>"},{"location":"development/testing/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code># Single test function\npytest tests/test_weak_label_core.py::test_match_symptoms_basic -v\n\n# Pattern matching\npytest -k \"negation\" -v  # All negation tests\npytest -k \"emoji\" -v     # All emoji tests\npytest -k \"unicode\" -v   # All unicode tests\n</code></pre>"},{"location":"development/testing/#test-architecture","title":"Test Architecture","text":""},{"location":"development/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                    # Pytest fixtures\n\u251c\u2500\u2500 test_weak_label_core.py        # 16 core tests\n\u251c\u2500\u2500 test_weak_label_edge.py        # 98 edge case tests\n\u251c\u2500\u2500 test_integration.py            # 26 integration tests\n\u2514\u2500\u2500 test_curation.py               # 4 curation tests\n</code></pre>"},{"location":"development/testing/#test-composition-pattern","title":"Test Composition Pattern","text":"<p>Tests use composition for shared setup:</p> <pre><code># Base test class\nclass WeakLabelTestBase:\n    \"\"\"Shared fixtures and utilities.\"\"\"\n\n    @pytest.fixture\n    def symptom_lexicon(self):\n        return [\"itching\", \"redness\", \"burning sensation\"]\n\n    @pytest.fixture\n    def product_lexicon(self):\n        return [\"Lotion X\", \"Cream Y\"]\n\n# Edge case test class (inherits base)\nclass TestWeakLabelEdgeCases(WeakLabelTestBase):\n    \"\"\"Edge case tests with shared fixtures.\"\"\"\n\n    def test_emoji_handling(self, symptom_lexicon):\n        text = \"Patient has \ud83d\ude0a severe itching\"\n        spans = match_symptoms(text, symptom_lexicon)\n        assert len(spans) &gt; 0\n</code></pre> <p>Benefits: - Avoid duplicate fixture code - Easy to extend with new test categories - Clear inheritance hierarchy</p>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#1-core-tests-16","title":"1. Core Tests (16)","text":"<p>Purpose: Validate basic functionality</p> <p>Examples:</p> <pre><code>def test_match_symptoms_basic(symptom_lexicon):\n    \"\"\"Test basic symptom matching.\"\"\"\n    text = \"Patient has severe itching\"\n    spans = match_symptoms(text, symptom_lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] in [\"itching\", \"severe itching\"]\n    assert spans[0][\"label\"] == \"SYMPTOM\"\n\ndef test_match_products_basic(product_lexicon):\n    \"\"\"Test basic product matching.\"\"\"\n    text = \"Used Lotion X twice daily\"\n    spans = match_products(text, product_lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] == \"Lotion X\"\n    assert spans[0][\"label\"] == \"PRODUCT\"\n\ndef test_negation_forward():\n    \"\"\"Test forward negation detection.\"\"\"\n    text = \"No history of itching\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    assert len(spans) == 1\n    assert spans[0].get(\"negated\", False) is True\n</code></pre> <p>Coverage: - Symptom matching - Product matching - Fuzzy matching - Negation detection (forward/backward) - Confidence scoring</p>"},{"location":"development/testing/#2-edge-case-tests-98","title":"2. Edge Case Tests (98)","text":"<p>Purpose: Validate boundary conditions and special cases</p> <p>Categories:</p>"},{"location":"development/testing/#unicode-emoji-12-tests","title":"Unicode &amp; Emoji (12 tests)","text":"<pre><code>def test_emoji_within_text():\n    \"\"\"Test emoji doesn't break span detection.\"\"\"\n    text = \"Patient has \ud83d\ude0a severe itching and \ud83c\udf21\ufe0f redness\"\n    spans = match_symptoms(text, [\"itching\", \"redness\"])\n    assert len(spans) == 2\n\ndef test_unicode_medical_symbols():\n    \"\"\"Test medical unicode symbols.\"\"\"\n    text = \"Patient has \u22653 episodes of severe itching\"\n    spans = match_symptoms(text, [\"itching\"])\n    assert len(spans) == 1\n</code></pre>"},{"location":"development/testing/#negation-patterns-24-tests","title":"Negation Patterns (24 tests)","text":"<pre><code>def test_bidirectional_negation():\n    \"\"\"Test both forward and backward negation.\"\"\"\n    # Forward\n    text1 = \"No history of itching\"\n    spans1 = match_symptoms(text1, [\"itching\"])\n    assert spans1[0].get(\"negated\", False) is True\n\n    # Backward\n    text2 = \"Itching was denied\"\n    spans2 = match_symptoms(text2, [\"itching\"])\n    assert spans2[0].get(\"negated\", False) is True\n\ndef test_negation_out_of_scope():\n    \"\"\"Test negation beyond window.\"\"\"\n    text = \"Patient denies fever but reports itching\"\n    spans = match_symptoms(text, [\"itching\"], negation_window=5)\n    itching_span = next(s for s in spans if s[\"text\"] == \"itching\")\n    assert itching_span.get(\"negated\", False) is False\n</code></pre>"},{"location":"development/testing/#boundary-cases-18-tests","title":"Boundary Cases (18 tests)","text":"<pre><code>def test_last_token_alignment():\n    \"\"\"Test last-token alignment filter.\"\"\"\n    text = \"Patient has severe itching today\"\n    spans = match_symptoms(text, [\"itch\"])  # Partial match\n    # Should NOT match \"itch\" (doesn't end at token boundary)\n    assert not any(s[\"text\"] == \"itch\" for s in spans)\n\ndef test_sentence_boundary():\n    \"\"\"Test span doesn't cross sentence.\"\"\"\n    text = \"Patient has redness. New sentence with itching.\"\n    spans = match_symptoms(text, [\"redness\", \"itching\"])\n    assert len(spans) == 2\n</code></pre>"},{"location":"development/testing/#anatomy-filter-15-tests","title":"Anatomy Filter (15 tests)","text":"<pre><code>def test_anatomy_singleton_rejection():\n    \"\"\"Test single anatomy token rejected.\"\"\"\n    text = \"Apply to skin twice daily\"\n    spans = match_symptoms(text, [\"skin\"])\n    assert len(spans) == 0  # Generic anatomy alone rejected\n\ndef test_anatomy_with_symptom_keyword():\n    \"\"\"Test anatomy accepted with symptom.\"\"\"\n    text = \"Patient has skin redness\"\n    spans = match_symptoms(text, [\"skin\"])\n    assert len(spans) &gt; 0  # Accepted due to \"redness\" co-occurrence\n</code></pre>"},{"location":"development/testing/#validation-errors-29-tests","title":"Validation &amp; Errors (29 tests)","text":"<pre><code>def test_empty_lexicon():\n    \"\"\"Test empty lexicon handling.\"\"\"\n    text = \"Patient has itching\"\n    spans = match_symptoms(text, [])\n    assert spans == []\n\ndef test_empty_text():\n    \"\"\"Test empty text handling.\"\"\"\n    spans = match_symptoms(\"\", [\"itching\"])\n    assert spans == []\n\ndef test_confidence_bounds():\n    \"\"\"Test confidence clamped to [0, 1].\"\"\"\n    text = \"Patient has severe itching\"\n    spans = match_symptoms(text, [\"severe itching\"])\n    for span in spans:\n        assert 0.0 &lt;= span[\"confidence\"] &lt;= 1.0\n</code></pre>"},{"location":"development/testing/#3-integration-tests-26","title":"3. Integration Tests (26)","text":"<p>Purpose: Validate end-to-end workflows</p> <p>Categories:</p>"},{"location":"development/testing/#pipeline-integration-11-tests","title":"Pipeline Integration (11 tests)","text":"<pre><code>def test_pipeline_end_to_end():\n    \"\"\"Test full pipeline from text to entities.\"\"\"\n    from src.pipeline import simple_inference\n\n    text = \"Patient used Lotion X and experienced severe itching\"\n    results = simple_inference([text])\n\n    assert len(results) == 1\n    assert \"text\" in results[0]\n    assert \"entities\" in results[0]\n    assert len(results[0][\"entities\"]) &gt;= 2  # SYMPTOM + PRODUCT\n\ndef test_pipeline_jsonl_export():\n    \"\"\"Test JSONL persistence.\"\"\"\n    import tempfile\n    from src.pipeline import simple_inference\n\n    texts = [\"Text 1 with itching\", \"Text 2 with redness\"]\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n        output_path = f.name\n\n    results = simple_inference(texts, output_jsonl=output_path)\n\n    # Verify file created\n    with open(output_path) as f:\n        lines = f.readlines()\n    assert len(lines) == len(texts)\n</code></pre>"},{"location":"development/testing/#scale-tests-9-tests","title":"Scale Tests (9 tests)","text":"<pre><code>def test_large_batch_processing():\n    \"\"\"Test batch processing 100+ texts.\"\"\"\n    from src.pipeline import simple_inference\n\n    texts = [f\"Patient {i} has itching\" for i in range(100)]\n    results = simple_inference(texts)\n\n    assert len(results) == 100\n    assert all(\"entities\" in r for r in results)\n\ndef test_long_text_handling():\n    \"\"\"Test very long texts (&gt;1000 chars).\"\"\"\n    text = \"Patient has itching. \" * 50  # ~1000 chars\n    spans = match_symptoms(text, [\"itching\"])\n    assert len(spans) &gt; 0\n</code></pre>"},{"location":"development/testing/#performance-tests-6-tests","title":"Performance Tests (6 tests)","text":"<pre><code>import time\n\ndef test_inference_speed():\n    \"\"\"Test inference time per text.\"\"\"\n    from src.pipeline import simple_inference\n\n    texts = [f\"Patient {i} has itching\" for i in range(10)]\n\n    start = time.time()\n    results = simple_inference(texts)\n    elapsed = time.time() - start\n\n    # Should process &lt;1 sec/text on CPU\n    assert elapsed / len(texts) &lt; 1.0\n</code></pre>"},{"location":"development/testing/#4-curation-tests-4","title":"4. Curation Tests (4)","text":"<p>Purpose: Validate annotation pipeline</p> <pre><code>def test_weak_label_export_format():\n    \"\"\"Test weak labels exportable to Label Studio.\"\"\"\n    from src.pipeline import simple_inference\n    import json\n\n    text = \"Patient has itching\"\n    results = simple_inference([text])\n\n    # Convert to Label Studio format\n    task = {\n        \"data\": {\"text\": results[0][\"text\"]},\n        \"predictions\": [{\n            \"result\": [\n                {\n                    \"value\": {\n                        \"start\": e[\"start\"],\n                        \"end\": e[\"end\"],\n                        \"text\": e[\"text\"],\n                        \"labels\": [e[\"label\"]]\n                    },\n                    \"from_name\": \"label\",\n                    \"to_name\": \"text\",\n                    \"type\": \"labels\"\n                }\n                for e in results[0][\"entities\"]\n            ]\n        }]\n    }\n\n    # Validate JSON serializable\n    json_str = json.dumps(task)\n    assert json_str\n</code></pre>"},{"location":"development/testing/#fixtures","title":"Fixtures","text":""},{"location":"development/testing/#shared-fixtures-conftestpy","title":"Shared Fixtures (conftest.py)","text":"<pre><code>import pytest\n\n@pytest.fixture\ndef symptom_lexicon():\n    \"\"\"Standard symptom lexicon.\"\"\"\n    return [\n        \"itching\", \"redness\", \"burning\", \"swelling\",\n        \"severe itching\", \"burning sensation\", \"dry skin\"\n    ]\n\n@pytest.fixture\ndef product_lexicon():\n    \"\"\"Standard product lexicon.\"\"\"\n    return [\"Lotion X\", \"Cream Y\", \"Soap Z\"]\n\n@pytest.fixture\ndef sample_texts():\n    \"\"\"Sample complaint texts.\"\"\"\n    return [\n        \"Patient has severe itching\",\n        \"No redness reported\",\n        \"Used Lotion X twice daily\"\n    ]\n</code></pre>"},{"location":"development/testing/#test-specific-fixtures","title":"Test-Specific Fixtures","text":"<pre><code>@pytest.fixture\ndef negation_config():\n    \"\"\"Config with extended negation window.\"\"\"\n    from src.config import AppConfig\n    return AppConfig(negation_window=7)\n\n@pytest.fixture\ndef temp_output_dir(tmp_path):\n    \"\"\"Temporary output directory.\"\"\"\n    output_dir = tmp_path / \"output\"\n    output_dir.mkdir()\n    return output_dir\n</code></pre>"},{"location":"development/testing/#assertions-validation","title":"Assertions &amp; Validation","text":""},{"location":"development/testing/#entity-assertions","title":"Entity Assertions","text":"<pre><code>def assert_entity_valid(entity):\n    \"\"\"Validate entity structure.\"\"\"\n    assert \"text\" in entity\n    assert \"start\" in entity\n    assert \"end\" in entity\n    assert \"label\" in entity\n    assert entity[\"label\"] in [\"SYMPTOM\", \"PRODUCT\"]\n    assert 0 &lt;= entity.get(\"confidence\", 0.0) &lt;= 1.0\n\ndef assert_span_bounds(text, entity):\n    \"\"\"Validate span boundaries.\"\"\"\n    assert 0 &lt;= entity[\"start\"] &lt; len(text)\n    assert entity[\"start\"] &lt; entity[\"end\"] &lt;= len(text)\n    assert text[entity[\"start\"]:entity[\"end\"]] == entity[\"text\"]\n</code></pre>"},{"location":"development/testing/#negation-assertions","title":"Negation Assertions","text":"<pre><code>def assert_negated(text, entity_text, lexicon):\n    \"\"\"Assert entity is negated.\"\"\"\n    spans = match_symptoms(text, lexicon)\n    entity = next(s for s in spans if s[\"text\"] == entity_text)\n    assert entity.get(\"negated\", False) is True\n\ndef assert_not_negated(text, entity_text, lexicon):\n    \"\"\"Assert entity is NOT negated.\"\"\"\n    spans = match_symptoms(text, lexicon)\n    entity = next(s for s in spans if s[\"text\"] == entity_text)\n    assert entity.get(\"negated\", False) is False\n</code></pre>"},{"location":"development/testing/#coverage","title":"Coverage","text":""},{"location":"development/testing/#current-coverage-example","title":"Current Coverage (Example)","text":"<pre><code>Name                        Stmts   Miss  Cover\n-----------------------------------------------\nsrc/__init__.py                 0      0   100%\nsrc/config.py                  45      2    96%\nsrc/model.py                   52      3    94%\nsrc/weak_label.py             187     12    94%\nsrc/pipeline.py                78      5    94%\nsrc/llm_agent.py               34     28    18%  # Stub implementation\n-----------------------------------------------\nTOTAL                         396     50    87%\n</code></pre>"},{"location":"development/testing/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code># HTML report\npytest --cov=src --cov-report=html\n\n# Open in browser\nstart htmlcov/index.html  # Windows\nopen htmlcov/index.html   # macOS\n\n# Terminal report\npytest --cov=src --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>SpanForge runs tests on 6 configurations (2 OS \u00d7 3 Python versions):</p> <p>.github/workflows/test.yml:</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest]\n        python-version: ['3.9', '3.10', '3.11']\n\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install pytest pytest-cov\n\n      - name: Run tests\n        run: pytest -v --cov=src --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>.pre-commit-config.yaml:</p> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: pytest\n        name: pytest\n        entry: pytest\n        language: system\n        pass_filenames: false\n        always_run: true\n</code></pre> <p>Install hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now tests run automatically before each commit.</p>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Write tests first (TDD) - define expected behavior</li> <li>Use descriptive names - <code>test_negation_forward_window_5</code> not <code>test1</code></li> <li>One assertion per test - easier to debug failures</li> <li>Use fixtures - avoid duplicate setup code</li> <li>Test edge cases - empty inputs, boundary values, unicode</li> <li>Mock external calls - don't hit HuggingFace API in tests</li> <li>Run locally before push - ensure CI will pass</li> <li>Track coverage - aim for \u226590% on core modules</li> </ol>"},{"location":"development/testing/#debugging-failed-tests","title":"Debugging Failed Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Show print statements\npytest -v -s\n\n# Show locals on failure\npytest -v -l\n\n# Stop on first failure\npytest -x\n</code></pre>"},{"location":"development/testing/#debugging-with-pdb","title":"Debugging with pdb","text":"<pre><code>def test_example():\n    text = \"Patient has itching\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    # Drop into debugger\n    import pdb; pdb.set_trace()\n\n    assert len(spans) == 1\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>import pytest\n\n@pytest.mark.parametrize(\"text,expected\", [\n    (\"No itching\", True),\n    (\"Patient has itching\", False),\n    (\"Itching was denied\", True),\n])\ndef test_negation_parametrized(text, expected):\n    \"\"\"Test negation with multiple cases.\"\"\"\n    spans = match_symptoms(text, [\"itching\"])\n    assert spans[0].get(\"negated\", False) == expected\n</code></pre>"},{"location":"development/testing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"development/testing/#benchmark-suite","title":"Benchmark Suite","text":"<pre><code>import pytest\nimport time\n\n@pytest.mark.benchmark\ndef test_match_symptoms_speed(benchmark):\n    \"\"\"Benchmark symptom matching.\"\"\"\n    text = \"Patient has severe itching and redness\"\n    lexicon = [\"itching\", \"redness\", \"burning\"]\n\n    result = benchmark(match_symptoms, text, lexicon)\n    assert len(result) &gt; 0\n\n# Run benchmarks\npytest -v -m benchmark --benchmark-only\n</code></pre>"},{"location":"development/testing/#expected-performance","title":"Expected Performance","text":"Operation Texts Time (CPU) Time (GPU) <code>match_symptoms</code> 1 ~10ms N/A <code>simple_inference</code> 1 ~200ms ~50ms <code>simple_inference</code> (batch=32) 32 ~5s ~1s"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Development: Contributing - Contribution guidelines</li> <li>User Guide: Weak Labeling - Understand tested features</li> <li>API Reference - Function signatures</li> </ul>"},{"location":"user-guide/annotation/","title":"Annotation Guide","text":"<p>Comprehensive guide to human annotation workflow using Label Studio.</p>"},{"location":"user-guide/annotation/#overview","title":"Overview","text":"<p>SpanForge uses Label Studio for manual annotation and weak label refinement. Annotation workflow:</p> <ol> <li>Export weak labels from pipeline</li> <li>Import to Label Studio as pre-annotations</li> <li>Human annotation - correct, add, remove spans</li> <li>Export gold labels - validated annotations</li> <li>Convert to training format - JSONL with provenance</li> <li>Quality assurance - inter-annotator agreement, coverage</li> </ol> <pre><code>graph LR\n    A[Weak Labels] --&gt; B[Label Studio Import]\n    B --&gt; C[Human Annotation]\n    C --&gt; D[Export]\n    D --&gt; E[Gold JSONL]\n    E --&gt; F[Model Training]\n    E --&gt; G[Quality Reports]</code></pre>"},{"location":"user-guide/annotation/#setup","title":"Setup","text":""},{"location":"user-guide/annotation/#installation","title":"Installation","text":"<pre><code># Install Label Studio\npip install label-studio\n\n# Disable telemetry (privacy)\nsetx LABEL_STUDIO_DISABLE_TELEMETRY 1\n\n# Or for current session only:\n$env:LABEL_STUDIO_DISABLE_TELEMETRY = \"1\"\n</code></pre>"},{"location":"user-guide/annotation/#launch","title":"Launch","text":"<pre><code># Start server (local only)\nlabel-studio start --host localhost --port 8080\n\n# Open browser\n# Navigate to: http://localhost:8080\n</code></pre>"},{"location":"user-guide/annotation/#project-configuration","title":"Project Configuration","text":"<p>Create project with this Label Config (<code>data/annotation/config/label_config.xml</code>):</p> <pre><code>&lt;View&gt;\n  &lt;Text name=\"text\" value=\"$text\"/&gt;\n  &lt;Labels name=\"label\" toName=\"text\"&gt;\n    &lt;Label value=\"SYMPTOM\" background=\"#FF6B6B\"/&gt;\n    &lt;Label value=\"PRODUCT\" background=\"#4ECDC4\"/&gt;\n  &lt;/Labels&gt;\n&lt;/View&gt;\n</code></pre> <p>Label Descriptions: - SYMPTOM (red): Adverse events, symptoms, reactions (e.g., \"itching\", \"severe redness\") - PRODUCT (teal): Product mentions, brand names, drug names (e.g., \"Lotion X\", \"ibuprofen\")</p>"},{"location":"user-guide/annotation/#annotation-workflow","title":"Annotation Workflow","text":""},{"location":"user-guide/annotation/#1-export-weak-labels","title":"1. Export Weak Labels","text":"<pre><code>from src.pipeline import simple_inference\nimport json\n\n# Load texts\ntexts = load_texts(\"complaints.csv\")\n\n# Generate weak labels\nresults = simple_inference(texts)\n\n# Export to JSONL (Label Studio format)\noutput_path = \"data/annotation/exports/weak_labels.jsonl\"\nwith open(output_path, \"w\") as f:\n    for i, result in enumerate(results):\n        task = {\n            \"id\": i,\n            \"data\": {\"text\": result[\"text\"]},\n            \"predictions\": [{\n                \"result\": [\n                    {\n                        \"value\": {\n                            \"start\": entity[\"start\"],\n                            \"end\": entity[\"end\"],\n                            \"text\": entity[\"text\"],\n                            \"labels\": [entity[\"label\"]]\n                        },\n                        \"from_name\": \"label\",\n                        \"to_name\": \"text\",\n                        \"type\": \"labels\"\n                    }\n                    for entity in result[\"entities\"]\n                ]\n            }]\n        }\n        f.write(json.dumps(task) + \"\\n\")\n\nprint(f\"Exported {len(results)} tasks to {output_path}\")\n</code></pre>"},{"location":"user-guide/annotation/#2-import-to-label-studio","title":"2. Import to Label Studio","text":"<ol> <li>Create Project: Click \"Create Project\"</li> <li>Project Name: \"SpanForge Annotation\"</li> <li>Labeling Setup: Paste label config XML (see above)</li> <li>Data Import: </li> <li>Click \"Import\"</li> <li>Select <code>weak_labels.jsonl</code></li> <li>Check \"Treat as pre-annotations\"</li> <li>Save</li> </ol>"},{"location":"user-guide/annotation/#3-annotate-tasks","title":"3. Annotate Tasks","text":""},{"location":"user-guide/annotation/#task-interface","title":"Task Interface","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Task 1 of 100                                     [Skip] [Submit] \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  Patient experienced severe itching after using Lotion X       \u2502\n\u2502                      ^^^^^^^^^^^^^^^^              ^^^^^^^^^    \u2502\n\u2502                      SYMPTOM (weak)                PRODUCT (weak)\u2502\n\u2502                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Labels:                                                        \u2502\n\u2502  \u2610 SYMPTOM   \u2610 PRODUCT                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/annotation/#annotation-actions","title":"Annotation Actions","text":"<p>Select span: 1. Click and drag to highlight text 2. Choose label (SYMPTOM or PRODUCT) 3. Span appears with colored background</p> <p>Edit span: 1. Click existing span 2. Adjust boundaries by dragging edges 3. Or delete and recreate</p> <p>Remove span: 1. Click span to select 2. Press <code>Backspace</code> or click trash icon</p> <p>Keyboard shortcuts: - <code>S</code> - Select SYMPTOM label - <code>P</code> - Select PRODUCT label - <code>Backspace</code> - Delete selected span - <code>Ctrl+Enter</code> - Submit task - <code>Ctrl+Space</code> - Skip task</p>"},{"location":"user-guide/annotation/#4-annotation-guidelines","title":"4. Annotation Guidelines","text":""},{"location":"user-guide/annotation/#boundary-rules","title":"Boundary Rules","text":"<p>\u2713 CORRECT: <pre><code>\"severe burning sensation\"  # Include full phrase\n\"redness and swelling\"      # Include conjunction\n\"dry skin\"                  # Include modifier + noun\n</code></pre></p> <p>\u2717 INCORRECT: <pre><code>\"severe burning sensation.\"  # Exclude punctuation\n\"burning\"                    # Missing \"sensation\" (truncated)\n\" itching \"                  # Exclude leading/trailing spaces\n</code></pre></p>"},{"location":"user-guide/annotation/#negation-policy","title":"Negation Policy","text":"<p>Annotate negated symptoms - mark the span even if negated:</p> <pre><code>\"No history of itching\"\n           ^^^^^^^^ SYMPTOM (annotate, flag as negated)\n</code></pre> <p>Rationale: Teaches model negation scope, improves recall.</p> <p>Flag negated spans (optional): - Add comment: \"NEGATED\" - Or use separate label \"SYMPTOM_NEG\" (requires config change)</p>"},{"location":"user-guide/annotation/#ambiguous-cases","title":"Ambiguous Cases","text":"<p>Generic anatomy (alone): <pre><code>\"Apply to skin twice daily\"\n         ^^^^ DON'T annotate (generic, no symptom)\n\n\"Patient has skin redness\"\n             ^^^^^^^^^^^^ SYMPTOM (symptom phrase)\n</code></pre></p> <p>Product vs. Ingredient: <pre><code>\"Lotion X\"          \u2192 PRODUCT (brand name)\n\"contains retinol\"  \u2192 Don't annotate (ingredient alone)\n\"retinol cream\"     \u2192 PRODUCT (product phrase)\n</code></pre></p> <p>Synonym Preference: <pre><code>\"pruritus\" \u2192 SYMPTOM (medical term, keep canonical)\n\"itching\"  \u2192 SYMPTOM (colloquial, keep canonical)\n# Canonicalization handled downstream\n</code></pre></p>"},{"location":"user-guide/annotation/#common-errors","title":"Common Errors","text":"Error Example Fix Partial span \"severe\" instead of \"severe itching\" Extend to full phrase Trailing punct \"redness.\" (includes period) Exclude punctuation Missed negation Skip \"itching\" in \"No itching\" Annotate, flag negated Anatomy alone Annotate \"skin\" in \"Apply to skin\" Skip unless symptom phrase Overlapping spans \"burning\" + \"burning sensation\" Choose longest, most specific"},{"location":"user-guide/annotation/#5-export-gold-labels","title":"5. Export Gold Labels","text":""},{"location":"user-guide/annotation/#from-label-studio-ui","title":"From Label Studio UI","text":"<ol> <li>Project Dashboard \u2192 Export</li> <li>Format: JSON</li> <li>Download \u2192 <code>annotations.json</code></li> <li>Save to: <code>data/annotation/exports/raw/</code></li> </ol>"},{"location":"user-guide/annotation/#convert-to-training-format","title":"Convert to Training Format","text":"<pre><code>import json\n\n# Load raw export\nwith open(\"data/annotation/exports/raw/annotations.json\") as f:\n    raw_annotations = json.load(f)\n\n# Convert to gold JSONL\ngold_path = \"data/annotation/exports/gold_annotations.jsonl\"\nwith open(gold_path, \"w\") as f:\n    for task in raw_annotations:\n        # Extract annotations\n        annotations = task.get(\"annotations\", [])\n        if not annotations:\n            continue  # Skip unannotated\n\n        # Use first annotation (or consensus if multiple)\n        result = annotations[0][\"result\"]\n\n        # Convert to entity format\n        entities = []\n        for span in result:\n            value = span[\"value\"]\n            entities.append({\n                \"text\": value[\"text\"],\n                \"start\": value[\"start\"],\n                \"end\": value[\"end\"],\n                \"label\": value[\"labels\"][0],\n                \"annotator\": annotations[0].get(\"completed_by\", \"unknown\"),\n                \"timestamp\": annotations[0].get(\"created_at\", \"\"),\n            })\n\n        # Write gold entry\n        gold_entry = {\n            \"id\": task[\"id\"],\n            \"text\": task[\"data\"][\"text\"],\n            \"entities\": entities,\n            \"source\": \"label_studio\",\n            \"batch\": \"batch_001\",\n        }\n        f.write(json.dumps(gold_entry) + \"\\n\")\n\nprint(f\"Converted {len(raw_annotations)} tasks to {gold_path}\")\n</code></pre>"},{"location":"user-guide/annotation/#6-quality-assurance","title":"6. Quality Assurance","text":""},{"location":"user-guide/annotation/#coverage-report","title":"Coverage Report","text":"<pre><code>import pandas as pd\n\n# Load gold annotations\ngold = pd.read_json(\"data/annotation/exports/gold_annotations.jsonl\", lines=True)\n\n# Compute coverage\ntotal_tasks = len(gold)\nwith_symptoms = gold[\"entities\"].apply(lambda e: any(ent[\"label\"] == \"SYMPTOM\" for ent in e)).sum()\nwith_products = gold[\"entities\"].apply(lambda e: any(ent[\"label\"] == \"PRODUCT\" for ent in e)).sum()\nempty = gold[\"entities\"].apply(lambda e: len(e) == 0).sum()\n\nprint(f\"Total tasks: {total_tasks}\")\nprint(f\"Tasks with SYMPTOM: {with_symptoms} ({with_symptoms/total_tasks*100:.1f}%)\")\nprint(f\"Tasks with PRODUCT: {with_products} ({with_products/total_tasks*100:.1f}%)\")\nprint(f\"Tasks with no entities: {empty} ({empty/total_tasks*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/annotation/#inter-annotator-agreement","title":"Inter-Annotator Agreement","text":"<p>For overlapping annotations (multiple annotators per task):</p> <pre><code>from itertools import combinations\n\ndef compute_iou(span1, span2):\n    \"\"\"Compute IOU between two spans.\"\"\"\n    start = max(span1[\"start\"], span2[\"start\"])\n    end = min(span1[\"end\"], span2[\"end\"])\n    intersection = max(0, end - start)\n    union = (span1[\"end\"] - span1[\"start\"]) + (span2[\"end\"] - span2[\"start\"]) - intersection\n    return intersection / union if union &gt; 0 else 0.0\n\ndef compute_agreement(annotations1, annotations2):\n    \"\"\"Compute agreement between two annotators.\"\"\"\n    total_matches = 0\n    total_entities = len(annotations1) + len(annotations2)\n\n    for ann1 in annotations1:\n        for ann2 in annotations2:\n            if compute_iou(ann1, ann2) &gt;= 0.5 and ann1[\"label\"] == ann2[\"label\"]:\n                total_matches += 1\n                break\n\n    return 2 * total_matches / total_entities if total_entities &gt; 0 else 0.0\n\n# Load multi-annotated tasks\ntasks_multi = [task for task in raw_annotations if len(task.get(\"annotations\", [])) &gt;= 2]\n\n# Compute pairwise agreement\nagreements = []\nfor task in tasks_multi:\n    anns = task[\"annotations\"]\n    for ann1, ann2 in combinations(anns, 2):\n        entities1 = [s[\"value\"] for s in ann1[\"result\"]]\n        entities2 = [s[\"value\"] for s in ann2[\"result\"]]\n        agreement = compute_agreement(entities1, entities2)\n        agreements.append(agreement)\n\navg_agreement = sum(agreements) / len(agreements) if agreements else 0.0\nprint(f\"Average inter-annotator agreement (IOU \u2265 0.5): {avg_agreement:.2%}\")\n</code></pre>"},{"location":"user-guide/annotation/#integrity-tests","title":"Integrity Tests","text":"<pre><code>def validate_gold_annotations(gold_path):\n    \"\"\"Validate gold annotation integrity.\"\"\"\n    errors = []\n\n    with open(gold_path) as f:\n        for i, line in enumerate(f, start=1):\n            doc = json.loads(line)\n\n            # Check required fields\n            if \"text\" not in doc or \"entities\" not in doc:\n                errors.append(f\"Line {i}: Missing required fields\")\n                continue\n\n            text = doc[\"text\"]\n\n            # Check entity integrity\n            for j, entity in enumerate(doc[\"entities\"]):\n                # Check bounds\n                if entity[\"start\"] &lt; 0 or entity[\"end\"] &gt; len(text):\n                    errors.append(f\"Line {i}, entity {j}: Out of bounds\")\n\n                # Check text slice\n                expected_text = text[entity[\"start\"]:entity[\"end\"]]\n                if entity[\"text\"] != expected_text:\n                    errors.append(f\"Line {i}, entity {j}: Text mismatch\")\n\n                # Check label\n                if entity[\"label\"] not in [\"SYMPTOM\", \"PRODUCT\"]:\n                    errors.append(f\"Line {i}, entity {j}: Invalid label\")\n\n    if errors:\n        print(f\"Found {len(errors)} errors:\")\n        for error in errors[:10]:  # Show first 10\n            print(f\"  - {error}\")\n    else:\n        print(\"\u2713 All annotations valid\")\n\nvalidate_gold_annotations(\"data/annotation/exports/gold_annotations.jsonl\")\n</code></pre>"},{"location":"user-guide/annotation/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"user-guide/annotation/#consensus-annotation","title":"Consensus Annotation","text":"<p>For high-quality gold labels, use multiple annotators + consensus:</p> <pre><code>def consensus_annotation(task_annotations):\n    \"\"\"\n    Build consensus from multiple annotations.\n\n    Args:\n        task_annotations: List of annotation dicts\n\n    Returns:\n        Consensus entities (majority vote)\n    \"\"\"\n    from collections import defaultdict\n\n    # Group overlapping spans\n    span_groups = defaultdict(list)\n    for ann in task_annotations:\n        for entity in ann[\"result\"]:\n            value = entity[\"value\"]\n            key = (value[\"start\"], value[\"end\"], value[\"labels\"][0])\n            span_groups[key].append(value)\n\n    # Majority vote (\u226550% agreement)\n    consensus = []\n    min_agree = len(task_annotations) // 2 + 1\n    for key, spans in span_groups.items():\n        if len(spans) &gt;= min_agree:\n            consensus.append({\n                \"start\": key[0],\n                \"end\": key[1],\n                \"label\": key[2],\n                \"text\": spans[0][\"text\"],\n                \"votes\": len(spans)\n            })\n\n    return consensus\n\n# Apply to multi-annotated tasks\nfor task in tasks_multi:\n    consensus = consensus_annotation(task[\"annotations\"])\n    print(f\"Task {task['id']}: {len(consensus)} consensus spans\")\n</code></pre>"},{"location":"user-guide/annotation/#active-learning","title":"Active Learning","text":"<p>Prioritize uncertain examples for annotation:</p> <pre><code>from src.pipeline import simple_inference\n\n# Generate weak labels with confidence\nresults = simple_inference(texts)\n\n# Find low-confidence cases\nuncertain = [\n    result for result in results\n    if any(0.65 &lt;= e[\"confidence\"] &lt; 0.80 for e in result[\"entities\"])\n]\n\n# Export for annotation\nexport_to_label_studio(uncertain, \"data/annotation/exports/uncertain_batch.jsonl\")\n</code></pre>"},{"location":"user-guide/annotation/#annotation-drift-detection","title":"Annotation Drift Detection","text":"<p>Monitor annotation consistency over time:</p> <pre><code>import pandas as pd\n\n# Load annotations with timestamps\ngold = pd.read_json(\"data/annotation/exports/gold_annotations.jsonl\", lines=True)\ngold[\"timestamp\"] = pd.to_datetime(gold[\"entities\"].apply(lambda e: e[0][\"timestamp\"] if e else None))\n\n# Group by week\ngold[\"week\"] = gold[\"timestamp\"].dt.isocalendar().week\n\n# Compute weekly stats\nweekly = gold.groupby(\"week\").agg({\n    \"entities\": lambda x: sum(len(e) for e in x),  # Total entities\n}).reset_index()\n\nweekly[\"symptom_rate\"] = gold.groupby(\"week\")[\"entities\"].apply(\n    lambda x: sum(sum(1 for e in entities if e[\"label\"] == \"SYMPTOM\") for entities in x) / max(1, sum(len(e) for e in x))\n).values\n\nprint(weekly)\n\n# Flag drift: sudden &gt;20% change in symptom_rate\nfor i in range(1, len(weekly)):\n    change = abs(weekly.loc[i, \"symptom_rate\"] - weekly.loc[i-1, \"symptom_rate\"])\n    if change &gt; 0.20:\n        print(f\"\u26a0 Drift detected in week {weekly.loc[i, 'week']}: {change:.1%} change\")\n</code></pre>"},{"location":"user-guide/annotation/#best-practices","title":"Best Practices","text":"<ol> <li>Calibration round - annotate 50-100 samples, discuss disagreements, update guidelines</li> <li>Regular breaks - avoid fatigue (max 2 hours continuous annotation)</li> <li>Randomize order - prevent ordering bias</li> <li>Double annotation - 10-20% overlap for agreement monitoring</li> <li>Version guidelines - update docs as edge cases emerge</li> <li>Track provenance - record annotator, timestamp, batch ID</li> <li>Audit regularly - check for drift, errors, inconsistencies</li> </ol>"},{"location":"user-guide/annotation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/annotation/#issue-label-studio-connection-refused","title":"Issue: Label Studio connection refused","text":"<p>Solutions: <pre><code># Check if server running\nnetstat -an | findstr \"8080\"\n\n# Restart server\nlabel-studio start --host localhost --port 8080\n\n# Check firewall\n# Ensure localhost:8080 not blocked\n</code></pre></p>"},{"location":"user-guide/annotation/#issue-import-fails","title":"Issue: Import fails","text":"<p>Solutions: - Check JSONL format (one valid JSON per line) - Validate JSON: <code>python -m json.tool weak_labels.jsonl</code> - Ensure <code>data.text</code> field present - Check predictions structure matches label config</p>"},{"location":"user-guide/annotation/#issue-low-inter-annotator-agreement","title":"Issue: Low inter-annotator agreement","text":"<p>Solutions: - Review guidelines with annotators - Conduct calibration session - Add more examples to guidelines - Clarify ambiguous cases (anatomy, negation) - Consider majority vote consensus</p>"},{"location":"user-guide/annotation/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Understand weak labels</li> <li>Pipeline Guide - Generate annotations</li> <li>Configuration - Tune weak labeling</li> <li>Development: Testing - Test annotation pipeline</li> </ul>"},{"location":"user-guide/negation/","title":"Negation Detection Guide","text":"<p>Comprehensive guide to negation detection in SpanForge.</p>"},{"location":"user-guide/negation/#overview","title":"Overview","text":"<p>Negation detection identifies spans that are mentioned but explicitly denied or ruled out by the text. Critical for:</p> <ul> <li>Clinical accuracy - distinguish present vs. absent symptoms</li> <li>Adverse event classification - separate actual AEs from negative history</li> <li>Model training - teach models negation scope</li> <li>Quality control - flag potential annotation errors</li> </ul>"},{"location":"user-guide/negation/#architecture","title":"Architecture","text":"<p>SpanForge implements bidirectional negation scope detection with configurable windows.</p> <pre><code>graph LR\n    A[Text] --&gt; B[Detect Negation Cues]\n    B --&gt; C[Forward Scope]\n    B --&gt; D[Backward Scope]\n    C --&gt; E[Mark Spans]\n    D --&gt; E\n    E --&gt; F[Negated Spans]</code></pre>"},{"location":"user-guide/negation/#negation-tokens","title":"Negation Tokens","text":""},{"location":"user-guide/negation/#standard-negation-cues","title":"Standard Negation Cues","text":"<pre><code>NEGATION_TOKENS = {\n    # Direct negation\n    \"no\", \"not\", \"none\", \"never\", \"neither\", \"nor\",\n\n    # Clinical negation\n    \"denies\", \"denied\", \"negative\", \"absent\", \"absence\", \"absence of\",\n    \"free\", \"free of\", \"without\", \"fails to\", \"failed to\",\n\n    # Rule-out language\n    \"rule out\", \"ruled out\", \"r/o\", \"ruling out\",\n\n    # Medical descriptors\n    \"unremarkable\", \"non\", \"non-\",\n\n    # Temporal negation\n    \"no longer\", \"no more\", \"ceased\"\n}\n</code></pre>"},{"location":"user-guide/negation/#negation-categories","title":"Negation Categories","text":"Category Examples Use Case Direct no, not, never General negation Clinical denies, absent, negative Medical reports Rule-out r/o, rule out Differential diagnosis Descriptor unremarkable, non- Test results Temporal no longer, ceased Status changes"},{"location":"user-guide/negation/#detection-algorithm","title":"Detection Algorithm","text":""},{"location":"user-guide/negation/#bidirectional-scope","title":"Bidirectional Scope","text":"<pre><code>def is_negated(text: str, span_start: int, span_end: int, window: int = 5) -&gt; bool:\n    \"\"\"\n    Detect if span is negated (bidirectional).\n\n    Args:\n        text: Full text\n        span_start: Span start character offset\n        span_end: Span end character offset\n        window: Scope in tokens (default: 5)\n\n    Returns:\n        True if negated\n    \"\"\"\n    tokens = text.split()\n    span_text = text[span_start:span_end]\n    span_tokens = span_text.split()\n\n    # Locate span in token stream\n    span_token_start = len(text[:span_start].split())\n    span_token_end = span_token_start + len(span_tokens)\n\n    # Forward: negation cue BEFORE span\n    forward_start = max(0, span_token_start - window)\n    for i in range(forward_start, span_token_start):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    # Backward: negation cue AFTER span\n    backward_end = min(len(tokens), span_token_end + window)\n    for i in range(span_token_end, backward_end):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    return False\n</code></pre>"},{"location":"user-guide/negation/#window-mechanics","title":"Window Mechanics","text":"<p>Forward Negation: <pre><code>Token indices: [0,   1,       2,  3,      4,       5]\nText:          \"No  history  of  severe  itching  today\"\n               ^^^           ^^  ^^^^^^^^^^^^^^\n               cue           |   span [3:5]\n               |             |\n               |&lt;--window---&gt;|\n               [0:3]\n</code></pre> - Span tokens: [3, 5) = \"severe itching\" - Forward window: [0, 3) = \"No history of\" - Negation cue \"No\" at index 0 \u2208 [0, 3) \u2192 NEGATED</p> <p>Backward Negation: <pre><code>Token indices: [0,      1,   2,      3,  4]\nText:          \"Itching  was  denied  by  patient\"\n               ^^^^^^^^      ^^^^^^\n               span [0:1]    cue\n                        |&lt;-window-&gt;|\n                        [1:4]\n</code></pre> - Span tokens: [0, 1) = \"Itching\" - Backward window: [1, 6) = \"was denied by patient\" - Negation cue \"denied\" at index 2 \u2208 [1, 6) \u2192 NEGATED</p>"},{"location":"user-guide/negation/#examples","title":"Examples","text":""},{"location":"user-guide/negation/#forward-negation","title":"Forward Negation","text":"<pre><code># Simple forward negation\ntext = \"No itching reported\"\nspan = {\"text\": \"itching\", \"start\": 3, \"end\": 10}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (cue \"No\" at token 0, span at token 1, distance=1 \u2264 window)\n\n# Multi-word forward negation\ntext = \"Patient denies severe burning sensation\"\nspan = {\"text\": \"severe burning sensation\", \"start\": 15, \"end\": 39}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (cue \"denies\" at token 1, span at tokens 2-4, distance=1 \u2264 window)\n\n# Out-of-scope forward\ntext = \"Patient denies fever but reports itching\"\nspan = {\"text\": \"itching\", \"start\": 37, \"end\": 44}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: False (cue \"denies\" at token 1, span at token 6, distance=5 &gt; window)\n</code></pre>"},{"location":"user-guide/negation/#backward-negation","title":"Backward Negation","text":"<pre><code># Simple backward negation\ntext = \"Itching was denied\"\nspan = {\"text\": \"Itching\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (span at token 0, cue \"denied\" at token 2, distance=2 \u2264 window)\n\n# Clinical backward negation\ntext = \"Redness and swelling were unremarkable\"\nspan = {\"text\": \"Redness\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (span at token 0, cue \"unremarkable\" at token 4, distance=4 \u2264 window)\n\n# Out-of-scope backward\ntext = \"Redness present but unrelated to drug. Denied taking medication.\"\nspan = {\"text\": \"Redness\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: False (span at token 0, cue \"Denied\" at token 9, distance=9 &gt; window)\n</code></pre>"},{"location":"user-guide/negation/#complex-patterns","title":"Complex Patterns","text":"<pre><code># Conjunction scope\ntext = \"No history of itching or redness\"\nspan_itching = {\"text\": \"itching\", \"start\": 14, \"end\": 21}\nspan_redness = {\"text\": \"redness\", \"start\": 25, \"end\": 32}\nis_negated(text, span_itching[\"start\"], span_itching[\"end\"], window=5)\n# Result: True (forward negation)\nis_negated(text, span_redness[\"start\"], span_redness[\"end\"], window=5)\n# Result: True (forward negation, scope extends through \"or\")\n\n# Negation + affirmation\ntext = \"No burning but does have itching\"\nspan_burning = {\"text\": \"burning\", \"start\": 3, \"end\": 10}\nspan_itching = {\"text\": \"itching\", \"start\": 25, \"end\": 32}\nis_negated(text, span_burning[\"start\"], span_burning[\"end\"], window=5)\n# Result: True (forward negation)\nis_negated(text, span_itching[\"start\"], span_itching[\"end\"], window=5)\n# Result: False (affirmation \"does have\" breaks negation scope)\n\n# Rule-out language\ntext = \"Rule out severe allergic reaction\"\nspan = {\"text\": \"severe allergic reaction\", \"start\": 9, \"end\": 33}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (forward negation, multi-word cue \"rule out\")\n</code></pre>"},{"location":"user-guide/negation/#window-tuning","title":"Window Tuning","text":""},{"location":"user-guide/negation/#precision-vs-recall-trade-off","title":"Precision vs. Recall Trade-off","text":"Window Size Precision Recall False Positives False Negatives Use Case 1-2 Very High Low Few Many Conservative, short sentences 3-4 High Medium Some Some Balanced, standard grammar 5 (default) Balanced Balanced Moderate Moderate General use 6-7 Medium High Many Few Long sentences, complex syntax 8-10 Low Very High Very Many Very Few Exploratory, over-mark"},{"location":"user-guide/negation/#tuning-recommendations","title":"Tuning Recommendations","text":"<pre><code>from src.config import AppConfig\n\n# Conservative: short-range negation only\nconfig_conservative = AppConfig(negation_window=3)\n\n# Standard: balanced precision/recall (recommended)\nconfig_standard = AppConfig(negation_window=5)\n\n# Aggressive: long-range negation\nconfig_aggressive = AppConfig(negation_window=7)\n\n# Exploratory: catch all possible negations\nconfig_exploratory = AppConfig(negation_window=10)\n</code></pre>"},{"location":"user-guide/negation/#empirical-tuning","title":"Empirical Tuning","text":"<pre><code>import pandas as pd\nfrom src.weak_label import match_symptoms\n\n# Load gold annotations\ngold = pd.read_csv(\"gold_annotations.csv\")\n\n# Evaluate different windows\nfor window in [3, 5, 7, 10]:\n    config = AppConfig(negation_window=window)\n\n    # Run weak labeling\n    predictions = []\n    for text in gold[\"text\"]:\n        spans = match_symptoms(text, lexicon)\n        predictions.extend(spans)\n\n    # Compute metrics\n    metrics = evaluate_negation(gold, predictions)\n    print(f\"Window={window}: P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, F1={metrics['f1']:.2f}\")\n\n# Expected output:\n# Window=3: P=0.92, R=0.78, F1=0.84\n# Window=5: P=0.88, R=0.85, F1=0.86  \u2190 Best F1\n# Window=7: P=0.82, R=0.89, F1=0.85\n# Window=10: P=0.74, R=0.93, F1=0.82\n</code></pre>"},{"location":"user-guide/negation/#edge-cases","title":"Edge Cases","text":""},{"location":"user-guide/negation/#affirmative-overrides","title":"Affirmative Overrides","text":"<pre><code># Affirmation breaks negation scope\ntext = \"No fever but does have itching\"\n# \"itching\" is NOT negated (affirmation \"does have\" resets scope)\n\n# Implementation: Track affirmative cues\nAFFIRMATIVE_CUES = {\"does have\", \"has\", \"reports\", \"complains of\", \"presents with\"}\n# TODO: Not yet implemented; planned for future release\n</code></pre>"},{"location":"user-guide/negation/#double-negation","title":"Double Negation","text":"<pre><code># Double negation = affirmation\ntext = \"Patient does not deny itching\"\n# \"itching\" is AFFIRMED (logically)\n# Current system: Marks as NEGATED (structural negation only)\n# TODO: Semantic negation resolution planned\n</code></pre>"},{"location":"user-guide/negation/#negation-boundaries","title":"Negation Boundaries","text":"<pre><code># Sentence boundaries reset scope\ntext = \"No fever. Patient has itching.\"\n# \"itching\" is NOT negated (new sentence resets scope)\n\n# Implementation: Sentence splitting\nsentences = text.split('. ')\nfor sent in sentences:\n    # Process each sentence independently\n    spans = match_symptoms(sent, lexicon)\n</code></pre>"},{"location":"user-guide/negation/#integration-with-pipeline","title":"Integration with Pipeline","text":""},{"location":"user-guide/negation/#weak-labeling","title":"Weak Labeling","text":"<pre><code>from src.weak_label import match_symptoms\nfrom src.config import AppConfig\n\nconfig = AppConfig(negation_window=5)\ntext = \"No history of itching or redness\"\n\n# Detect symptoms\nspans = match_symptoms(text, lexicon)\n\n# Filter by negation status\npositive_symptoms = [s for s in spans if not s.get(\"negated\", False)]\nnegated_symptoms = [s for s in spans if s.get(\"negated\", False)]\n\nprint(f\"Positive: {positive_symptoms}\")  # []\nprint(f\"Negated: {negated_symptoms}\")    # [\"itching\", \"redness\"]\n</code></pre>"},{"location":"user-guide/negation/#pipeline-inference","title":"Pipeline Inference","text":"<pre><code>from src.pipeline import simple_inference\n\ntext = \"Patient denies burning but reports redness\"\n\n# Run pipeline (includes negation detection)\nresults = simple_inference([text])\n\n# Check negation flags\nfor span in results[0][\"entities\"]:\n    print(f\"{span['text']}: negated={span.get('negated', False)}\")\n\n# Output:\n# burning: negated=True\n# redness: negated=False\n</code></pre>"},{"location":"user-guide/negation/#annotation-export","title":"Annotation Export","text":"<pre><code>from src.weak_label import match_symptoms\nimport json\n\n# Generate weak labels with negation\ntexts = [\"No itching reported\", \"Patient has redness\"]\nresults = []\n\nfor text in texts:\n    spans = match_symptoms(text, lexicon)\n    results.append({\n        \"text\": text,\n        \"entities\": [\n            {\n                \"text\": s[\"text\"],\n                \"start\": s[\"start\"],\n                \"end\": s[\"end\"],\n                \"label\": s[\"label\"],\n                \"negated\": s.get(\"negated\", False),\n                \"confidence\": s[\"confidence\"]\n            }\n            for s in spans\n        ]\n    })\n\n# Save for Label Studio\nwith open(\"weak_labels_with_negation.jsonl\", \"w\") as f:\n    for result in results:\n        f.write(json.dumps(result) + \"\\n\")\n</code></pre>"},{"location":"user-guide/negation/#best-practices","title":"Best Practices","text":"<ol> <li>Always export negation flags - critical for downstream models</li> <li>Use default window (5) - well-calibrated for most cases</li> <li>Tune on evaluation set - measure P/R on gold negations</li> <li>Handle sentence boundaries - split long texts</li> <li>Document custom negation tokens - track domain-specific cues</li> <li>Validate with clinicians - ensure negation patterns match domain</li> <li>Track negation statistics - monitor % negated spans</li> </ol>"},{"location":"user-guide/negation/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/negation/#issue-over-marking-negations","title":"Issue: Over-marking negations","text":"<p>Symptoms: - Many false positive negations - Affirmative spans marked as negated</p> <p>Solutions: <pre><code># Reduce window\nconfig = AppConfig(negation_window=3)\n\n# Audit NEGATION_TOKENS for overly broad terms\n# Remove: \"non\" (captures \"non-prescription\", \"non-stop\")\n\n# Add sentence splitting\nsentences = text.split('. ')\n</code></pre></p>"},{"location":"user-guide/negation/#issue-under-marking-negations","title":"Issue: Under-marking negations","text":"<p>Symptoms: - Negated spans not detected - Complex negation patterns missed</p> <p>Solutions: <pre><code># Increase window\nconfig = AppConfig(negation_window=7)\n\n# Add domain-specific negation tokens\nNEGATION_TOKENS.update([\"absence of\", \"free of\", \"no evidence of\"])\n\n# Enable backward negation (already default)\n</code></pre></p>"},{"location":"user-guide/negation/#issue-negation-crosses-sentence-boundaries","title":"Issue: Negation crosses sentence boundaries","text":"<p>Symptoms: - Negation scope bleeds into next sentence</p> <p>Solutions: <pre><code># Sentence splitting before negation detection\ndef split_sentences(text):\n    \"\"\"Split on sentence boundaries.\"\"\"\n    return text.replace('! ', '!|').replace('? ', '?|').replace('. ', '.|').split('|')\n\nsentences = split_sentences(text)\nall_spans = []\noffset = 0\nfor sent in sentences:\n    spans = match_symptoms(sent, lexicon)\n    # Adjust offsets\n    for span in spans:\n        span[\"start\"] += offset\n        span[\"end\"] += offset\n    all_spans.extend(spans)\n    offset += len(sent) + 1  # +1 for delimiter\n</code></pre></p>"},{"location":"user-guide/negation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/negation/#negation-specific-metrics","title":"Negation-Specific Metrics","text":"<pre><code>def evaluate_negation(gold_annotations, predicted_spans):\n    \"\"\"\n    Evaluate negation detection performance.\n\n    Args:\n        gold_annotations: Gold spans with negation flags\n        predicted_spans: Predicted spans with negation flags\n\n    Returns:\n        Dict with negation precision/recall/F1\n    \"\"\"\n    true_pos_neg = 0   # Correctly marked as negated\n    false_pos_neg = 0  # Incorrectly marked as negated\n    false_neg_neg = 0  # Should be negated, but not marked\n\n    for pred in predicted_spans:\n        # Find matching gold span (IOU \u2265 0.5)\n        gold_match = find_matching_gold(pred, gold_annotations)\n        if gold_match:\n            if pred.get(\"negated\", False) and gold_match.get(\"negated\", False):\n                true_pos_neg += 1\n            elif pred.get(\"negated\", False) and not gold_match.get(\"negated\", False):\n                false_pos_neg += 1\n            elif not pred.get(\"negated\", False) and gold_match.get(\"negated\", False):\n                false_neg_neg += 1\n\n    precision = true_pos_neg / (true_pos_neg + false_pos_neg)\n    recall = true_pos_neg / (true_pos_neg + false_neg_neg)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n</code></pre>"},{"location":"user-guide/negation/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Full weak labeling system</li> <li>Configuration - Tune negation window</li> <li>API Reference - Function documentation</li> </ul>"},{"location":"user-guide/pipeline/","title":"Pipeline Guide","text":"<p>Comprehensive guide to SpanForge's end-to-end inference pipeline.</p>"},{"location":"user-guide/pipeline/#overview","title":"Overview","text":"<p>The pipeline combines BioBERT contextual embeddings with lexicon-based weak labeling for biomedical NER. Processes raw text through tokenization, entity detection, and optional persistence.</p> <pre><code>graph TB\n    A[Raw Text] --&gt; B[Tokenization]\n    B --&gt; C[BioBERT Encoding]\n    C --&gt; D[Weak Label Matching]\n    D --&gt; E[Negation Detection]\n    E --&gt; F[Confidence Scoring]\n    F --&gt; G[Entity Spans]\n    G --&gt; H[Optional JSONL Export]</code></pre>"},{"location":"user-guide/pipeline/#architecture","title":"Architecture","text":""},{"location":"user-guide/pipeline/#components","title":"Components","text":"Component Module Function Purpose Tokenizer <code>src.model</code> <code>get_tokenizer()</code> Load BioBERT tokenizer Model <code>src.model</code> <code>get_model()</code> Load BioBERT encoder Encoder <code>src.model</code> <code>encode_text()</code> Generate token embeddings Weak Labeler <code>src.weak_label</code> <code>match_symptoms()</code>, <code>match_products()</code> Lexicon-based entity detection Negation <code>src.weak_label</code> <code>is_negated()</code> Bidirectional negation scope Pipeline <code>src.pipeline</code> <code>simple_inference()</code> Orchestrate end-to-end"},{"location":"user-guide/pipeline/#data-flow","title":"Data Flow","text":"<pre><code>text = \"Patient reports severe itching\"\n\n# 1. Tokenization\ntokens = tokenizer(text)\n# {'input_ids': [101, 5317, 3756, 5729, 24501, 102], ...}\n\n# 2. BioBERT Encoding\nencodings = model(**tokens)\n# {last_hidden_state: tensor([...]), ...}\n\n# 3. Weak Labeling\nsymptoms = match_symptoms(text, symptom_lexicon)\nproducts = match_products(text, product_lexicon)\n# [{'text': 'severe itching', 'start': 16, 'end': 30, ...}]\n\n# 4. Negation Detection\nfor span in symptoms:\n    span['negated'] = is_negated(text, span['start'], span['end'])\n\n# 5. Output\nresult = {\n    'text': text,\n    'entities': symptoms + products\n}\n</code></pre>"},{"location":"user-guide/pipeline/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/pipeline/#basic-inference","title":"Basic Inference","text":"<pre><code>from src.pipeline import simple_inference\n\n# Single text\ntext = \"Patient experienced burning sensation after using Product X\"\nresults = simple_inference([text])\n\n# Access entities\nfor entity in results[0][\"entities\"]:\n    print(f\"{entity['label']}: {entity['text']} (confidence: {entity['confidence']:.2f})\")\n\n# Output:\n# SYMPTOM: burning sensation (confidence: 0.91)\n# PRODUCT: Product X (confidence: 0.95)\n</code></pre>"},{"location":"user-guide/pipeline/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.pipeline import simple_inference\n\n# Multiple texts\ntexts = [\n    \"Patient has severe redness\",\n    \"No itching reported\",\n    \"Used Lotion Y with no adverse effects\"\n]\n\n# Process batch\nresults = simple_inference(texts)\n\n# Iterate results\nfor i, result in enumerate(results):\n    print(f\"\\nText {i+1}: {result['text']}\")\n    for entity in result[\"entities\"]:\n        negated = \" (negated)\" if entity.get(\"negated\", False) else \"\"\n        print(f\"  - {entity['label']}: {entity['text']}{negated}\")\n\n# Output:\n# Text 1: Patient has severe redness\n#   - SYMPTOM: severe redness\n# Text 2: No itching reported\n#   - SYMPTOM: itching (negated)\n# Text 3: Used Lotion Y with no adverse effects\n#   - PRODUCT: Lotion Y\n</code></pre>"},{"location":"user-guide/pipeline/#jsonl-persistence","title":"JSONL Persistence","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = load_texts(\"complaints.csv\")\n\n# Export to JSONL\noutput_path = \"data/output/entities.jsonl\"\nresults = simple_inference(texts, output_jsonl=output_path)\n\nprint(f\"Saved {len(results)} results to {output_path}\")\n\n# Read back\nimport json\nwith open(output_path) as f:\n    for line in f:\n        doc = json.loads(line)\n        print(f\"Text: {doc['text']}\")\n        print(f\"Entities: {len(doc['entities'])}\")\n</code></pre>"},{"location":"user-guide/pipeline/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/pipeline/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from src.config import AppConfig\nfrom src.pipeline import simple_inference\n\n# Custom config\nconfig = AppConfig(\n    negation_window=7,\n    fuzzy_scorer=\"jaccard\",\n    max_seq_len=512,\n    device=\"cuda\"\n)\n\n# Use in pipeline (config automatically loaded via get_config())\nresults = simple_inference(texts)\n</code></pre>"},{"location":"user-guide/pipeline/#filtering-entities","title":"Filtering Entities","text":"<pre><code># High-confidence entities only\nhigh_conf = [\n    entity for entity in result[\"entities\"]\n    if entity[\"confidence\"] &gt;= 0.85\n]\n\n# Non-negated symptoms\npositive_symptoms = [\n    entity for entity in result[\"entities\"]\n    if entity[\"label\"] == \"SYMPTOM\" and not entity.get(\"negated\", False)\n]\n\n# Products with symptoms (co-occurrence)\nhas_both = any(e[\"label\"] == \"SYMPTOM\" for e in result[\"entities\"]) and \\\n           any(e[\"label\"] == \"PRODUCT\" for e in result[\"entities\"])\n</code></pre>"},{"location":"user-guide/pipeline/#entity-grouping","title":"Entity Grouping","text":"<pre><code>from collections import defaultdict\n\n# Group by label\ngrouped = defaultdict(list)\nfor entity in result[\"entities\"]:\n    grouped[entity[\"label\"]].append(entity)\n\nprint(f\"Symptoms: {len(grouped['SYMPTOM'])}\")\nprint(f\"Products: {len(grouped['PRODUCT'])}\")\n\n# Group by canonical form\ncanonical_groups = defaultdict(list)\nfor entity in result[\"entities\"]:\n    canonical_groups[entity[\"canonical\"]].append(entity)\n\n# Find duplicates\nduplicates = {k: v for k, v in canonical_groups.items() if len(v) &gt; 1}\n</code></pre>"},{"location":"user-guide/pipeline/#confidence-histograms","title":"Confidence Histograms","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Collect confidences\nconfidences = [e[\"confidence\"] for result in results for e in result[\"entities\"]]\n\n# Plot distribution\nplt.hist(confidences, bins=20, range=(0, 1.0), edgecolor='black')\nplt.xlabel(\"Confidence Score\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Entity Confidence Distribution\")\nplt.axvline(x=0.85, color='r', linestyle='--', label='Threshold')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/pipeline/#pipeline-components","title":"Pipeline Components","text":""},{"location":"user-guide/pipeline/#1-tokenization","title":"1. Tokenization","text":"<pre><code>from src.model import get_tokenizer, encode_text\n\ntokenizer = get_tokenizer()\ntext = \"Patient has severe burning sensation\"\n\n# Tokenize\ntokens = tokenizer(\n    text,\n    padding=\"max_length\",\n    truncation=True,\n    max_length=256,\n    return_tensors=\"pt\"\n)\n\nprint(tokens[\"input_ids\"])\n# tensor([[  101,  5317,  2038,  5729, 10566,  8006,   102,     0,     0, ...]])\n\n# Token IDs to text\ndecoded = tokenizer.decode(tokens[\"input_ids\"][0])\nprint(decoded)\n# [CLS] Patient has severe burning sensation [SEP] [PAD] [PAD] ...\n</code></pre>"},{"location":"user-guide/pipeline/#2-biobert-encoding","title":"2. BioBERT Encoding","text":"<pre><code>from src.model import get_model, encode_text\n\nmodel = get_model()\ntext = \"Patient has severe burning sensation\"\n\n# Encode\nencoding = encode_text(text)\n\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\nprint(encoding[\"input_ids\"].shape)\n# torch.Size([1, 256])\n\n# Get embeddings (requires model forward pass)\nwith torch.no_grad():\n    outputs = model(**encoding)\n    embeddings = outputs.last_hidden_state  # [1, 256, 768]\n\nprint(f\"Embedding shape: {embeddings.shape}\")\n# Embedding shape: torch.Size([1, 256, 768])\n</code></pre>"},{"location":"user-guide/pipeline/#3-weak-labeling","title":"3. Weak Labeling","text":"<pre><code>from src.weak_label import match_symptoms, match_products\nimport pandas as pd\n\n# Load lexicons\nsymptoms = pd.read_csv(\"data/lexicon/symptoms.csv\")[\"symptom\"].tolist()\nproducts = pd.read_csv(\"data/lexicon/products.csv\")[\"product\"].tolist()\n\ntext = \"Patient used Lotion X and experienced severe itching\"\n\n# Detect entities\nsymptom_spans = match_symptoms(text, symptoms)\nproduct_spans = match_products(text, products)\n\nprint(f\"Symptoms: {[s['text'] for s in symptom_spans]}\")\nprint(f\"Products: {[p['text'] for p in product_spans]}\")\n\n# Output:\n# Symptoms: ['severe itching']\n# Products: ['Lotion X']\n</code></pre>"},{"location":"user-guide/pipeline/#4-negation-detection","title":"4. Negation Detection","text":"<pre><code>from src.weak_label import match_symptoms\nfrom src.config import AppConfig\n\nconfig = AppConfig(negation_window=5)\ntext = \"No history of itching or redness\"\n\n# Detect symptoms\nspans = match_symptoms(text, symptoms)\n\n# Check negation\nfor span in spans:\n    negated = span.get(\"negated\", False)\n    print(f\"{span['text']}: negated={negated}\")\n\n# Output:\n# itching: negated=True\n# redness: negated=True\n</code></pre>"},{"location":"user-guide/pipeline/#5-postprocessing","title":"5. Postprocessing","text":"<pre><code>from src.pipeline import postprocess_predictions\n\n# Predictions (mock)\npredictions = [\n    {\"text\": \"itching\", \"start\": 10, \"end\": 17, \"label\": \"SYMPTOM\", \"confidence\": 0.92},\n    {\"text\": \"itching\", \"start\": 10, \"end\": 17, \"label\": \"SYMPTOM\", \"confidence\": 0.88},  # duplicate\n    {\"text\": \"redness\", \"start\": 22, \"end\": 29, \"label\": \"SYMPTOM\", \"confidence\": 0.75},\n]\n\n# Deduplicate &amp; filter\nprocessed = postprocess_predictions(predictions, min_confidence=0.80)\n\nprint(f\"Original: {len(predictions)} spans\")\nprint(f\"Processed: {len(processed)} spans\")\n\n# Output:\n# Original: 3 spans\n# Processed: 1 spans (deduplicated, filtered by confidence)\n</code></pre>"},{"location":"user-guide/pipeline/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/pipeline/#cpu-optimization","title":"CPU Optimization","text":"<pre><code>from src.config import AppConfig\n\n# CPU-friendly config\nconfig = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,  # Shorter sequences\n)\n\n# Process in smaller batches\nbatch_size = 8\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i+batch_size]\n    results = simple_inference(batch)\n</code></pre>"},{"location":"user-guide/pipeline/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>from src.config import AppConfig\nimport torch\n\n# GPU config\nconfig = AppConfig(\n    device=\"cuda\",\n    max_seq_len=512,\n)\n\n# Enable cuDNN autotuner\ntorch.backends.cudnn.benchmark = True\n\n# Larger batches\nbatch_size = 64\nresults = simple_inference(texts[:batch_size])\n</code></pre>"},{"location":"user-guide/pipeline/#memory-management","title":"Memory Management","text":"<pre><code>import torch\n\n# Process very large datasets\nresults = []\nfor i, text in enumerate(texts):\n    result = simple_inference([text])\n    results.append(result[0])\n\n    # Clear cache every 100 texts\n    if i % 100 == 0:\n        torch.cuda.empty_cache()\n        print(f\"Processed {i}/{len(texts)}\")\n</code></pre>"},{"location":"user-guide/pipeline/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/pipeline/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\"Valid text\", \"\", None, \"Another valid text\"]\n\n# Handle errors\nresults = []\nfor text in texts:\n    try:\n        if not text:\n            raise ValueError(\"Empty text\")\n        result = simple_inference([text])\n        results.append(result[0])\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        results.append({\"text\": text, \"entities\": [], \"error\": str(e)})\n</code></pre>"},{"location":"user-guide/pipeline/#input-validation","title":"Input Validation","text":"<pre><code>def validate_input(text):\n    \"\"\"Validate input text.\"\"\"\n    if not isinstance(text, str):\n        raise TypeError(f\"Expected str, got {type(text)}\")\n    if not text.strip():\n        raise ValueError(\"Empty text\")\n    if len(text) &gt; 10000:\n        raise ValueError(\"Text too long (&gt;10,000 chars)\")\n    return text.strip()\n\n# Safe inference\nvalidated_texts = [validate_input(t) for t in texts]\nresults = simple_inference(validated_texts)\n</code></pre>"},{"location":"user-guide/pipeline/#integration-patterns","title":"Integration Patterns","text":""},{"location":"user-guide/pipeline/#stream-processing","title":"Stream Processing","text":"<pre><code>from src.pipeline import simple_inference\n\ndef process_stream(input_stream, output_stream, batch_size=32):\n    \"\"\"Process streaming data.\"\"\"\n    batch = []\n    for text in input_stream:\n        batch.append(text)\n        if len(batch) &gt;= batch_size:\n            results = simple_inference(batch)\n            for result in results:\n                output_stream.write(json.dumps(result) + \"\\n\")\n            batch = []\n\n    # Process remaining\n    if batch:\n        results = simple_inference(batch)\n        for result in results:\n            output_stream.write(json.dumps(result) + \"\\n\")\n\n# Usage\nwith open(\"input.txt\") as infile, open(\"output.jsonl\", \"w\") as outfile:\n    process_stream(infile, outfile)\n</code></pre>"},{"location":"user-guide/pipeline/#database-integration","title":"Database Integration","text":"<pre><code>import sqlite3\nfrom src.pipeline import simple_inference\n\n# Read from database\nconn = sqlite3.connect(\"complaints.db\")\ncursor = conn.execute(\"SELECT id, text FROM complaints WHERE processed = 0 LIMIT 1000\")\n\n# Process\nresults = []\nfor row_id, text in cursor:\n    result = simple_inference([text])[0]\n    results.append((row_id, json.dumps(result)))\n\n# Write back\nconn.executemany(\n    \"UPDATE complaints SET entities = ?, processed = 1 WHERE id = ?\",\n    [(entities, row_id) for row_id, entities in results]\n)\nconn.commit()\n</code></pre>"},{"location":"user-guide/pipeline/#rest-api","title":"REST API","text":"<pre><code>from flask import Flask, request, jsonify\nfrom src.pipeline import simple_inference\n\napp = Flask(__name__)\n\n@app.route(\"/extract\", methods=[\"POST\"])\ndef extract_entities():\n    \"\"\"Entity extraction endpoint.\"\"\"\n    data = request.get_json()\n    text = data.get(\"text\", \"\")\n\n    if not text:\n        return jsonify({\"error\": \"Missing text\"}), 400\n\n    # Extract entities\n    results = simple_inference([text])\n\n    return jsonify(results[0])\n\n# Run server\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000)\n\n# Test:\n# curl -X POST http://localhost:5000/extract \\\n#   -H \"Content-Type: application/json\" \\\n#   -d '{\"text\": \"Patient has severe itching\"}'\n</code></pre>"},{"location":"user-guide/pipeline/#testing","title":"Testing","text":""},{"location":"user-guide/pipeline/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom src.pipeline import simple_inference\n\ndef test_pipeline_basic():\n    \"\"\"Test basic pipeline inference.\"\"\"\n    text = \"Patient has severe itching\"\n    results = simple_inference([text])\n\n    assert len(results) == 1\n    assert results[0][\"text\"] == text\n    assert len(results[0][\"entities\"]) &gt; 0\n\ndef test_pipeline_empty():\n    \"\"\"Test empty input.\"\"\"\n    results = simple_inference([])\n    assert results == []\n\ndef test_pipeline_negation():\n    \"\"\"Test negation detection.\"\"\"\n    text = \"No itching reported\"\n    results = simple_inference([text])\n\n    entities = results[0][\"entities\"]\n    assert any(e[\"text\"] == \"itching\" and e.get(\"negated\", False) for e in entities)\n</code></pre>"},{"location":"user-guide/pipeline/#integration-tests","title":"Integration Tests","text":"<pre><code>import tempfile\nfrom src.pipeline import simple_inference\n\ndef test_pipeline_jsonl_export():\n    \"\"\"Test JSONL export.\"\"\"\n    texts = [\"Text 1\", \"Text 2\"]\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n        output_path = f.name\n\n    # Export\n    results = simple_inference(texts, output_jsonl=output_path)\n\n    # Read back\n    with open(output_path) as f:\n        lines = f.readlines()\n\n    assert len(lines) == len(texts)\n\n    import json\n    parsed = [json.loads(line) for line in lines]\n    assert all(\"text\" in doc and \"entities\" in doc for doc in parsed)\n</code></pre>"},{"location":"user-guide/pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Batch processing - process multiple texts at once for efficiency</li> <li>Error handling - wrap inference in try-except for production</li> <li>Input validation - check text length, encoding, emptiness</li> <li>Memory management - clear GPU cache periodically for large datasets</li> <li>Confidence filtering - set minimum thresholds for downstream use</li> <li>JSONL persistence - use for audit trails and reproducibility</li> <li>Monitoring - track processing time, entity counts, confidence distribution</li> </ol>"},{"location":"user-guide/pipeline/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/pipeline/#issue-slow-processing","title":"Issue: Slow processing","text":"<p>Solutions: <pre><code># Use GPU\nconfig = AppConfig(device=\"cuda\")\n\n# Reduce sequence length\nconfig = AppConfig(max_seq_len=128)\n\n# Process in batches\nbatch_size = 32\nresults = simple_inference(texts[:batch_size])\n</code></pre></p>"},{"location":"user-guide/pipeline/#issue-cuda-out-of-memory","title":"Issue: CUDA out of memory","text":"<p>Solutions: <pre><code># Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Reduce batch size\nbatch_size = 8\n\n# Clear cache\nimport torch\ntorch.cuda.empty_cache()\n</code></pre></p>"},{"location":"user-guide/pipeline/#issue-low-entity-recall","title":"Issue: Low entity recall","text":"<p>Solutions: <pre><code># Lower fuzzy threshold\nspans = match_symptoms(text, lexicon, fuzzy_threshold=82.0)\n\n# Extend negation window\nconfig = AppConfig(negation_window=7)\n\n# Audit lexicon coverage\nmissing_terms = find_missing_lexicon_terms(gold_annotations, lexicon)\n</code></pre></p>"},{"location":"user-guide/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Advanced techniques</li> <li>Negation Guide - Negation patterns</li> <li>Configuration - Tune parameters</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"user-guide/weak-labeling/","title":"Weak Labeling Guide","text":"<p>Comprehensive guide to SpanForge's weak labeling system for biomedical entity recognition.</p>"},{"location":"user-guide/weak-labeling/#overview","title":"Overview","text":"<p>Weak labeling uses lexicon-based fuzzy matching with rule-based filters to automatically annotate symptoms and product mentions without manual labels. Produces high-recall, moderate-precision spans suitable for:</p> <ol> <li>Bootstrapping annotation - seed Label Studio with candidate spans</li> <li>Active learning - prioritize uncertain examples</li> <li>Evaluation baselines - compare supervised models</li> </ol>"},{"location":"user-guide/weak-labeling/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Input Text] --&gt; B[Fuzzy Matching]\n    B --&gt; C[Jaccard Filter]\n    C --&gt; D[Last-Token Alignment]\n    D --&gt; E[Anatomy Filter]\n    E --&gt; F[Negation Detection]\n    F --&gt; G[Confidence Scoring]\n    G --&gt; H[Spans]</code></pre>"},{"location":"user-guide/weak-labeling/#core-components","title":"Core Components","text":""},{"location":"user-guide/weak-labeling/#1-lexicon-matching","title":"1. Lexicon Matching","text":""},{"location":"user-guide/weak-labeling/#fuzzy-matching-wratio","title":"Fuzzy Matching (WRatio)","text":"<p>Uses RapidFuzz WRatio for typo-tolerant matching.</p> <p>Algorithm: <pre><code>1. Tokenize text into n-grams (1-6 tokens)\n2. For each n-gram:\n   a. Compute WRatio against lexicon entries\n   b. If score \u2265 threshold (default: 88.0):\n      - Record match\n</code></pre></p> <p>Example: <pre><code>from src.weak_label import match_symptoms\n\ntext = \"Patient experienced seveer itching\"  # typo: \"seveer\"\nlexicon = [\"severe itching\", \"itching\", \"redness\"]\n\nspans = match_symptoms(text, lexicon, fuzzy_threshold=88.0)\n# Matches \"seveer itching\" \u2192 \"severe itching\" (WRatio: 94.7)\n</code></pre></p> <p>WRatio Characteristics: - Handles typos: \"seveer\" \u2192 \"severe\" - Handles word order: \"itching severe\" \u2192 \"severe itching\" - Handles partial matches: \"severe burning itching\" \u2192 \"severe itching\"</p>"},{"location":"user-guide/weak-labeling/#jaccard-token-set-filter","title":"Jaccard Token-Set Filter","text":"<p>Filters out low-quality fuzzy matches using token overlap.</p> <p>Algorithm: <pre><code>1. Tokenize span and lexicon entry\n2. Compute Jaccard similarity: |A \u2229 B| / |A \u222a B| * 100\n3. Accept if Jaccard \u2265 threshold (default: 40.0)\n</code></pre></p> <p>Example: <pre><code># Good match: high fuzzy + high Jaccard\nspan = \"burning sensation\"\nentry = \"burning\"\n# WRatio: 90.0, Jaccard: 50.0 \u2192 ACCEPT\n\n# Bad match: high fuzzy + low Jaccard (coincidental similarity)\nspan = \"patient history\"\nentry = \"burning\"\n# WRatio: 89.0, Jaccard: 0.0 \u2192 REJECT\n</code></pre></p> <p>Why Jaccard? - Prevents false positives from short common words - Ensures semantic relevance - Complements fuzzy matching</p>"},{"location":"user-guide/weak-labeling/#2-rule-based-filters","title":"2. Rule-Based Filters","text":""},{"location":"user-guide/weak-labeling/#last-token-alignment","title":"Last-Token Alignment","text":"<p>Requires multi-token fuzzy matches to end at token boundaries.</p> <p>Rationale: Prevents partial-word matches.</p> <pre><code># ACCEPT: \"severe itching\" ends at token boundary\ntext = \"Patient has severe itching today\"\nmatch = \"severe itching\"  # \u2713 ends after \"itching\"\n\n# REJECT: \"severe itch\" doesn't end at boundary\ntext = \"Patient has severe itching today\"\nmatch = \"severe itch\"  # \u2717 ends mid-word \"itching\"\n</code></pre> <p>Implementation: <pre><code>def is_last_token_aligned(text, start, end):\n    \"\"\"Check if span ends at token boundary.\"\"\"\n    if end &gt;= len(text):\n        return True\n    next_char = text[end]\n    return next_char in [' ', '.', ',', '!', '?', '\\n']\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#anatomy-filter","title":"Anatomy Filter","text":"<p>Rejects single-token anatomy mentions unless co-occurring with symptom keywords.</p> <p>Rationale: \"skin\" alone is too generic; \"skin redness\" is a symptom.</p> <pre><code>ANATOMY_TOKENS = {\"skin\", \"eye\", \"face\", \"hand\", \"arm\", ...}\nSYMPTOM_KEYWORDS = {\"burning\", \"itching\", \"redness\", \"pain\", ...}\n\n# REJECT: standalone anatomy\ntext = \"Apply to skin twice daily\"\nmatch = \"skin\"  # \u2717 no symptom co-occurrence\n\n# ACCEPT: anatomy + symptom\ntext = \"Patient reported skin burning\"\nmatch = \"skin\"  # \u2713 co-occurs with \"burning\"\n</code></pre> <p>List of Anatomy Tokens: <pre><code>skin, eye, eyes, face, hand, hands, arm, arms, leg, legs, \nfoot, feet, scalp, chest, back, neck, finger, fingers, \ntoe, toes, nail, nails, lip, lips, mouth, tongue, throat, \nstomach, abdomen, head\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#3-negation-detection","title":"3. Negation Detection","text":"<p>Bidirectional negation scope detection with configurable window.</p> <p>Forward Negation (standard): <pre><code>Negation cue \u2192 [window tokens] \u2192 span\n         \"no\"     [history of]    itching\n</code></pre></p> <p>Backward Negation (new): <pre><code>   span   \u2190 [window tokens] \u2190 Negation cue\nitching     [was denied by]       patient\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#negation-tokens","title":"Negation Tokens","text":"<pre><code>NEGATION_TOKENS = {\n    \"no\", \"not\", \"none\", \"never\", \"without\", \"denies\", \n    \"denied\", \"negative\", \"free\", \"absent\", \"rule out\", \n    \"ruled out\", \"r/o\", \"unremarkable\", \"non\", \"free of\",\n    \"absence\", \"absence of\", \"fails to\", \"failed to\"\n}\n</code></pre>"},{"location":"user-guide/weak-labeling/#algorithm","title":"Algorithm","text":"<pre><code>def is_negated(text, span_start, span_end, window=5):\n    \"\"\"\n    Check if span is negated (forward or backward).\n\n    Args:\n        text: Full text\n        span_start: Span start character offset\n        span_end: Span end character offset\n        window: Negation scope in tokens (default: 5)\n\n    Returns:\n        True if negated\n    \"\"\"\n    tokens = text.split()\n    span_tokens = text[span_start:span_end].split()\n\n    # Find span token indices\n    span_token_start = len(text[:span_start].split())\n    span_token_end = span_token_start + len(span_tokens)\n\n    # Forward: negation cue before span\n    forward_start = max(0, span_token_start - window)\n    for i in range(forward_start, span_token_start):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    # Backward: negation cue after span\n    backward_end = min(len(tokens), span_token_end + window)\n    for i in range(span_token_end, backward_end):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    return False\n</code></pre> <p>Examples:</p> <pre><code># Forward negation\ntext = \"No history of itching or redness\"\n# \"itching\" (tokens 3-3) negated by \"No\" (token 0) [window=5]\nis_negated(text, ..., window=5)  # True\n\n# Backward negation\ntext = \"Itching was denied by patient\"\n# \"Itching\" (token 0) negated by \"denied\" (token 2) [window=5]\nis_negated(text, ..., window=5)  # True\n\n# Out of scope\ntext = \"Patient denies fever but reports itching\"\n# \"itching\" (token 6) NOT negated by \"denies\" (token 1) [window=5, gap=5]\nis_negated(text, ..., window=5)  # False\n</code></pre>"},{"location":"user-guide/weak-labeling/#tuning-negation-window","title":"Tuning Negation Window","text":"Window Precision Recall Use Case 3 High Low Conservative, short sentences 5 Balanced Balanced Default recommendation 7 Medium High Long sentences, complex grammar 10 Low Very High Exploratory, accept over-marking <pre><code>from src.config import AppConfig\n\n# Conservative negation\nconfig = AppConfig(negation_window=3)\n\n# Aggressive negation\nconfig = AppConfig(negation_window=10)\n</code></pre>"},{"location":"user-guide/weak-labeling/#4-confidence-scoring","title":"4. Confidence Scoring","text":"<p>Weighted combination of fuzzy and Jaccard scores.</p> <p>Formula: <pre><code>confidence = 0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score\nconfidence = min(confidence, 1.0)  # clamp\n</code></pre></p> <p>Rationale: - Fuzzy score (80% weight) - primary signal - Jaccard score (20% weight) - quality gate - Clamping prevents impossible &gt;1.0 values</p> <p>Example: <pre><code>span = \"burning sensation\"\nlexicon_entry = \"burning\"\n\nfuzzy_score = 90.0  # high similarity\njaccard_score = 50.0  # moderate overlap\n\nconfidence = 0.8 * 90.0 + 0.2 * 50.0\nconfidence = 72.0 + 10.0 = 82.0\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/weak-labeling/#custom-lexicons","title":"Custom Lexicons","text":"<pre><code>from src.weak_label import match_symptoms\n\n# Load custom lexicon\ncustom_lexicon = [\n    \"proprietary syndrome X\",\n    \"brand-specific reaction\",\n    \"custom symptom term\"\n]\n\nspans = match_symptoms(\n    text=\"Patient had brand-specific reaction\",\n    lexicon=custom_lexicon,\n    fuzzy_threshold=88.0\n)\n</code></pre>"},{"location":"user-guide/weak-labeling/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.weak_label import match_symptoms, match_products\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"complaints.csv\")\n\n# Load lexicons\nsymptoms = pd.read_csv(\"data/lexicon/symptoms.csv\")[\"symptom\"].tolist()\nproducts = pd.read_csv(\"data/lexicon/products.csv\")[\"product\"].tolist()\n\n# Process batch\nresults = []\nfor text in df[\"complaint_text\"]:\n    symptom_spans = match_symptoms(text, symptoms)\n    product_spans = match_products(text, products)\n    results.append({\n        \"text\": text,\n        \"symptoms\": symptom_spans,\n        \"products\": product_spans\n    })\n</code></pre>"},{"location":"user-guide/weak-labeling/#confidence-filtering","title":"Confidence Filtering","text":"<pre><code># High-confidence spans only\nspans = match_symptoms(text, lexicon)\nhigh_conf = [s for s in spans if s[\"confidence\"] &gt;= 0.85]\n\n# Low-confidence spans (for review)\nreview_queue = [s for s in spans if 0.65 &lt;= s[\"confidence\"] &lt; 0.85]\n</code></pre>"},{"location":"user-guide/weak-labeling/#negation-aware-filtering","title":"Negation-Aware Filtering","text":"<pre><code># Exclude negated spans\npositive_spans = [s for s in spans if not s.get(\"negated\", False)]\n\n# Negated spans only (for training negation classifier)\nnegated_spans = [s for s in spans if s.get(\"negated\", False)]\n</code></pre>"},{"location":"user-guide/weak-labeling/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/weak-labeling/#cpu-optimization","title":"CPU Optimization","text":"<pre><code># Use exact matching for small lexicons\ndef exact_match(text, lexicon):\n    \"\"\"Exact substring matching (fast).\"\"\"\n    spans = []\n    for term in lexicon:\n        start = 0\n        while True:\n            idx = text.lower().find(term.lower(), start)\n            if idx == -1:\n                break\n            spans.append({\n                \"text\": text[idx:idx+len(term)],\n                \"start\": idx,\n                \"end\": idx + len(term),\n                \"label\": \"SYMPTOM\",\n                \"confidence\": 1.0,\n                \"canonical\": term\n            })\n            start = idx + 1\n    return spans\n</code></pre>"},{"location":"user-guide/weak-labeling/#threshold-tuning","title":"Threshold Tuning","text":"<pre><code># Higher thresholds = higher precision, lower recall\nstrict_spans = match_symptoms(text, lexicon, fuzzy_threshold=92.0)\n\n# Lower thresholds = higher recall, lower precision\nlenient_spans = match_symptoms(text, lexicon, fuzzy_threshold=82.0)\n</code></pre>"},{"location":"user-guide/weak-labeling/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/weak-labeling/#precisionrecall","title":"Precision/Recall","text":"<pre><code>def evaluate_weak_labels(gold_annotations, predicted_spans):\n    \"\"\"\n    Evaluate weak labeling performance.\n\n    Args:\n        gold_annotations: List of gold spans\n        predicted_spans: List of predicted spans\n\n    Returns:\n        Dict with precision, recall, F1\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for pred in predicted_spans:\n        matched = False\n        for gold in gold_annotations:\n            # IOU overlap \u2265 0.5\n            overlap = compute_iou(pred, gold)\n            if overlap &gt;= 0.5 and pred[\"label\"] == gold[\"label\"]:\n                true_positives += 1\n                matched = True\n                break\n        if not matched:\n            false_positives += 1\n\n    false_negatives = len(gold_annotations) - true_positives\n\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n</code></pre>"},{"location":"user-guide/weak-labeling/#confidence-calibration","title":"Confidence Calibration","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Plot confidence distribution\nconfidences = [s[\"confidence\"] for s in spans]\nplt.hist(confidences, bins=20, range=(0, 100))\nplt.xlabel(\"Confidence Score\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Weak Label Confidence Distribution\")\nplt.show()\n</code></pre>"},{"location":"user-guide/weak-labeling/#best-practices","title":"Best Practices","text":"<ol> <li>Start with high thresholds (fuzzy=90, Jaccard=45) - prioritize precision</li> <li>Tune on evaluation set - measure P/R/F1 on gold annotations</li> <li>Use bidirectional negation - captures more negation patterns</li> <li>Filter anatomy singletons - reduces false positives</li> <li>Require last-token alignment - prevents partial-word matches</li> <li>Cache lexicon lookups - speeds up batch processing</li> <li>Version lexicons - track changes for reproducibility</li> </ol>"},{"location":"user-guide/weak-labeling/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/weak-labeling/#issue-high-false-positive-rate","title":"Issue: High false positive rate","text":"<p>Causes: - Fuzzy threshold too low - Missing anatomy filter - Lexicon contains generic terms</p> <p>Solutions: <pre><code># Increase thresholds\nspans = match_symptoms(text, lexicon, fuzzy_threshold=92.0)\n\n# Add anatomy filter (already default)\n\n# Audit lexicon for generic terms\ngeneric_terms = [\"skin\", \"patient\", \"product\"]  # remove these\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#issue-missing-multi-word-symptoms","title":"Issue: Missing multi-word symptoms","text":"<p>Causes: - Lexicon only has single-word entries - Last-token alignment too strict</p> <p>Solutions: <pre><code># Add multi-word entries to lexicon\nlexicon = [\n    \"burning sensation\",  # not just \"burning\"\n    \"severe itching\",     # not just \"itching\"\n    \"dry skin\"            # not just \"dry\"\n]\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#issue-over-aggressive-negation","title":"Issue: Over-aggressive negation","text":"<p>Causes: - Negation window too large - Negation token list too broad</p> <p>Solutions: <pre><code># Reduce window\nconfig = AppConfig(negation_window=3)\n\n# Audit NEGATION_TOKENS for overly broad terms\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Tune thresholds</li> <li>Negation Guide - Advanced negation patterns</li> <li>API Reference - Full function documentation</li> </ul>"}]}