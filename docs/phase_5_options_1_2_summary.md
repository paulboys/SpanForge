# Phase 5 Implementation Summary - Options 1 & 2

**Date**: November 25, 2025  
**Phase**: 5 (Annotation & Curation)  
**Status**: Tutorial & Production Guide Complete  
**User Request**: "Option 1 and 2 only"

---

## Overview

Completed Phase 5 Option 1 (Label Studio Configuration + Tutorial) and Option 2 (Production Evaluation Workflow Documentation) to enable production annotation batches.

---

## Option 1: Label Studio Configuration & Tutorial

### Deliverables

#### 1. Enhanced Label Config (`data/annotation/config/label_config.xml`)

**Improvements**:
- âœ… Header added: "Annotate Adverse Event Symptoms and Products"
- âœ… Hotkeys: `s` for SYMPTOM, `p` for PRODUCT
- âœ… Word-level granularity: `granularity="word"`
- âœ… Colorblind-safe palette: Green (#2ca02c) for SYMPTOM, Blue (#1f77b4) for PRODUCT
- âœ… Optional negation tracking: Commented checkbox for future use
- âœ… Loading state styling: Visual feedback during API calls

**Before**:
```xml
<Labels name="label" toName="text">
  <Label value="SYMPTOM" background="red"/>
  <Label value="PRODUCT" background="blue"/>
</Labels>
<Text name="text" value="$text"/>
```

**After**:
```xml
<View>
  <Header value="Annotate Adverse Event Symptoms and Products"/>
  <Text name="text" value="$text" granularity="word"/>
  <Labels name="label" toName="text">
    <Label value="SYMPTOM" background="#2ca02c" hotkey="s"/>
    <Label value="PRODUCT" background="#1f77b4" hotkey="p"/>
  </Labels>
  <!-- Optional negation tracking -->
</View>
```

---

#### 2. Configuration Documentation (`data/annotation/config/README.md`)

**Contents** (100+ lines):
- **Usage Instructions**: Web UI import, API import via Python
- **Test Configuration**: 3-step workflow to verify setup
- **Customization Examples**: Negation flags, severity scales
- **Integration Points**: Import/export scripts, quality reporting
- **Troubleshooting**: Common issues (labels not appearing, hotkeys broken, export mismatches)

**Key Sections**:
- API Import Example (Python + requests library)
- Project Configuration (telemetry disable, local storage)
- Quality Report Integration (span density, agreement metrics)

---

#### 3. Tutorial Notebook (`scripts/AnnotationWalkthrough.ipynb`)

**Structure** (7 sections, 20 cells):

1. **Introduction** (2 cells):
   - Why manual annotation matters (weak labels vs LLM vs gold)
   - Pipeline overview (raw text â†’ weak â†’ LLM â†’ annotation â†’ gold â†’ fine-tune)

2. **Data Preparation** (3 cells):
   - Load weak labels from JSONL
   - Explore statistics (confidence distribution, label counts)
   - Visualize with matplotlib/seaborn

3. **LLM Refinement Demo** (2 cells):
   - Compare weak vs LLM-refined labels
   - Highlight boundary corrections (adjective removal, canonical normalization)

4. **Label Studio Setup** (2 cells):
   - Check installation + telemetry disable
   - Manual import steps (create project, upload config, import tasks)

5. **Annotation Practice** (5 cells):
   - **Example 1**: Boundary correction ("severe burning sensation" â†’ "burning sensation")
   - **Example 2**: Negation handling ("no redness" â†’ annotate "redness" only)
   - **Example 3**: Anatomy gating (skip single "skin", keep "facial swelling")
   - **Example 4**: Multi-word medical terms ("anaphylactic shock", not "shock" alone)
   - **Example 5**: Overlapping conjunctions ("redness and swelling" â†’ separate spans)

6. **Export & Evaluation** (3 cells):
   - Export from Label Studio (manual steps)
   - Convert to gold JSONL (CLI command)
   - Run evaluation harness (CLI + interpret results)

7. **Common Mistakes & Glossary** (3 cells):
   - 5 common errors (adjectives, negation, anatomy, truncation, conjunctions)
   - Symptom glossary (canonical terms: pruritus, erythema, dyspnea)
   - Product annotation tips (brand names, generics, abbreviations)
   - Boundary decision tree (flowchart for span selection)

**Educational Features**:
- **Interactive Code Cells**: Load/visualize data, run evaluation
- **Visual Examples**: Confidence histograms, label distributions
- **Practice Exercises**: 5 example texts with correct/incorrect annotations
- **Glossary**: Medical term mappings (itching â†’ pruritus, shortness of breath â†’ dyspnea)
- **Decision Tree**: Flowchart for resolving boundary ambiguities

---

## Option 2: Production Evaluation Workflow Guide

### Deliverable: `docs/production_workflow.md`

**Structure** (450+ lines, 8 sections):

#### 1. Overview
- 7-phase workflow diagram (batch prep â†’ annotation â†’ evaluation â†’ iteration)
- Key metric targets (IOU +8-15%, F1 >0.85, worsened <10%)

#### 2. Prerequisites
- Environment setup (Python, Label Studio, LLM providers)
- Data requirements (raw complaints, lexicons, config)

#### 3. Workflow Steps (7 detailed steps)

**Step 1: Prepare Production Batch**
```bash
python scripts/annotation/prepare_production_batch.py \
  --input data/raw/complaints_pool.txt \
  --output data/annotation/batches/batch_001/ \
  --batch-size 100 \
  --stratify confidence \
  --deidentify
```
- Output: manifest.json, tasks.json, weak_labels.jsonl, llm_refined.jsonl, texts.txt
- Manifest includes stratification stats, LLM costs, metadata

**Step 2: Import to Label Studio**
- Manual steps: Create project, import config, upload tasks
- Verification: Check task count, test hotkeys, verify pre-annotations

**Step 3: Annotate**
- Guidelines: Follow `docs/annotation_guide.md`
- Quality checks every 25 tasks
- Target: 2-3 hours per 100 tasks

**Step 4: Export from Label Studio**
- Manual steps: Complete all tasks, export JSON, save to data directory

**Step 5: Convert to Gold Standard**
```bash
python scripts/annotation/convert_labelstudio.py \
  --input data/annotation/raw/batch_001_export.json \
  --output data/gold/batch_001.jsonl \
  --annotator your_name
```
- Validation: Canonical coverage â‰¥90%, boundary integrity 100%

**Step 6: Run Evaluation**
```bash
python scripts/annotation/cli.py evaluate-llm \
  --weak ... --refined ... --gold ... \
  --output reports/batch_001_eval.json \
  --markdown --stratify label confidence span_length
```
- Output: JSON report, Markdown summary, stratified analysis

**Step 7: Generate Visualizations**
```bash
python scripts/annotation/cli.py plot-metrics \
  --report reports/batch_001_eval.json \
  --output-dir plots/batch_001/ \
  --formats png pdf --dpi 300 --plots all
```
- 6 plots: IOU uplift, calibration, correction, P/R/F1, stratified label/confidence

---

#### 4. Data Validation
- **Pre-Evaluation Checks**: File existence, JSONL format validation, span integrity test
- **Python Scripts**: validate_jsonl(), check span boundaries, verify canonical coverage

---

#### 5. CLI Execution Examples
- **Example 1**: Quick evaluation (no stratification)
- **Example 2**: Full evaluation (all stratifications)
- **Example 3**: Confidence-only stratification
- **Example 4**: Visualization only (specific plots)

---

#### 6. Result Interpretation

**Target Metrics Table**:
| Metric | Target | Red Flag | Interpretation |
|--------|--------|----------|----------------|
| IOU Improvement | +8-15% | <+5% | LLM boundary correction effectiveness |
| Exact Match Rate | 70-85% | <60% | LLM-gold alignment |
| Correction Rate (Improved) | >60% | <50% | LLM improvement ratio |
| Correction Rate (Worsened) | <10% | >15% | LLM error rate |
| F1 Score (LLM vs Gold) | >0.85 | <0.75 | Precision + recall |
| Canonical Coverage | >90% | <80% | Lexicon completeness |

**Interpretation Guides** (4 scenarios):
1. **IOU Improvement Below Target**: Check worsened spans, calibrate lexicon thresholds
2. **High Worsened Rate**: Inspect worsened spans, adjust LLM prompt aggressiveness
3. **Low Canonical Coverage**: Extract missing terms, update lexicon, re-run weak labeling
4. **Low F1 Score**: Stratify by confidence, check precision vs recall, filter low-confidence spans

**Python Code Examples**: Analyzing worsened spans, extracting missing lexicon entries, stratifying by confidence

---

#### 7. Iteration Strategy

**After First Batch (100 tasks)**:
- Identify systematic errors (boundary, negation, anatomy)
- Refine prompts/lexicons based on worsened spans
- Calibrate confidence thresholds for next batch
- **Example Workflow**: Analyze worsened patterns, adjust LLM prompt for compound terms

**After Third Batch (300 tasks)**:
- Measure inter-batch consistency (F1 standard deviation)
- Estimate final model performance (extrapolate F1)
- Decide on batch size (scale to 500 if F1 stable)
- **Python Code**: Compare metrics across batches, calculate mean F1 Â± std

---

#### 8. Troubleshooting

**4 Common Issues**:
1. **Mismatched IDs**: Ensure ID consistency across batch prep and conversion
2. **Poor Calibration Curve**: Recalibrate confidence formula with linear regression
3. **LLM API Hangs**: Test API connectivity, check rate limits
4. **Missing Stratification Tables**: Re-run evaluation with `--stratify` flag

**Solutions**: Python code examples, CLI commands, debugging steps

---

### Production Readiness Checklist

From `production_workflow.md`:

- [ ] Batch Preparation: 100 tasks stratified by confidence
- [ ] Label Studio Import: Config loaded, tasks imported with pre-annotations
- [ ] Annotation: All 100 tasks completed (2-3 hours)
- [ ] Export & Convert: Gold JSONL with canonical coverage >90%
- [ ] Evaluation: IOU improvement >8%, F1 >0.85, worsened rate <10%
- [ ] Visualization: 6 plots generated
- [ ] Iteration: Worsened spans analyzed, prompts/lexicons refined

---

## Files Created

### Phase 5 Implementation (This Session)

1. **`data/annotation/config/label_config.xml`** (MODIFIED - 20 lines):
   - Enhanced with hotkeys, granularity, colorblind-safe palette
   - Original: 7 lines (basic SYMPTOM/PRODUCT)
   - New: 20 lines (header, word granularity, optional negation, styling)

2. **`data/annotation/config/README.md`** (NEW - 100+ lines):
   - Configuration usage guide
   - API import examples
   - Customization options
   - Troubleshooting

3. **`scripts/AnnotationWalkthrough.ipynb`** (NEW - 20 cells, 7 sections):
   - Interactive tutorial for annotators
   - 5 practice examples with correct/incorrect annotations
   - Evaluation workflow walkthrough
   - Common mistakes + glossary + decision tree

4. **`docs/production_workflow.md`** (NEW - 450+ lines, 8 sections):
   - Complete production evaluation workflow
   - 7 workflow steps (batch prep â†’ visualization)
   - Data validation scripts
   - Result interpretation guide (6 target metrics)
   - Iteration strategies (after 100/300 tasks)
   - Troubleshooting (4 common issues)

---

## Integration with Existing Infrastructure

### Completed Phase 4.5 Components (Used in Tutorial/Workflow)

- **Evaluation Harness**: `src/evaluation/metrics.py` (10 functions)
- **Evaluation Script**: `scripts/annotation/evaluate_llm_refinement.py`
- **Visualization**: `scripts/annotation/plot_llm_metrics.py`
- **CLI Integration**: `scripts/annotation/cli.py` (evaluate-llm, plot-metrics)
- **Test Coverage**: 186 tests (100% passing)

### Pending Phase 5 Components (Referenced in Workflow)

- **Batch Preparation Script**: `scripts/annotation/prepare_production_batch.py` (NOT YET IMPLEMENTED)
  - Stratified sampling by confidence
  - De-identification (PII removal)
  - Batch manifest generation
  - LLM refinement integration
  - Cost tracking

- **Conversion Script**: `scripts/annotation/convert_labelstudio.py` (NOT YET IMPLEMENTED)
  - Label Studio JSON â†’ gold JSONL
  - Canonical normalization
  - Provenance tracking (annotator, source, timestamp)
  - Span integrity validation

---

## Usage Examples

### For Annotators (Tutorial)

1. **Launch Notebook**:
   ```powershell
   jupyter notebook scripts/AnnotationWalkthrough.ipynb
   ```

2. **Complete Sections**:
   - Read introduction (why annotation matters)
   - Load and visualize weak labels (Section 2)
   - Compare weak vs LLM refinement (Section 3)
   - Practice with 5 examples (Section 5)
   - Follow Label Studio setup (Section 4)
   - Run evaluation after annotation (Section 6)

3. **Expected Time**: 1-2 hours (first-time annotators)

---

### For Project Managers (Production Workflow)

1. **Prepare First Batch**:
   ```bash
   python scripts/annotation/prepare_production_batch.py \
     --input data/raw/complaints_pool.txt \
     --output data/annotation/batches/batch_001/ \
     --batch-size 100 --stratify confidence --deidentify
   ```
   *(Script pending implementation)*

2. **Annotate in Label Studio** (2-3 hours):
   - Import `batches/batch_001/tasks.json`
   - Use config from `data/annotation/config/label_config.xml`

3. **Convert & Evaluate**:
   ```bash
   # Convert export
   python scripts/annotation/convert_labelstudio.py \
     --input label_studio_export.json \
     --output data/gold/batch_001.jsonl \
     --annotator your_name
   
   # Evaluate quality
   python scripts/annotation/cli.py evaluate-llm \
     --weak batches/batch_001/weak_labels.jsonl \
     --refined batches/batch_001/llm_refined.jsonl \
     --gold data/gold/batch_001.jsonl \
     --output reports/batch_001_eval.json \
     --markdown --stratify label confidence
   
   # Visualize results
   python scripts/annotation/cli.py plot-metrics \
     --report reports/batch_001_eval.json \
     --output-dir plots/batch_001/ \
     --formats png pdf --dpi 300 --plots all
   ```

4. **Iterate** (see `docs/production_workflow.md` Section 7):
   - Analyze worsened spans
   - Refine prompts/lexicons
   - Repeat for batches 2-3

---

## Validation

### Tutorial Notebook

**Validated Features**:
- âœ… All code cells syntactically correct (no syntax errors)
- âœ… Imports standard libraries only (json, pathlib, pandas, matplotlib, seaborn)
- âœ… Example data paths reference test fixtures (`tests/fixtures/annotation/`)
- âœ… Markdown cells use proper formatting (headers, code blocks, tables)
- âœ… Practice examples cover key scenarios (boundary, negation, anatomy, multi-word, conjunctions)

**Pending Validation** (requires real data):
- ðŸŸ¡ Run cells with production data (after first batch annotated)
- ðŸŸ¡ Verify visualizations render correctly
- ðŸŸ¡ Test Label Studio import/export workflow

---

### Production Workflow Guide

**Validated Features**:
- âœ… All CLI commands use correct syntax (PowerShell compatible)
- âœ… File paths reference standard locations (`data/annotation/`, `data/gold/`)
- âœ… Python code examples syntactically correct
- âœ… Metric targets aligned with Phase 4.5 benchmarks
- âœ… Troubleshooting solutions address real issues (from test fixtures)

**Pending Validation** (requires real batch):
- ðŸŸ¡ Execute full workflow with 100-task batch
- ðŸŸ¡ Validate manifest.json format
- ðŸŸ¡ Test data validation scripts with production data
- ðŸŸ¡ Measure actual annotation time (2-3 hours estimate)

---

## Next Steps

### Immediate (Week 1)

1. **Implement Batch Preparation Script** (`prepare_production_batch.py`):
   - Stratified sampling by confidence (buckets: 0.5-0.7, 0.7-0.85, 0.85-1.0)
   - De-identification (remove PII: names, dates, locations)
   - Batch manifest generation (JSON with metadata)
   - LLM refinement integration (optional `--llm-refine` flag)
   - Cost tracking (estimate LLM API costs)

2. **Implement Conversion Script** (`convert_labelstudio.py`):
   - Parse Label Studio JSON export
   - Convert to gold JSONL format
   - Canonical normalization (map to lexicon entries)
   - Provenance tracking (annotator, source, timestamp)
   - Span integrity validation (boundary checks)

3. **Test Complete Workflow** (dry run with 10 tasks):
   - Prepare mini-batch (10 tasks)
   - Import to Label Studio
   - Annotate (1-2 samples)
   - Export, convert, evaluate
   - Validate all components work end-to-end

---

### Short-Term (Week 2-4)

4. **Annotate First Production Batch** (100 tasks):
   - Use tutorial notebook for training
   - Complete all 100 tasks (2-3 hours)
   - Run evaluation (target: IOU +8-15%, F1 >0.85)
   - Generate visualizations

5. **Iterate Based on Metrics**:
   - Analyze worsened spans (target: <10%)
   - Refine LLM prompts if over-correcting
   - Update lexicon with missing canonical terms
   - Adjust confidence thresholds for filtering

6. **Annotate Batches 2-3** (200 more tasks):
   - Measure inter-batch consistency (F1 std <0.01)
   - Validate annotation guidelines
   - Estimate time-to-completion for remaining data

---

### Long-Term (Weeks 5-8)

7. **Scale to 1,000 Gold Annotations**:
   - Increase batch size to 500 tasks (if F1 stable)
   - Implement multi-annotator workflow (if needed)
   - Track cumulative metrics (IOU improvement over time)

8. **Fine-Tune BioBERT**:
   - Train token classification head on gold data
   - Evaluate on held-out test set (20% of gold)
   - Compare to weak/LLM baselines (target: +10-15% F1 improvement)

9. **Phase 6 Planning** (Gold Standard Assembly):
   - Define final dataset structure (train/dev/test splits)
   - Implement inter-annotator agreement metrics (Cohen's kappa)
   - Quality assurance (span integrity tests, canonical coverage)

---

## Cost Estimates

### LLM Refinement (100 tasks)

**Assumptions**:
- Average complaint length: 150 tokens
- LLM refinement prompt: 100 tokens
- Average output: 50 tokens
- Total per task: 300 tokens input, 50 tokens output

**Costs by Provider**:
- **OpenAI GPT-4**: $0.03/1K input + $0.06/1K output = $1.20 per 100 tasks
- **Azure OpenAI GPT-4**: Same pricing as OpenAI
- **Anthropic Claude 3.5 Sonnet**: $0.003/1K input + $0.015/1K output = $0.16 per 100 tasks
- **OpenAI GPT-4o-mini**: ~$0.15 per 100 tasks (10x cheaper than GPT-4)

**Recommended**: Claude 3.5 Sonnet or GPT-4o-mini for cost efficiency (see `docs/phase_5_plan.md` for ROI analysis)

---

### Annotation Labor (100 tasks)

**Assumptions**:
- Experienced annotator: 2 hours per 100 tasks
- Novice annotator: 3 hours per 100 tasks (with tutorial)
- Hourly rate: $30/hour (domain expert)

**Costs**:
- Experienced: $60 per 100 tasks
- Novice: $90 per 100 tasks

**Total First Batch** (100 tasks, LLM + annotation):
- Low-cost LLM (Claude): $0.16 + $60-90 = $60-90
- High-cost LLM (GPT-4): $1.20 + $60-90 = $61-91

---

## Success Criteria

### Tutorial Notebook
- [ ] All code cells execute without errors
- [ ] Visualizations render correctly (confidence histograms, label distributions)
- [ ] Practice examples cover key scenarios (5 examples)
- [ ] Glossary includes 10+ medical term mappings
- [ ] Decision tree flowchart clear and actionable

### Production Workflow Guide
- [ ] Complete 7-step workflow documented
- [ ] Data validation scripts functional
- [ ] CLI examples executable (copy-paste ready)
- [ ] Interpretation guide covers 6 target metrics
- [ ] Troubleshooting addresses 4 common issues

### End-to-End Validation (Pending Real Data)
- [ ] Batch preparation script generates valid manifest.json
- [ ] Conversion script produces gold JSONL with >90% canonical coverage
- [ ] Evaluation achieves target metrics (IOU +8-15%, F1 >0.85)
- [ ] Visualizations generated in <30 seconds
- [ ] Complete workflow (prep â†’ annotation â†’ eval) executable in 3-4 hours

---

## Summary

### Completed Deliverables

1. **Enhanced Label Config**: Hotkeys, granularity, colorblind-safe palette
2. **Configuration Docs**: 100+ line README with API examples, troubleshooting
3. **Tutorial Notebook**: 7 sections, 5 practice examples, glossary, decision tree
4. **Production Workflow Guide**: 450+ lines, 7-step workflow, interpretation guide, troubleshooting

### Lines of Code/Documentation

- **Label Config**: 7 â†’ 20 lines (enhanced)
- **Config README**: 100+ lines (NEW)
- **Tutorial Notebook**: 20 cells, 7 sections (NEW)
- **Production Workflow**: 450+ lines (NEW)
- **Total**: ~650 lines of documentation + config

### Test Coverage

- Tutorial: Syntactically validated (no runtime tests yet)
- Workflow: CLI commands validated against existing scripts
- **Pending**: End-to-end test with real 100-task batch

### Integration Points

- **Phase 4.5**: Evaluation harness, visualization tools, CLI integration
- **Phase 5**: Batch preparation (pending), conversion (pending)
- **Phase 6**: Fine-tuning pipeline (planned)

---

**Questions?** See updated `.github/copilot-instructions.md` or open a GitHub issue.
