{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b748dda4",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "### Why Manual Annotation?\n",
    "\n",
    "**Weak labels** (rule-based, lexicon-driven) provide a starting point but have limitations:\n",
    "- **Boundary errors**: \"severe burning sensation\" vs \"burning sensation\"\n",
    "- **False positives**: Matching anatomy tokens (\"skin\") without context\n",
    "- **Missing synonyms**: Lexicons incomplete for colloquial phrasing\n",
    "\n",
    "**LLM refinement** improves weak labels but still needs human validation:\n",
    "- +8-15% IOU improvement over weak labels alone\n",
    "- 5-10% worsened rate (over-correction, hallucination)\n",
    "\n",
    "**Gold standard annotations** enable:\n",
    "- Fine-tuning BioBERT for domain-specific NER (target: F1 >0.90)\n",
    "- Evaluation harness for measuring weak/LLM quality\n",
    "- Iterative improvement of heuristics and prompts\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "   ‚Üì\n",
    "Weak Labels (lexicon + fuzzy matching)\n",
    "   ‚Üì\n",
    "LLM Refinement (boundary correction, canonical normalization)\n",
    "   ‚Üì\n",
    "Human Annotation (Label Studio)\n",
    "   ‚Üì\n",
    "Gold Standard JSONL\n",
    "   ‚Üì\n",
    "Evaluation (IOU improvement, correction rate, P/R/F1)\n",
    "   ‚Üì\n",
    "Fine-Tuned BioBERT Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e181d4",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation\n",
    "\n",
    "### Load Weak Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load weak labels from test fixtures\n",
    "weak_path = Path('tests/fixtures/annotation/weak_baseline.jsonl')\n",
    "\n",
    "weak_records = []\n",
    "with open(weak_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            weak_records.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(weak_records)} records\")\n",
    "print(f\"\\nSample record:\")\n",
    "print(json.dumps(weak_records[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afa149",
   "metadata": {},
   "source": [
    "### Explore Weak Label Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract span statistics\n",
    "all_spans = []\n",
    "for rec in weak_records:\n",
    "    for span in rec.get('spans', []):\n",
    "        all_spans.append({\n",
    "            'text': span['text'],\n",
    "            'label': span['label'],\n",
    "            'confidence': span.get('confidence', 1.0),\n",
    "            'length': len(span['text'])\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_spans)\n",
    "\n",
    "print(\"\\n=== Weak Label Statistics ===\")\n",
    "print(f\"Total spans: {len(df)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "print(df['confidence'].describe())\n",
    "print(f\"\\nSpan length distribution:\")\n",
    "print(df['length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc99fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Confidence histogram\n",
    "axes[0].hist(df['confidence'], bins=20, alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Confidence Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Weak Label Confidence Distribution')\n",
    "axes[0].axvline(df['confidence'].median(), color='red', linestyle='--', label=f'Median: {df[\"confidence\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Label counts\n",
    "label_counts = df['label'].value_counts()\n",
    "axes[1].bar(label_counts.index, label_counts.values, alpha=0.7, color=['green', 'blue'])\n",
    "axes[1].set_xlabel('Entity Type')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Entity Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: Low-confidence spans (< 0.80) will benefit most from LLM refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55906b6f",
   "metadata": {},
   "source": [
    "## Section 3: LLM Refinement Demo\n",
    "\n",
    "### Compare Weak vs LLM-Refined Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM-refined labels\n",
    "llm_path = Path('tests/fixtures/annotation/gold_with_llm_refined.jsonl')\n",
    "\n",
    "llm_records = []\n",
    "with open(llm_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            llm_records.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(llm_records)} LLM-refined records\\n\")\n",
    "\n",
    "# Compare first record\n",
    "rec = llm_records[0]\n",
    "print(f\"Text: {rec['text']}\\n\")\n",
    "\n",
    "print(\"WEAK LABELS:\")\n",
    "for span in rec.get('spans', []):\n",
    "    print(f\"  - [{span['start']}-{span['end']}] '{span['text']}' ({span['label']}, conf={span.get('confidence', 1.0):.2f})\")\n",
    "\n",
    "print(\"\\nLLM SUGGESTIONS:\")\n",
    "for span in rec.get('llm_suggestions', []):\n",
    "    print(f\"  - [{span['start']}-{span['end']}] '{span['text']}' ({span['label']})\")\n",
    "    if 'rationale' in span:\n",
    "        print(f\"    Rationale: {span['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce218817",
   "metadata": {},
   "source": [
    "### Highlight Boundary Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find modified spans\n",
    "for rec in llm_records:\n",
    "    weak_spans = {(s['start'], s['end'], s['text']) for s in rec.get('spans', [])}\n",
    "    llm_spans = {(s['start'], s['end'], s['text']) for s in rec.get('llm_suggestions', [])}\n",
    "    \n",
    "    changed = weak_spans ^ llm_spans  # Symmetric difference\n",
    "    if changed:\n",
    "        print(f\"\\nüìù Task: {rec['id']}\")\n",
    "        print(f\"Text: {rec['text'][:80]}...\")\n",
    "        \n",
    "        # Show before/after\n",
    "        weak_dict = {(s['start'], s['end']): s for s in rec.get('spans', [])}\n",
    "        llm_dict = {(s['start'], s['end']): s for s in rec.get('llm_suggestions', [])}\n",
    "        \n",
    "        for weak_span in rec.get('spans', []):\n",
    "            weak_key = (weak_span['start'], weak_span['end'])\n",
    "            llm_match = [s for s in rec.get('llm_suggestions', []) if s['label'] == weak_span['label']]\n",
    "            \n",
    "            if llm_match and llm_match[0]['text'] != weak_span['text']:\n",
    "                print(f\"  BEFORE: '{weak_span['text']}' (confidence={weak_span.get('confidence', 1.0):.2f})\")\n",
    "                print(f\"  AFTER:  '{llm_match[0]['text']}'\")\n",
    "                if 'rationale' in llm_match[0]:\n",
    "                    print(f\"  WHY:    {llm_match[0]['rationale']}\")\n",
    "\n",
    "print(\"\\n‚úÖ LLM typically corrects:\")\n",
    "print(\"   - Removes adjectives: 'severe burning' ‚Üí 'burning'\")\n",
    "print(\"   - Trims determiners: 'the redness' ‚Üí 'redness'\")\n",
    "print(\"   - Normalizes to canonical: 'itching' ‚Üí 'pruritus' (if in lexicon)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4f24d",
   "metadata": {},
   "source": [
    "## Section 4: Label Studio Setup\n",
    "\n",
    "### Install Label Studio (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad811ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Label Studio is installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['label-studio', '--version'], capture_output=True, text=True)\n",
    "    print(f\"‚úÖ Label Studio installed: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Label Studio not found. Install with:\")\n",
    "    print(\"   pip install label-studio\")\n",
    "    print(\"\\nAfter installation, disable telemetry:\")\n",
    "    print(\"   PowerShell: $env:LABEL_STUDIO_DISABLE_TELEMETRY=1\")\n",
    "    print(\"   CMD: set LABEL_STUDIO_DISABLE_TELEMETRY=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84492ce",
   "metadata": {},
   "source": [
    "### Import Configuration\n",
    "\n",
    "**Manual Steps** (one-time setup):\n",
    "\n",
    "1. **Launch Label Studio**:\n",
    "   ```bash\n",
    "   label-studio start\n",
    "   ```\n",
    "   Opens at http://localhost:8080\n",
    "\n",
    "2. **Create Project**:\n",
    "   - Click \"Create Project\"\n",
    "   - Name: \"Adverse Event NER\"\n",
    "   - Description: \"Symptom and product annotation for biomedical complaints\"\n",
    "\n",
    "3. **Import Label Config**:\n",
    "   - Go to Settings ‚Üí Labeling Interface\n",
    "   - Click \"Code\" tab\n",
    "   - Copy contents from `data/annotation/config/label_config.xml`\n",
    "   - Click \"Save\"\n",
    "\n",
    "4. **Import Tasks**:\n",
    "   - Go to project dashboard\n",
    "   - Click \"Import\" button\n",
    "   - Upload JSON file (generated below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Label Studio import JSON with pre-annotations\n",
    "output_path = Path('data/annotation/imports/tutorial_tasks.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tasks = []\n",
    "for rec in llm_records[:5]:  # First 5 tasks for tutorial\n",
    "    task = {\n",
    "        'data': {'text': rec['text']},\n",
    "        'predictions': [{\n",
    "            'result': [\n",
    "                {\n",
    "                    'value': {\n",
    "                        'start': s['start'],\n",
    "                        'end': s['end'],\n",
    "                        'text': s['text'],\n",
    "                        'labels': [s['label']]\n",
    "                    },\n",
    "                    'from_name': 'label',\n",
    "                    'to_name': 'text',\n",
    "                    'type': 'labels'\n",
    "                }\n",
    "                for s in rec.get('llm_suggestions', rec.get('spans', []))\n",
    "            ]\n",
    "        }]\n",
    "    }\n",
    "    tasks.append(task)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tasks, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Exported {len(tasks)} tasks to {output_path}\")\n",
    "print(f\"\\nImport to Label Studio:\")\n",
    "print(f\"  1. Open Label Studio project\")\n",
    "print(f\"  2. Click 'Import' button\")\n",
    "print(f\"  3. Upload {output_path}\")\n",
    "print(f\"  4. Tasks will appear with pre-annotations (LLM suggestions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44104f",
   "metadata": {},
   "source": [
    "## Section 5: Annotation Practice\n",
    "\n",
    "### Example 1: Boundary Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec838283",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = \"\"\"\n",
    "Patient reports severe burning sensation after applying the cream.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(example_1)\n",
    "\n",
    "print(\"\\nWEAK LABEL:\")\n",
    "print(\"  - 'severe burning sensation' (SYMPTOM, confidence=0.82)\")\n",
    "\n",
    "print(\"\\nLLM SUGGESTION:\")\n",
    "print(\"  - 'burning sensation' (SYMPTOM)\")\n",
    "print(\"  Rationale: Removed non-medical adjective 'severe'\")\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT ANNOTATION:\")\n",
    "print(\"  - 'burning sensation' (SYMPTOM) [character span: 15-33]\")\n",
    "print(\"  - 'the cream' (PRODUCT) [character span: 48-57]\")\n",
    "\n",
    "print(\"\\nüìñ RULE:\")\n",
    "print(\"  Exclude intensity adjectives (severe, mild, slight) from symptom spans.\")\n",
    "print(\"  Medical lexicons use canonical terms without modifiers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5ed7e",
   "metadata": {},
   "source": [
    "### Example 2: Negation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2 = \"\"\"\n",
    "No redness observed, but patient complains of itching.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(example_2)\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT ANNOTATION:\")\n",
    "print(\"  - 'redness' (SYMPTOM) [character span: 3-10]\")\n",
    "print(\"  - 'itching' (SYMPTOM) [character span: 45-52]\")\n",
    "\n",
    "print(\"\\nüìñ RULE:\")\n",
    "print(\"  Annotate negated symptoms (e.g., 'no redness') as SYMPTOM spans.\")\n",
    "print(\"  Do NOT include the negation word ('no', 'without', 'absence of').\")\n",
    "print(\"  Rationale: Model can learn negation context from surrounding tokens.\")\n",
    "\n",
    "print(\"\\n‚ùì OPTIONAL:\")\n",
    "print(\"  If Label Studio supports custom attributes, add 'negated=true' flag.\")\n",
    "print(\"  See label_config.xml for negation checkbox option.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387fedf",
   "metadata": {},
   "source": [
    "### Example 3: Anatomy Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b565bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_3 = \"\"\"\n",
    "Facial swelling appeared on skin after exposure.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(example_3)\n",
    "\n",
    "print(\"\\n‚ùå INCORRECT:\")\n",
    "print(\"  - 'Facial' (SYMPTOM) ‚Üê Single anatomy token, not a symptom\")\n",
    "print(\"  - 'skin' (SYMPTOM) ‚Üê Single anatomy token without symptom context\")\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT ANNOTATION:\")\n",
    "print(\"  - 'swelling' (SYMPTOM) [character span: 7-15]\")\n",
    "\n",
    "print(\"\\nüìñ RULE:\")\n",
    "print(\"  Skip single anatomy tokens (skin, face, arm, leg, etc.) UNLESS:\")\n",
    "print(\"  1. Part of multi-word symptom: 'facial swelling', 'skin rash'\")\n",
    "print(\"  2. Symptom keyword present in lexicon: 'facial swelling' (if lexicon has compound term)\")\n",
    "\n",
    "print(\"\\nüí° TIP:\")\n",
    "print(\"  If lexicon has 'facial swelling' as canonical term, annotate full phrase.\")\n",
    "print(\"  Otherwise, annotate only 'swelling' (core symptom).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81183120",
   "metadata": {},
   "source": [
    "### Example 4: Multi-Word Medical Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_4 = \"\"\"\n",
    "Patient experienced anaphylactic shock and difficulty breathing.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(example_4)\n",
    "\n",
    "print(\"\\n‚ùå INCORRECT:\")\n",
    "print(\"  - 'shock' (SYMPTOM) ‚Üê Incomplete medical term\")\n",
    "print(\"  - 'breathing' (SYMPTOM) ‚Üê Missing context ('difficulty')\")\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT ANNOTATION:\")\n",
    "print(\"  - 'anaphylactic shock' (SYMPTOM) [character span: 20-39]\")\n",
    "print(\"  - 'difficulty breathing' (SYMPTOM) [character span: 44-63]\")\n",
    "\n",
    "print(\"\\nüìñ RULE:\")\n",
    "print(\"  Preserve multi-word medical terms from lexicon:\")\n",
    "print(\"  - 'anaphylactic shock' (not 'shock' alone)\")\n",
    "print(\"  - 'burning sensation' (not 'burning' alone)\")\n",
    "print(\"  - 'difficulty breathing' (not 'breathing' alone)\")\n",
    "\n",
    "print(\"\\nüí° TIP:\")\n",
    "print(\"  When uncertain, check lexicon (data/lexicon/symptoms.csv).\")\n",
    "print(\"  If multi-word term exists, use full phrase. Otherwise, use core symptom.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46cc2d",
   "metadata": {},
   "source": [
    "### Example 5: Overlapping Conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae09068",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_5 = \"\"\"\n",
    "Redness and swelling observed at injection site.\n",
    "\"\"\"\n",
    "\n",
    "print(\"TEXT:\")\n",
    "print(example_5)\n",
    "\n",
    "print(\"\\n‚ùå INCORRECT:\")\n",
    "print(\"  - 'Redness and swelling' (SYMPTOM) ‚Üê Conjunction included\")\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT ANNOTATION:\")\n",
    "print(\"  - 'Redness' (SYMPTOM) [character span: 0-7]\")\n",
    "print(\"  - 'swelling' (SYMPTOM) [character span: 12-20]\")\n",
    "\n",
    "print(\"\\nüìñ RULE:\")\n",
    "print(\"  Annotate symptoms separately when connected by conjunctions (and, or).\")\n",
    "print(\"  Exclude the conjunction itself from spans.\")\n",
    "\n",
    "print(\"\\n‚ùì EDGE CASE:\")\n",
    "print(\"  If lexicon has compound symptom with 'and' (rare):\")\n",
    "print(\"  - 'red and swollen' ‚Üí Check lexicon first\")\n",
    "print(\"  - Default: Separate spans unless lexicon explicitly lists compound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff5723",
   "metadata": {},
   "source": [
    "## Section 6: Export & Evaluation\n",
    "\n",
    "### Export from Label Studio\n",
    "\n",
    "**Manual Steps**:\n",
    "\n",
    "1. **Complete Annotations**:\n",
    "   - Annotate all imported tasks in Label Studio\n",
    "   - Click \"Submit\" after each task\n",
    "\n",
    "2. **Export JSON**:\n",
    "   - Go to project dashboard\n",
    "   - Click \"Export\" button\n",
    "   - Select \"JSON\" format\n",
    "   - Download file (e.g., `project-1-export.json`)\n",
    "\n",
    "3. **Save to Data Directory**:\n",
    "   - Move exported file to `data/annotation/raw/tutorial_export.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca7f72",
   "metadata": {},
   "source": [
    "### Convert to Gold Standard JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversion script (after manual export)\n",
    "import subprocess\n",
    "\n",
    "convert_cmd = [\n",
    "    'python', 'scripts/annotation/convert_labelstudio.py',\n",
    "    '--input', 'data/annotation/raw/tutorial_export.json',\n",
    "    '--output', 'data/gold/tutorial_gold.jsonl',\n",
    "    '--source', 'tutorial_batch',\n",
    "    '--annotator', 'tutorial_user',\n",
    "    '--symptom-lexicon', 'data/lexicon/symptoms.csv',\n",
    "    '--product-lexicon', 'data/lexicon/products.csv'\n",
    "]\n",
    "\n",
    "print(\"Converting Label Studio export to gold JSONL...\\n\")\n",
    "print(\"Command:\")\n",
    "print(' '.join(convert_cmd))\n",
    "print(\"\\n(Run after completing Label Studio annotation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c0a6a",
   "metadata": {},
   "source": [
    "### Run Evaluation Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate annotation quality (after conversion)\n",
    "eval_cmd = [\n",
    "    'python', 'scripts/annotation/cli.py', 'evaluate-llm',\n",
    "    '--weak', 'tests/fixtures/annotation/weak_baseline.jsonl',\n",
    "    '--refined', 'tests/fixtures/annotation/gold_with_llm_refined.jsonl',\n",
    "    '--gold', 'data/gold/tutorial_gold.jsonl',\n",
    "    '--output', 'data/annotation/reports/tutorial_eval.json',\n",
    "    '--markdown',\n",
    "    '--stratify', 'label', 'confidence'\n",
    "]\n",
    "\n",
    "print(\"Evaluating annotation quality...\\n\")\n",
    "print(\"Command:\")\n",
    "print(' '.join(eval_cmd))\n",
    "print(\"\\n(Run after converting gold JSONL)\")\n",
    "\n",
    "print(\"\\nüìä Expected Metrics:\")\n",
    "print(\"  - IOU Improvement: +8-15% (weak ‚Üí LLM vs gold)\")\n",
    "print(\"  - Exact Match Rate: 70-85% (LLM boundaries align with gold)\")\n",
    "print(\"  - Correction Rate: >60% improved, <10% worsened\")\n",
    "print(\"  - F1 Score: >0.85 (LLM precision/recall vs gold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54172e30",
   "metadata": {},
   "source": [
    "### Interpret Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation report (if exists)\n",
    "eval_path = Path('data/annotation/reports/tutorial_eval.json')\n",
    "\n",
    "if eval_path.exists():\n",
    "    with open(eval_path, 'r') as f:\n",
    "        eval_report = json.load(f)\n",
    "    \n",
    "    overall = eval_report.get('overall', {})\n",
    "    \n",
    "    print(\"=== EVALUATION SUMMARY ===\")\n",
    "    print(f\"\\nIOU Improvement:\")\n",
    "    print(f\"  Weak:  {overall.get('weak_mean_iou', 0):.3f}\")\n",
    "    print(f\"  LLM:   {overall.get('llm_mean_iou', 0):.3f}\")\n",
    "    print(f\"  Delta: +{overall.get('iou_delta', 0):.3f} ({overall.get('iou_improvement_pct', 0):.1f}%)\")\n",
    "    \n",
    "    correction = overall.get('correction_rate', {})\n",
    "    print(f\"\\nCorrection Rate:\")\n",
    "    print(f\"  Improved:  {correction.get('improved', 0)}/{correction.get('total_modified', 0)} ({correction.get('improved_pct', 0):.1f}%)\")\n",
    "    print(f\"  Worsened:  {correction.get('worsened', 0)}/{correction.get('total_modified', 0)} ({correction.get('worsened_pct', 0):.1f}%)\")\n",
    "    \n",
    "    llm_prf = overall.get('llm_prf', {})\n",
    "    print(f\"\\nLLM Performance:\")\n",
    "    print(f\"  Precision: {llm_prf.get('precision', 0):.3f}\")\n",
    "    print(f\"  Recall:    {llm_prf.get('recall', 0):.3f}\")\n",
    "    print(f\"  F1:        {llm_prf.get('f1', 0):.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Good quality indicators:\")\n",
    "    if overall.get('iou_improvement_pct', 0) >= 8:\n",
    "        print(\"  ‚úì IOU improvement ‚â•8% (strong LLM refinement)\")\n",
    "    if correction.get('worsened_pct', 100) < 10:\n",
    "        print(\"  ‚úì Worsened rate <10% (LLM rarely introduces errors)\")\n",
    "    if llm_prf.get('f1', 0) >= 0.85:\n",
    "        print(\"  ‚úì F1 score ‚â•0.85 (high precision and recall)\")\n",
    "else:\n",
    "    print(\"‚è≥ Evaluation report not found. Complete annotation workflow first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fe344",
   "metadata": {},
   "source": [
    "## Section 7: Common Mistakes & Glossary\n",
    "\n",
    "### Common Annotation Errors\n",
    "\n",
    "#### 1. Including Intensity Adjectives\n",
    "‚ùå **Incorrect**: \"severe burning sensation\"  \n",
    "‚úÖ **Correct**: \"burning sensation\"  \n",
    "**Why**: Lexicons use canonical terms without modifiers\n",
    "\n",
    "#### 2. Missing Negation Context\n",
    "‚ùå **Incorrect**: Skip \"no redness\" (negated symptom)  \n",
    "‚úÖ **Correct**: Annotate \"redness\" as SYMPTOM  \n",
    "**Why**: Model learns negation from context; skipping loses training signal\n",
    "\n",
    "#### 3. Single Anatomy Tokens\n",
    "‚ùå **Incorrect**: \"skin\" alone (without symptom context)  \n",
    "‚úÖ **Correct**: Skip unless part of compound: \"skin rash\"  \n",
    "**Why**: Anatomy is not a symptom; causes false positives\n",
    "\n",
    "#### 4. Truncating Multi-Word Terms\n",
    "‚ùå **Incorrect**: \"shock\" (incomplete)  \n",
    "‚úÖ **Correct**: \"anaphylactic shock\" (full medical term)  \n",
    "**Why**: Lexicons preserve clinical meaning with compound terms\n",
    "\n",
    "#### 5. Including Conjunctions\n",
    "‚ùå **Incorrect**: \"redness and swelling\" (single span)  \n",
    "‚úÖ **Correct**: \"redness\" + \"swelling\" (separate spans)  \n",
    "**Why**: Each symptom is distinct entity\n",
    "\n",
    "### Symptom Glossary (Canonical Terms)\n",
    "\n",
    "| Colloquial | Canonical | Notes |\n",
    "|------------|-----------|-------|\n",
    "| itching | pruritus | Prefer medical term if in lexicon |\n",
    "| redness | erythema | Both acceptable; lexicon determines |\n",
    "| swelling | edema | Swelling more common in complaints |\n",
    "| burning | burning sensation | Use full phrase if lexicon has it |\n",
    "| dry skin | dryness | Canonical form without anatomy |\n",
    "| shortness of breath | dyspnea | Medical term preferred |\n",
    "| dizziness | vertigo | Technically distinct; context matters |\n",
    "\n",
    "**Rule of Thumb**: Check `data/lexicon/symptoms.csv` for canonical form. If colloquial term present, use as-is.\n",
    "\n",
    "### Product Annotation Tips\n",
    "\n",
    "- **Brand Names**: Annotate as written (\"Advil\", \"Tylenol\")\n",
    "- **Generic Names**: Lowercase OK (\"ibuprofen\", \"acetaminophen\")\n",
    "- **Abbreviations**: Include if common (\"NSAIDs\", \"OTC meds\")\n",
    "- **Descriptors**: Exclude generic descriptors (\"the medication\" ‚Üí skip)\n",
    "- **Combinations**: Annotate full product name (\"Advil PM\", not just \"Advil\")\n",
    "\n",
    "### Boundary Decision Tree\n",
    "\n",
    "```\n",
    "Is span a single anatomy token (skin, face, arm)?\n",
    "‚îú‚îÄ Yes ‚Üí Skip UNLESS part of compound symptom (\"skin rash\")\n",
    "‚îî‚îÄ No ‚Üí Continue\n",
    "\n",
    "Does span include intensity adjective (severe, mild, slight)?\n",
    "‚îú‚îÄ Yes ‚Üí Remove adjective, keep core symptom\n",
    "‚îî‚îÄ No ‚Üí Continue\n",
    "\n",
    "Is span multi-word? (\"burning sensation\", \"anaphylactic shock\")\n",
    "‚îú‚îÄ Yes ‚Üí Check lexicon for canonical compound term\n",
    "‚îÇ   ‚îú‚îÄ In lexicon ‚Üí Use full phrase\n",
    "‚îÇ   ‚îî‚îÄ Not in lexicon ‚Üí Use core symptom only\n",
    "‚îî‚îÄ No ‚Üí Annotate single-word symptom\n",
    "\n",
    "Is span negated? (\"no redness\", \"without swelling\")\n",
    "‚îú‚îÄ Yes ‚Üí Annotate symptom ONLY (exclude \"no\", \"without\")\n",
    "‚îî‚îÄ No ‚Üí Annotate as-is\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff8d85",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **Annotation Pipeline**: Raw text ‚Üí weak labels ‚Üí LLM refinement ‚Üí human curation ‚Üí gold standard  \n",
    "‚úÖ **Quality Metrics**: IOU improvement, correction rate, precision/recall/F1  \n",
    "‚úÖ **Boundary Rules**: Exclude adjectives, preserve multi-word terms, separate conjunctions  \n",
    "‚úÖ **Edge Cases**: Negation handling, anatomy gating, canonical normalization  \n",
    "‚úÖ **Evaluation**: Measuring annotation quality with evaluation harness\n",
    "\n",
    "### Production Workflow\n",
    "\n",
    "1. **Prepare Batch** (100 complaints):\n",
    "   ```bash\n",
    "   python scripts/annotation/prepare_production_batch.py \\\n",
    "     --input raw_complaints.txt \\\n",
    "     --output data/annotation/batches/batch_001/ \\\n",
    "     --batch-size 100\n",
    "   ```\n",
    "\n",
    "2. **Import to Label Studio**:\n",
    "   - Upload `batches/batch_001/tasks.json`\n",
    "   - Pre-annotations included (LLM suggestions)\n",
    "\n",
    "3. **Annotate** (2-3 hours per 100 tasks)\n",
    "\n",
    "4. **Export & Convert**:\n",
    "   ```bash\n",
    "   python scripts/annotation/convert_labelstudio.py \\\n",
    "     --input label_studio_export.json \\\n",
    "     --output data/gold/batch_001.jsonl \\\n",
    "     --annotator your_name\n",
    "   ```\n",
    "\n",
    "5. **Evaluate**:\n",
    "   ```bash\n",
    "   python scripts/annotation/cli.py evaluate-llm \\\n",
    "     --weak batches/batch_001/weak.jsonl \\\n",
    "     --refined batches/batch_001/llm_refined.jsonl \\\n",
    "     --gold data/gold/batch_001.jsonl \\\n",
    "     --output reports/batch_001_eval.json \\\n",
    "     --markdown --stratify label confidence\n",
    "   ```\n",
    "\n",
    "6. **Iterate**: Refine prompts/lexicons based on evaluation feedback\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Annotation Guide**: `docs/annotation_guide.md`\n",
    "- **Production Evaluation Guide**: `docs/production_evaluation.md`\n",
    "- **Phase 5 Plan**: `docs/phase_5_plan.md`\n",
    "- **LLM Providers**: `docs/llm_providers.md`\n",
    "\n",
    "### Questions?\n",
    "\n",
    "- Review annotation guide for boundary rules\n",
    "- Check lexicons (`data/lexicon/`) for canonical terms\n",
    "- Run evaluation harness to measure quality\n",
    "- Open GitHub issue for technical problems\n",
    "\n",
    "**Happy annotating! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
