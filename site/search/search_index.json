{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SpanForge","text":"<p>Biomedical Named Entity Recognition with BioBERT, Weak Labeling, and LLM Refinement</p> <p> </p> <p>SpanForge is a biomedical NER pipeline combining BioBERT contextual embeddings with lexicon-driven weak labeling and LLM-powered refinement for adverse event detection in consumer complaints. It includes an end-to-end annotation workflow (Label Studio), evaluation harness (10 metrics), and visualization tools.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd2c BioBERT Integration: State-of-the-art biomedical language model</li> <li>\ud83d\udcdd Weak Labeling: Fuzzy (0.88) + Jaccard gate (\u226540) with confidence scoring</li> <li>\ud83d\udeab Negation Detection: Bidirectional window (\u00b15 tokens), emoji handling</li> <li>\ud83e\udd16 LLM Refinement: Boundary correction, negation validation, canonical normalization (OpenAI/Azure/Anthropic)</li> <li>\u26a1 Fast Processing: &lt;100ms per document average (short texts)</li> <li>\ud83e\uddea Well-Tested: 296 tests; 99.3% passing (1 flaky performance test)</li> <li>\ud83d\udd04 CI/CD Ready: GitHub Actions test + security (Bandit, Safety, CodeQL)</li> <li>\ud83e\uddf0 Data Integration: FDA CAERS ingestion and weak labeling (<code>scripts/caers/download_caers.py</code>)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/paulboys/SpanForge.git\ncd SpanForge\npip install -r requirements.txt\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from src.weak_label import load_symptom_lexicon, load_product_lexicon, weak_label\nfrom pathlib import Path\n\n# Load lexicons\nsymptom_lex = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\"))\nproduct_lex = load_product_lexicon(Path(\"data/lexicon/products.csv\"))\n\n# Detect entities\ntext = \"After using this face cream, I developed severe burning sensation and redness.\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text} ({span.label}): {span.canonical} [conf={span.confidence:.2f}]\")\n# Output:\n# severe rash (SYMPTOM): Rash [conf=1.00]\n# hydra boost cream (PRODUCT): Hydra Boost Cream [conf=1.00]\n</code></pre>"},{"location":"#pipeline-inference","title":"Pipeline Inference","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\n    \"No irritation from the face wash, just mild dryness\",\n    \"The moisturizer caused redness and itching\"\n]\n\nresults = simple_inference(texts, persist_path=\"data/output/notebook_test.jsonl\")\n\nfor result in results:\n    print(f\"Found {len(result['weak_spans'])} entities\")\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Raw Text] --&gt; B[Weak Labels]\n    B --&gt; C[LLM Refinement]\n    C --&gt; D[Label Studio]\n    D --&gt; E[Gold Standard]\n    E --&gt; F[Evaluation + Visualization]\n    F --&gt; G[Model Training]</code></pre>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#1-weak-labeling","title":"1. Weak Labeling","text":"<ul> <li>Fuzzy Matching: WRatio \u226588 with Jaccard token-set \u226540</li> <li>Exact Matching: Case-insensitive with word boundaries</li> <li>Confidence Formula: <code>0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score</code></li> </ul>"},{"location":"#2-negation-detection","title":"2. Negation Detection","text":"<ul> <li>Bidirectional Windows: Forward (\"no itching\") + backward (\"itching absent\")</li> <li>Extended Cues: Clinical terms (absent, denies, negative) + resolution indicators (cleared, improved)</li> <li>Prefix Matching: Handles variants (resolved \u2192 resolv)</li> </ul>"},{"location":"#3-span-processing","title":"3. Span Processing","text":"<ul> <li>Overlap Resolution: Exact duplicate removal, contextual mention preservation</li> <li>Anatomy Gating: Skips generic single-token anatomy terms</li> <li>Last-Token Alignment: Multi-token fuzzy matches require matching final token</li> </ul>"},{"location":"#benchmarks-fixture-based","title":"Benchmarks (Fixture-based)","text":"<ul> <li>IOU uplift: +13.4% (weak \u2192 LLM)</li> <li>Exact match: 66.7% \u2192 100.0% (after refinement)</li> <li>P/R/F1: 1.000 (LLM spans vs gold fixtures)</li> <li>Avg. Time/Doc: &lt;100ms (short texts)</li> </ul>"},{"location":"#testing","title":"Testing","text":"<pre><code># Full suite\npytest -q\n\n# With coverage\npytest tests/ --cov=src --cov-report=html\n\n# Specific categories\npytest tests/edge_cases/ -v      # 98 edge cases\npytest tests/integration/ -v     # 26 integration tests\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation available at: SpanForge Docs</p> <ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Contributing Guide</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>SpanForge/\n\u251c\u2500\u2500 src/               # Core source code\n\u2502   \u251c\u2500\u2500 config.py      # Configuration management\n\u2502   \u251c\u2500\u2500 model.py       # BioBERT loading\n\u2502   \u251c\u2500\u2500 weak_label.py  # Weak labeling logic\n\u2502   \u251c\u2500\u2500 pipeline.py    # End-to-end pipeline\n\u2502   \u2514\u2500\u2500 llm_agent.py   # LLM refinement (experimental)\n\u251c\u2500\u2500 tests/             # Test suite (296 tests)\n\u2502   \u251c\u2500\u2500 fixtures/      # Annotation/evaluation fixtures\n\u2502   \u251c\u2500\u2500 weak_labeling/ # Edge cases and heuristics\n\u2502   \u251c\u2500\u2500 llm/           # LLM agent tests\n\u2502   \u2514\u2500\u2500 evaluation/    # Metrics and end-to-end checks\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 lexicon/       # Symptom &amp; product lexicons\n\u2502   \u2514\u2500\u2500 output/        # Pipeline outputs\n\u251c\u2500\u2500 scripts/           # Utility scripts\n\u251c\u2500\u2500 docs/              # MkDocs documentation\n\u2514\u2500\u2500 .github/           # CI/CD workflows\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Phase 1: Bootstrap &amp; Lexicon</li> <li> Phase 2: Weak Label Refinement</li> <li> Phase 3: Test Infrastructure &amp; Edge Cases</li> <li> Phase 4: CI/CD Integration</li> <li> Phase 4.5: LLM Refinement &amp; Evaluation Harness</li> <li> Phase 5: Annotation &amp; Curation Infrastructure (Label Studio config, tutorial, production workflow)</li> <li> Phase 5 (continued): Batch preparation scripts, first 100-task production batch</li> <li> Phase 6: Gold Standard Assembly (500+ annotations)</li> <li> Phase 7: Token Classification Fine-Tuning</li> <li> Phase 8: Domain Adaptation (MLM)</li> <li> Phase 9: Baseline Comparison (RoBERTa)</li> <li> Phase 10: Evaluation &amp; Calibration</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions welcome! Please see Contributing Guide for guidelines.</p> <ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> <li>Commit changes (<code>git commit -m 'Add amazing feature'</code>)</li> <li>Push to branch (<code>git push origin feature/amazing-feature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"#license","title":"License","text":"<p>MIT License - see License for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use SpanForge in your research, please cite:</p> <pre><code>@software{spanforge2025,\n  title = {SpanForge: Biomedical NER with BioBERT and Weak Labeling},\n  author = {SpanForge Contributors},\n  year = {2025},\n  url = {https://github.com/paulboys/SpanForge}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>BioBERT: Lee et al., \"BioBERT: a pre-trained biomedical language representation model\"</li> <li>Hugging Face Transformers: For model infrastructure</li> <li>RapidFuzz: For high-performance fuzzy matching</li> </ul> <p>Status: Annotation-Ready | Version: 0.5.0 | Last Updated: November 28, 2025</p>"},{"location":"annotation_guide/","title":"Annotation Guide","text":"SpanForge Annotation Guide <p>Standards for consistent SYMPTOM and PRODUCT span curation enabling high-fidelity adverse event modeling.</p>"},{"location":"annotation_guide/#annotation-guide","title":"Annotation Guide","text":"<p>Defines consistent rules for annotating SYMPTOM and PRODUCT spans in consumer adverse event complaints.</p>"},{"location":"annotation_guide/#objectives","title":"Objectives","text":"<ol> <li>Capture clinically meaningful symptom phrases (granular, complete).</li> <li>Standardize product references for model association.</li> <li>Preserve negated context (annotate + flag) for future modeling of absence.</li> <li>Minimize ambiguity and overlapping conflicts.</li> </ol>"},{"location":"annotation_guide/#labels","title":"Labels","text":"<ul> <li>SYMPTOM: Physiological or subjective adverse effect (\"redness\", \"severe rash\", \"nausea\", \"stinging\").</li> <li>PRODUCT: Product name/formulation or clear category reference (\"vitamin serum\", \"exfoliating scrub\").</li> </ul>"},{"location":"annotation_guide/#span-boundary-rules","title":"Span Boundary Rules","text":"Rule Examples Include modifiers integral to meaning <code>severe rash</code>, <code>mild dryness</code>, <code>burning pain</code> Exclude trailing punctuation <code>itching.</code> \u2192 <code>itching</code> Avoid partial capture Prefer <code>tiny itching spots</code> over <code>itching</code> alone Keep internal spacing &amp; casing as-is <code>hydra boost cream</code> preserved Exclude unrelated conjunctions <code>rash and</code> \u2192 <code>rash</code>"},{"location":"annotation_guide/#negation-handling-annotate-flag","title":"Negation Handling (Annotate + Flag)","text":"<p>Annotate negated symptoms (\"no irritation\", \"without redness\") and rely on conversion phase to set <code>negated=True</code>. This supports training for presence vs absence.</p> <p>Do NOT annotate if term clearly unrelated to adverse context (e.g., \"no product issues\" \u2192 skip <code>issues</code>).</p>"},{"location":"annotation_guide/#anatomy-tokens","title":"Anatomy Tokens","text":"<p>Skip isolated anatomy (<code>face</code>, <code>skin</code>) unless part of explicit symptom phrase (\"skin irritation\" \u2192 annotate <code>skin irritation</code>).</p>"},{"location":"annotation_guide/#overlaps-nested-spans","title":"Overlaps &amp; Nested Spans","text":"<ul> <li>Choose the most semantically complete span (<code>severe burning pain</code> preferred).</li> <li>If two plausible alternatives and uncertainty persists: keep both \u2192 adjudication tool resolves.</li> </ul>"},{"location":"annotation_guide/#product-vs-symptom-separation","title":"Product vs Symptom Separation","text":"<p>If a product term appears inside a symptom phrase but functions as a product reference, separate spans where boundaries are clean: <code>serum-induced itching</code> \u2192 <code>serum</code> (PRODUCT), <code>itching</code> (SYMPTOM).</p>"},{"location":"annotation_guide/#canonical-mapping","title":"Canonical Mapping","text":"<p>Surface span text is later mapped to canonical lexicon entries; missing variants should be added to lexicon CSVs. Do not force canonical wording during annotation\u2014capture verbatim text.</p>"},{"location":"annotation_guide/#conflict-resolution-planned-consensus","title":"Conflict Resolution (Planned Consensus)","text":"<ol> <li>Exact match majority for identical spans.</li> <li>Longest span tie-breaker when semantics equivalent.</li> <li>Differing labels on overlap \u2192 adjudication review output to <code>data/annotation/conflicts/</code>.</li> </ol>"},{"location":"annotation_guide/#provenance-fields-after-conversion","title":"Provenance Fields (After Conversion)","text":"<p><code>source</code>, <code>annotator</code>, <code>revision</code>, <code>canonical</code>, optional <code>concept_id</code> automatically injected\u2014no manual action required in UI.</p>"},{"location":"annotation_guide/#quality-checklist-before-export","title":"Quality Checklist Before Export","text":"<ul> <li>Boundaries precise (no punctuation, correct modifiers).</li> <li>Negated spans present (not deleted unless irrelevant context).</li> <li>No duplicate (start,end,label) tuples.</li> <li>Low conflict count (&lt;5% tasks flagged).</li> </ul>"},{"location":"annotation_guide/#edge-case-decisions","title":"Edge Case Decisions","text":"Scenario Action \"dry\" vs \"dryness\" Annotate verbatim form present Misspelling (\"nausia\") Annotate misspelling; canonical normalizes Compound (\"rash and itching\") Two spans if distinct sensations Intensifier only (\"very\") Exclude unless integral (\"very dry skin\" \u2192 include <code>very dry skin</code>) Slang (\"tummy pain\") Annotate; canonical maps to <code>abdominal pain</code> if lexicon contains mapping"},{"location":"annotation_guide/#common-pitfalls","title":"Common Pitfalls","text":"Pitfall Correction Dropping severity adjective Include full phrase Deleting negated symptom Keep &amp; rely on negation flag Including trailing period Trim punctuation Over-extending into next clause Limit to symptom/product phrase only"},{"location":"annotation_guide/#updating-this-guide","title":"Updating This Guide","text":"<p>Revise after initial adjudication cycle (\u2248 first 100 gold tasks). All changes recorded in provenance registry notes for traceability.</p>"},{"location":"annotation_guide/#faq","title":"FAQ","text":"<p>Should I annotate brand names? Yes, if they directly relate to the adverse context.</p> <p>Annotate plural symptoms? Yes; canonical mapping handles singular normalization.</p> <p>What about uncertain reactions? Annotate if consumer asserts possibility (\"might be causing redness\"). Model can later learn uncertainty patterns.</p> <p>Do I merge adjacent symptoms? Only if forming a unified phrase (\"redness and itching\" \u2192 two spans).</p>"},{"location":"annotation_guide/#next-steps","title":"Next Steps","text":"<p>After annotation export: run conversion \u2192 quality report \u2192 register batch \u2192 prepare for BIO tagging.</p>"},{"location":"ci_cd/","title":"CI/CD Documentation","text":""},{"location":"ci_cd/#overview","title":"Overview","text":"<p>SpanForge uses GitHub Actions for continuous integration and delivery. The CI/CD pipeline runs automated tests, linting, and quality checks on every push and pull request.</p>"},{"location":"ci_cd/#workflows","title":"Workflows","text":""},{"location":"ci_cd/#1-test-suite-testyml","title":"1. Test Suite (<code>test.yml</code>)","text":"<p>Triggers: - Push to <code>main</code> or <code>develop</code> branches - Pull requests to <code>main</code> or <code>develop</code> - Manual dispatch via GitHub UI</p> <p>Jobs:</p>"},{"location":"ci_cd/#test-matrix","title":"Test Matrix","text":"<ul> <li>Operating Systems: Ubuntu, Windows</li> <li>Python Versions: 3.9, 3.10, 3.11</li> <li>Total Configurations: 6 (2 OS \u00d7 3 Python versions)</li> </ul> <p>Test Stages: 1. Environment Verification: Runs <code>scripts/verify_env.py</code> 2. Core Tests: Unit tests for weak labeling, model loading, pipeline 3. Edge Case Tests: 98 parametrized edge case tests 4. Integration Tests: End-to-end pipeline and scale tests 5. Coverage Report: Generates coverage with pytest-cov</p> <p>Coverage Upload: - Only on Ubuntu + Python 3.10 configuration - Uploads to Codecov for tracking over time - Failure doesn't block CI (informational only)</p>"},{"location":"ci_cd/#lint-job","title":"Lint Job","text":"<ul> <li>Checks: ruff, black, isort</li> <li>Runs on Ubuntu + Python 3.10</li> <li>Non-blocking (continue-on-error: true)</li> </ul>"},{"location":"ci_cd/#build-check","title":"Build Check","text":"<ul> <li>Validates package can be built with <code>python -m build</code></li> <li>Checks distribution with <code>twine check</code></li> <li>Non-blocking</li> </ul>"},{"location":"ci_cd/#2-pre-commit-checks-pre-commityml","title":"2. Pre-commit Checks (<code>pre-commit.yml</code>)","text":"<p>Triggers: - Pull requests only</p> <p>Checks: - Trailing whitespace removal - End-of-file fixer - YAML/JSON validation - Large file detection (max 1MB) - Private key detection - Code formatting (black, isort, ruff) - Type checking (mypy, non-blocking)</p>"},{"location":"ci_cd/#local-development","title":"Local Development","text":""},{"location":"ci_cd/#setup-pre-commit-hooks","title":"Setup Pre-commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now hooks run automatically on <code>git commit</code>. To run manually:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"ci_cd/#run-tests-locally","title":"Run Tests Locally","text":"<pre><code># Full suite\npytest tests/ -v\n\n# With coverage\npytest tests/ --cov=src --cov-report=html\n\n# Specific category\npytest tests/edge_cases/ -v\npytest tests/integration/ -v\n\n# Parallel execution (faster)\npytest tests/ -n auto\n</code></pre>"},{"location":"ci_cd/#linting","title":"Linting","text":"<pre><code># Check only\nruff check src/ tests/ scripts/\nblack --check src/ tests/ scripts/\nisort --check-only src/ tests/ scripts/\n\n# Auto-fix\nruff check --fix src/ tests/ scripts/\nblack src/ tests/ scripts/\nisort src/ tests/ scripts/\n</code></pre>"},{"location":"ci_cd/#badge-integration","title":"Badge Integration","text":"<p>Add to README.md:</p> <pre><code>![Test Suite](https://github.com/paulboys/SpanForge/actions/workflows/test.yml/badge.svg)\n![Pre-commit](https://github.com/paulboys/SpanForge/actions/workflows/pre-commit.yml/badge.svg)\n[![codecov](https://codecov.io/gh/paulboys/SpanForge/branch/main/graph/badge.svg)](https://codecov.io/gh/paulboys/SpanForge)\n</code></pre>"},{"location":"ci_cd/#pull-request-requirements","title":"Pull Request Requirements","text":"<p>Before merging to <code>main</code>, ensure:</p> <ol> <li>\u2705 All test jobs pass (144/144 tests)</li> <li>\u2705 No ruff/black/isort violations</li> <li>\u2705 Coverage maintained or improved</li> <li>\u2705 Pre-commit checks pass</li> <li>\u2705 Documentation updated if needed</li> </ol>"},{"location":"ci_cd/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Current CI Times (Ubuntu + Python 3.10): - Environment setup: ~30s - Test execution: ~18s - Coverage generation: ~5s - Total: ~1-2 minutes per job</p> <p>Local Performance: - Full suite: ~17s (144 tests) - Edge cases: ~1.3s (98 tests) - Integration: ~44s (26 tests, includes 1000-doc stress test)</p>"},{"location":"ci_cd/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ci_cd/#tests-pass-locally-but-fail-in-ci","title":"Tests Pass Locally but Fail in CI","text":"<p>Check: - OS-specific path separators (<code>os.path.join</code> vs <code>pathlib.Path</code>) - Line endings (CRLF vs LF) - Timezone dependencies - File permissions</p>"},{"location":"ci_cd/#coverage-drops-unexpectedly","title":"Coverage Drops Unexpectedly","text":"<p>Causes: - New untested code paths - Removed tests without removing code - Conditional imports not triggered in test environment</p> <p>Fix: - Add tests for new features - Review coverage HTML report: <code>htmlcov/index.html</code> - Mark uncoverable lines: <code># pragma: no cover</code></p>"},{"location":"ci_cd/#pre-commit-hook-slow","title":"Pre-commit Hook Slow","text":"<p>Solutions: - Skip mypy locally: <code>SKIP=mypy git commit</code> - Run hooks in parallel: <code>pre-commit run --all-files --show-diff-on-failure</code> - Update hooks: <code>pre-commit autoupdate</code></p>"},{"location":"ci_cd/#future-enhancements","title":"Future Enhancements","text":""},{"location":"ci_cd/#planned-additions","title":"Planned Additions:","text":"<ol> <li>Nightly Builds: Extended stress tests (10k documents)</li> <li>Performance Regression: Track inference time over commits</li> <li>Security Scanning: Bandit, safety checks</li> <li>Documentation: Auto-generate API docs with Sphinx</li> <li>Release Automation: Tag-triggered PyPI publish</li> </ol>"},{"location":"ci_cd/#advanced-coverage-goals","title":"Advanced Coverage Goals:","text":"<ul> <li>Target: 90% coverage on <code>src/</code></li> <li>Branch coverage tracking</li> <li>Mutation testing with <code>mutmut</code></li> </ul>"},{"location":"ci_cd/#maintenance","title":"Maintenance","text":""},{"location":"ci_cd/#updating-dependencies","title":"Updating Dependencies","text":"<pre><code># Update GitHub Actions\n# Edit .github/workflows/*.yml, bump action versions\n\n# Update pre-commit hooks\npre-commit autoupdate\n\n# Update Python dependencies\npip list --outdated\n# Update requirements.txt accordingly\n</code></pre>"},{"location":"ci_cd/#monitoring","title":"Monitoring","text":"<ul> <li>GitHub Actions Dashboard: Monitor workflow runs</li> <li>Codecov Dashboard: Track coverage trends</li> <li>Dependabot: Enable for automated dependency updates</li> </ul> <p>Last Updated: Phase 4 (Nov 25, 2025) Status: \u2705 All workflows configured and tested</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Complete guide to SpanForge configuration options.</p>"},{"location":"configuration/#configuration-file","title":"Configuration File","text":"<p>SpanForge uses Pydantic BaseSettings for configuration management with environment variable support.</p>"},{"location":"configuration/#appconfig-parameters","title":"AppConfig Parameters","text":"Parameter Type Default Description <code>model_name</code> str <code>\"dmis-lab/biobert-base-cased-v1.1\"</code> HuggingFace model identifier <code>max_seq_len</code> int <code>256</code> Maximum sequence length for tokenization <code>device</code> str auto-detect Computation device (<code>'cuda'</code> or <code>'cpu'</code>) <code>seed</code> int <code>42</code> Random seed for reproducibility <code>negation_window</code> int <code>5</code> Tokens after negation cue to mark as negated <code>fuzzy_scorer</code> str <code>\"wratio\"</code> Fuzzy matching algorithm (<code>'wratio'</code> or <code>'jaccard'</code>) <code>llm_enabled</code> bool <code>False</code> Enable experimental LLM refinement <code>llm_provider</code> str <code>\"stub\"</code> LLM provider (<code>'stub'</code>, <code>'openai'</code>, <code>'azure'</code>) <code>llm_model</code> str <code>\"gpt-4\"</code> LLM model identifier <code>llm_min_confidence</code> float <code>0.65</code> Minimum confidence for LLM suggestions <code>llm_cache_path</code> str <code>\"data/annotation/exports/llm_cache.jsonl\"</code> LLM response cache file <code>llm_prompt_version</code> str <code>\"v1\"</code> Prompt template version"},{"location":"configuration/#usage-examples","title":"Usage Examples","text":""},{"location":"configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from src.config import AppConfig\n\n# Use defaults\nconfig = AppConfig()\nprint(config.device)  # 'cuda' if available, else 'cpu'\nprint(config.negation_window)  # 5\n</code></pre>"},{"location":"configuration/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Override defaults\nconfig = AppConfig(\n    negation_window=7,\n    fuzzy_scorer=\"jaccard\",\n    max_seq_len=512,\n    device=\"cpu\"\n)\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Set via environment variables (prefixed with your app name if needed):</p> <pre><code>export MODEL_NAME=\"dmis-lab/biobert-v1.1\"\nexport NEGATION_WINDOW=10\nexport DEVICE=\"cuda\"\n</code></pre> <pre><code># Automatically reads from environment\nconfig = AppConfig()\n</code></pre>"},{"location":"configuration/#seed-management","title":"Seed Management","text":"<pre><code>from src.config import set_seed\n\n# Set for reproducibility\nset_seed(42)\n\n# All random operations now deterministic\nimport random\nimport numpy as np\nprint(random.random())  # Same value every run\nprint(np.random.rand())  # Same value every run\n</code></pre>"},{"location":"configuration/#parameter-details","title":"Parameter Details","text":""},{"location":"configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"configuration/#model_name","title":"model_name","text":"<p>HuggingFace model identifier. Default is BioBERT base cased v1.1.</p> <p>Options: - <code>\"dmis-lab/biobert-base-cased-v1.1\"</code> (default) - <code>\"dmis-lab/biobert-large-cased-v1.1\"</code> - <code>\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"</code> - Any HuggingFace transformer model</p> <pre><code>config = AppConfig(model_name=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n</code></pre>"},{"location":"configuration/#max_seq_len","title":"max_seq_len","text":"<p>Maximum sequence length for tokenization. Longer texts are truncated.</p> <p>Recommendations: - <code>256</code>: Default, good balance for complaints (1-3 sentences) - <code>512</code>: Longer documents, increased memory usage - <code>128</code>: Short texts, faster processing</p> <pre><code>config = AppConfig(max_seq_len=512)  # For longer documents\n</code></pre>"},{"location":"configuration/#device","title":"device","text":"<p>Computation device. Auto-detects CUDA availability.</p> <pre><code># Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Use specific GPU\nconfig = AppConfig(device=\"cuda:1\")\n</code></pre>"},{"location":"configuration/#weak-labeling-configuration","title":"Weak Labeling Configuration","text":""},{"location":"configuration/#negation_window","title":"negation_window","text":"<p>Number of tokens after negation cue to mark as negated.</p> <p>Tuning: - <code>3-5</code>: Short-range negation (default: 5) - <code>7-10</code>: Long-range negation (more false positives) - <code>1-2</code>: Very conservative</p> <pre><code># Example: \"Patient has no history of itching or redness\"\nconfig = AppConfig(negation_window=7)  # Catches \"redness\" too\n</code></pre>"},{"location":"configuration/#fuzzy_scorer","title":"fuzzy_scorer","text":"<p>Fuzzy matching algorithm selection.</p> <p>Options: - <code>\"wratio\"</code> (default): WRatio scoring, better for misspellings - <code>\"jaccard\"</code>: Token-set Jaccard, better for synonym matching</p> <pre><code># For exact synonym matching\nconfig = AppConfig(fuzzy_scorer=\"jaccard\")\n</code></pre>"},{"location":"configuration/#llm-configuration-experimental","title":"LLM Configuration (Experimental)","text":""},{"location":"configuration/#llm_enabled","title":"llm_enabled","text":"<p>Enable LLM-based span refinement pipeline.</p> <pre><code>config = AppConfig(\n    llm_enabled=True,\n    llm_provider=\"openai\",  # or \"azure\", \"anthropic\"\n    llm_model=\"gpt-4\",\n    llm_min_confidence=0.7\n)\n</code></pre>"},{"location":"configuration/#llm_min_confidence","title":"llm_min_confidence","text":"<p>Minimum confidence threshold for accepting LLM suggestions.</p> <p>Recommendations: - <code>0.5-0.6</code>: Exploratory, more suggestions - <code>0.65-0.75</code>: Balanced (default: 0.65) - <code>0.8-0.9</code>: Conservative, high precision</p>"},{"location":"configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"configuration/#cpu-optimization","title":"CPU Optimization","text":"<pre><code>config = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,  # Shorter sequences\n    # Use exact matching only when possible\n)\n</code></pre>"},{"location":"configuration/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>config = AppConfig(\n    device=\"cuda\",\n    max_seq_len=512,  # Longer sequences\n)\n\n# Enable GPU optimizations\nimport torch\ntorch.backends.cudnn.benchmark = True\n</code></pre>"},{"location":"configuration/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.pipeline import simple_inference\n\n# Process large batches\ntexts = load_texts(\"large_dataset.txt\")  # e.g., 10,000 texts\n\n# Batch processing\nbatch_size = 32\nresults = []\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i+batch_size]\n    results.extend(simple_inference(batch))\n</code></pre>"},{"location":"configuration/#configuration-profiles","title":"Configuration Profiles","text":""},{"location":"configuration/#development-profile","title":"Development Profile","text":"<pre><code>dev_config = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,\n    negation_window=5,\n    seed=42,\n    llm_enabled=False\n)\n</code></pre>"},{"location":"configuration/#production-profile","title":"Production Profile","text":"<pre><code>prod_config = AppConfig(\n    device=\"cuda\",\n    max_seq_len=256,\n    negation_window=7,\n    fuzzy_scorer=\"wratio\",\n    seed=42,\n    llm_enabled=True,\n    llm_provider=\"azure\",\n    llm_min_confidence=0.75\n)\n</code></pre>"},{"location":"configuration/#testing-profile","title":"Testing Profile","text":"<pre><code>test_config = AppConfig(\n    device=\"cpu\",\n    seed=42,  # Deterministic\n    llm_enabled=False,  # No external calls\n    negation_window=5\n)\n</code></pre>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Always set seed for reproducible experiments</li> <li>Profile before tuning - measure actual performance</li> <li>Start with defaults - they work well for most cases</li> <li>Use environment variables for deployment secrets</li> <li>Document custom configs in code comments</li> </ol>"},{"location":"configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/#issue-cuda-out-of-memory","title":"Issue: CUDA out of memory","text":"<p>Solutions: <pre><code># Reduce sequence length\nconfig = AppConfig(max_seq_len=128)\n\n# Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Clear cache between batches\nimport torch\ntorch.cuda.empty_cache()\n</code></pre></p>"},{"location":"configuration/#issue-poor-negation-detection","title":"Issue: Poor negation detection","text":"<p>Solutions: <pre><code># Extend window for long-range negation\nconfig = AppConfig(negation_window=10)\n\n# Check NEGATION_TOKENS in src/weak_label.py\n# Add custom negation cues if needed\n</code></pre></p>"},{"location":"configuration/#issue-low-fuzzy-match-recall","title":"Issue: Low fuzzy match recall","text":"<p>Solutions: <pre><code># Try Jaccard scorer\nconfig = AppConfig(fuzzy_scorer=\"jaccard\")\n\n# Lower fuzzy threshold in match_symptoms()\nspans = match_symptoms(text, lexicon, fuzzy_threshold=85.0)\n</code></pre></p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>User Guide: Weak Labeling - Advanced techniques</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"heuristic/","title":"Heuristic & Thresholds","text":"SpanForge Weak Labeling Heuristics <p>Specification of lexical + fuzzy gating rules that bootstrap adverse event span discovery prior to supervised BioBERT fine-tuning.</p>"},{"location":"heuristic/#weak-labeling-heuristics","title":"Weak Labeling Heuristics","text":"<p>Defines rule-based span proposal logic prior to supervised training.</p>"},{"location":"heuristic/#inputs","title":"Inputs","text":"<ul> <li>Complaint texts</li> <li>Symptom lexicon: <code>data/lexicon/symptoms.csv</code></li> <li>Product lexicon: <code>data/lexicon/products.csv</code></li> </ul>"},{"location":"heuristic/#processing-steps","title":"Processing Steps","text":"<ol> <li>Candidate Windows: Iterate token windows up to max phrase length of lexicon entries.</li> <li>Fuzzy Scoring: Compute WRatio (RapidFuzz) between window text and lexicon term.</li> <li>Token-Set Jaccard: Lowercased token set overlap percentage.</li> <li>Gates: Accept if <code>fuzzy \u2265 0.88</code> AND <code>jaccard \u2265 40</code>.</li> <li>Alignment: Multi-token candidate must align on last token boundary with lexicon term (mitigates mid-span inflation).</li> <li>Anatomy Filtering: Skip single generic anatomy tokens unless symptom keyword co-occurs.</li> <li>Negation Marking: Build window of 5 tokens around negation cues; \u226550% token overlap \u2192 mark <code>negated=True</code>.</li> <li>Confidence: Weighted combination (see below); duplicates resolved by highest confidence.</li> </ol>"},{"location":"heuristic/#thresholds","title":"Thresholds","text":"Parameter Default Purpose Fuzzy WRatio 0.88 Balances lexical variant recall vs noise Jaccard % 40 Ensures partial but meaningful token overlap Negation window 5 tokens Captures local negation context Overlap for negation \u226550% Avoids spurious negation marking Multi-token alignment Enforced Reduces partial window drift"},{"location":"heuristic/#confidence-formula","title":"Confidence Formula","text":"<p><pre><code>confidence = clamp(0.8 * (fuzzy/100) + 0.2 * (jaccard/100), 0.0, 1.0)\n</code></pre> Fuzzy &amp; Jaccard are raw percentages (0\u2013100) before weighting.</p>"},{"location":"heuristic/#negation-cues-examples","title":"Negation Cues (Examples)","text":"<p><code>no</code>, <code>not</code>, <code>without</code>, <code>never</code>, <code>none</code>, <code>free of</code>, <code>lack of</code>. Token normalization handles casing; multi-word cues expanded via phrase tokenization.</p>"},{"location":"heuristic/#canonical-mapping","title":"Canonical Mapping","text":"<p>If lexicon entry matched, <code>canonical</code> set to curated term; otherwise fallback canonical = surface span (enables later normalization decisions &amp; lexicon expansion).</p>"},{"location":"heuristic/#duplicate-overlap-policy","title":"Duplicate / Overlap Policy","text":"<ul> <li>Exact duplicates (same start,end,label) \u2192 keep highest confidence only.</li> <li>Overlapping distinct spans retained; conflicts surfaced later for human review.</li> </ul>"},{"location":"heuristic/#exclusions","title":"Exclusions","text":"<ul> <li>Pure stopword spans.</li> <li>Isolated anatomy token without symptom context.</li> <li>Zero-length or punctuation-only windows.</li> </ul>"},{"location":"heuristic/#tuning-guidance","title":"Tuning Guidance","text":"Symptom Adjust Effect High false positives Increase fuzzy (0.90) or Jaccard (50) Precision \u2191, Recall \u2193 Low recall variants Lower Jaccard (35) first Recall \u2191 moderate Many partial matches Enforce stricter alignment Noise \u2193 <p>Tune one parameter at a time; evaluate on gold comparison script.</p>"},{"location":"heuristic/#planned-enhancements","title":"Planned Enhancements","text":"<ul> <li>Embedding similarity (BioBERT cosine) secondary gate.</li> <li>Contextual disambiguation around product proximity.</li> <li>Label-specific thresholds (PRODUCT vs SYMPTOM).</li> <li>Active learning: mine high-uncertainty spans for prioritization.</li> </ul>"},{"location":"heuristic/#drift-monitoring","title":"Drift Monitoring","text":"<p>Track PRODUCT:SYMPTOM ratio per batch; sudden deviation triggers threshold audit.</p>"},{"location":"heuristic/#safety-considerations","title":"Safety Considerations","text":"<p>Avoid aggressive threshold lowering early\u2014annotation burden &amp; noise escalate quickly.</p>"},{"location":"heuristic/#reference-implementation","title":"Reference Implementation","text":"<p>See <code>src/weak_label.py</code> for authoritative logic and configuration hooks.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9, 3.10, or 3.11</li> <li>Git</li> <li>4GB RAM minimum</li> <li>Optional: CUDA-capable GPU for faster inference</li> </ul>"},{"location":"installation/#standard-installation","title":"Standard Installation","text":""},{"location":"installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/paulboys/SpanForge.git\ncd SpanForge\n</code></pre>"},{"location":"installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"Condavenv <pre><code>conda create -n spanforge python=3.10\nconda activate spanforge\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\n# or\nvenv\\Scripts\\activate  # Windows\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python scripts/verify_env.py\n</code></pre> <p>Expected output: <pre><code>\u2713 PyTorch installed\n\u2713 Transformers installed\n\u2713 Device: cuda (or cpu)\n\u2713 BioBERT model downloadable\n\u2713 Environment ready\n</code></pre></p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For contributors who want to run tests and linting:</p> <pre><code># Install dev dependencies\npip install -r requirements.txt\npip install pytest pytest-cov pre-commit ruff black isort mypy\n\n# Setup pre-commit hooks\npre-commit install\n\n# Verify tests pass\npytest tests/ -v\n</code></pre>"},{"location":"installation/#documentation-build","title":"Documentation Build","text":"<p>To build documentation locally:</p> <pre><code>pip install -r docs-requirements.txt\nmkdocs serve\n</code></pre> <p>Visit http://127.0.0.1:8000 to view docs.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#issue-pytorch-cuda-not-detected","title":"Issue: PyTorch CUDA not detected","text":"<p>Solution: Install PyTorch with CUDA support: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"installation/#issue-transformers-model-download-fails","title":"Issue: Transformers model download fails","text":"<p>Solution: Check internet connection or use offline model: <pre><code># Download model manually\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\nmodel.save_pretrained(\"./models/biobert\")\n</code></pre></p>"},{"location":"installation/#issue-import-errors","title":"Issue: Import errors","text":"<p>Solution: Ensure you're in the project root and virtual environment is activated: <pre><code>pwd  # Should show SpanForge directory\nwhich python  # Should show venv python\n</code></pre></p>"},{"location":"installation/#optional-components","title":"Optional Components","text":""},{"location":"installation/#gpu-acceleration","title":"GPU Acceleration","text":"<p>For CUDA support: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"installation/#label-studio-for-annotation","title":"Label Studio (for annotation)","text":"<pre><code>pip install label-studio\nsetx LABEL_STUDIO_DISABLE_TELEMETRY 1  # Windows\n# or\nexport LABEL_STUDIO_DISABLE_TELEMETRY=1  # Linux/Mac\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration Options</li> <li>API Reference</li> </ul>"},{"location":"llm_integration/","title":"LLM-Based Span Refinement","text":"<p>Version: 1.1 Last Updated: November 27, 2025 Phase: 4.5 - LLM Integration Complete</p>"},{"location":"llm_integration/#overview","title":"Overview","text":"<p>SpanForge integrates Large Language Models (LLMs) to refine weak label span boundaries, validate negation, and normalize entities before human annotation. This optional step reduces annotator workload by:</p> <ul> <li>Correcting boundaries: Removes superfluous adjectives (\"severe burning\" \u2192 \"burning\")</li> <li>Validating negation: Confirms negated spans maintain semantic accuracy</li> <li>Normalizing terms: Maps colloquial language to canonical lexicon entries</li> <li>Quantifying improvement: Tracks IOU uplift and correction rates via evaluation harness</li> </ul>"},{"location":"llm_integration/#quick-start","title":"Quick Start","text":""},{"location":"llm_integration/#1-installation","title":"1. Installation","text":"<pre><code># Install LLM dependencies (optional)\npip install -r requirements-llm.txt\n</code></pre> <p>Includes: - <code>openai&gt;=1.0.0</code> (OpenAI + Azure OpenAI) - <code>anthropic&gt;=0.18.0</code> (Anthropic Claude) - <code>tenacity&gt;=8.0.0</code> (retry logic with exponential backoff)</p>"},{"location":"llm_integration/#2-configure-provider","title":"2. Configure Provider","text":"<p>Set Environment Variables:</p> <pre><code># OpenAI\n$env:OPENAI_API_KEY = \"sk-...\"\n\n# Azure OpenAI\n$env:AZURE_OPENAI_API_KEY = \"your-key\"\n$env:AZURE_OPENAI_ENDPOINT = \"https://your-resource.openai.azure.com/\"\n\n# Anthropic\n$env:ANTHROPIC_API_KEY = \"sk-ant-...\"\n</code></pre>"},{"location":"llm_integration/#3-run-refinement","title":"3. Run Refinement","text":"<pre><code>python scripts/annotation/cli.py refine-llm \\\n  --weak data/weak_labels.jsonl \\\n  --output data/llm_refined.jsonl \\\n  --provider openai \\\n  --model gpt-4-turbo\n</code></pre>"},{"location":"llm_integration/#4-evaluate-results","title":"4. Evaluate Results","text":"<pre><code># Generate metrics report\npython scripts/annotation/evaluate_llm_refinement.py \\\n  --weak data/weak_labels.jsonl \\\n  --refined data/llm_refined.jsonl \\\n  --gold data/gold_standard.jsonl \\\n  --output data/annotation/reports/evaluation.json \\\n  --markdown \\\n  --stratify label confidence span_length\n\n# Visualize improvements (optional)\npip install -r requirements-viz.txt\npython scripts/annotation/plot_llm_metrics.py \\\n  --report data/annotation/reports/evaluation.json \\\n  --output-dir data/annotation/plots/\n</code></pre>"},{"location":"llm_integration/#supported-providers","title":"Supported Providers","text":""},{"location":"llm_integration/#1-stub-mode-default","title":"1. Stub Mode (Default)","text":"<ul> <li>Use Case: Testing without API costs</li> <li>Behavior: Returns empty suggestions for dry-run testing</li> <li>Configuration: <code>llm_provider=\"stub\"</code> (no API key needed)</li> </ul>"},{"location":"llm_integration/#2-openai","title":"2. OpenAI","text":"<ul> <li>Models: <code>gpt-4</code>, <code>gpt-4-turbo</code>, <code>gpt-3.5-turbo</code></li> <li>Cost: ~$0.03/1K input tokens (gpt-4), ~$0.0005/1K (gpt-3.5-turbo)</li> <li>Environment: <code>OPENAI_API_KEY</code></li> </ul>"},{"location":"llm_integration/#3-azure-openai","title":"3. Azure OpenAI","text":"<ul> <li>Models: Your deployed models (e.g., custom gpt-4 deployment)</li> <li>Cost: Same as OpenAI pricing</li> <li>Environment: <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code></li> </ul>"},{"location":"llm_integration/#4-anthropic-claude","title":"4. Anthropic Claude","text":"<ul> <li>Models: <code>claude-3-5-sonnet-20241022</code>, <code>claude-3-haiku</code>, <code>claude-3-opus</code></li> <li>Cost: ~$0.003/1K input tokens (sonnet), ~$0.00025/1K (haiku)</li> <li>Environment: <code>ANTHROPIC_API_KEY</code></li> </ul>"},{"location":"llm_integration/#configuration","title":"Configuration","text":""},{"location":"llm_integration/#code-configuration","title":"Code Configuration","text":"<pre><code>from src.config import AppConfig\nfrom src.llm_agent import LLMAgent\n\nconfig = AppConfig(\n    llm_enabled=True,\n    llm_provider=\"openai\",  # \"stub\", \"openai\", \"azure\", \"anthropic\"\n    llm_model=\"gpt-4-turbo\",\n    llm_temperature=0.1,  # Low temperature for consistency\n    llm_max_retries=3,  # Exponential backoff retry\n    llm_min_confidence=0.65,  # Confidence threshold for keeping suggestions\n    llm_cache_path=\"data/annotation/exports/llm_cache.jsonl\",\n    llm_prompt_version=\"v1\",\n)\n\nagent = LLMAgent(config)\n</code></pre>"},{"location":"llm_integration/#cli-configuration","title":"CLI Configuration","text":"<pre><code>python scripts/annotation/cli.py refine-llm \\\n  --weak weak.jsonl \\\n  --output refined.jsonl \\\n  --provider openai \\\n  --model gpt-4-turbo \\\n  --temperature 0.1 \\\n  --min-confidence 0.65 \\\n  --dry-run  # Preview without API calls\n</code></pre>"},{"location":"llm_integration/#workflow-integration","title":"Workflow Integration","text":"<pre><code>graph LR\n    A[Raw Text] --&gt; B[Weak Labeler]\n    B --&gt; C{LLM Enabled?}\n    C --&gt;|Yes| D[LLM Refinement]\n    C --&gt;|No| E[Label Studio]\n    D --&gt; E\n    E --&gt; F[Human Annotation]\n    F --&gt; G[Gold JSONL]\n    G --&gt; H[Evaluation]</code></pre> <p>Recommended Path: 1. Generate weak labels with high recall (accept some noise) 2. Apply LLM refinement to mid-confidence spans (0.55\u20130.75) 3. Import to Label Studio for human review 4. Evaluate: weak \u2192 LLM \u2192 gold to measure IOU uplift</p>"},{"location":"llm_integration/#features","title":"Features","text":""},{"location":"llm_integration/#response-caching","title":"Response Caching","text":"<ul> <li>All API responses cached to <code>data/annotation/exports/llm_cache.jsonl</code></li> <li>Keyed by prompt hash (SHA256) for deterministic lookup</li> <li>Prevents duplicate API calls and reduces costs</li> </ul>"},{"location":"llm_integration/#retry-logic-via-tenacity","title":"Retry Logic (via <code>tenacity</code>)","text":"<ul> <li>Exponential backoff: 2s, 4s, 8s delays</li> <li>Up to 3 attempts for transient failures (rate limits, timeouts)</li> <li>Graceful degradation if retry package not installed</li> </ul>"},{"location":"llm_integration/#error-handling","title":"Error Handling","text":"<ul> <li>Missing API key: Clear error with setup instructions</li> <li>SDK not installed: Prompts to run <code>pip install -r requirements-llm.txt</code></li> <li>API errors: Returns empty spans with error note (no crash)</li> <li>Unsupported provider: Validation error with supported provider list</li> </ul>"},{"location":"llm_integration/#provenance-tracking","title":"Provenance Tracking","text":"<p>Each LLM-refined span includes: - <code>source=\"llm_refine\"</code> (vs <code>\"weak\"</code>) - <code>llm_confidence</code> (0-1 score) - <code>canonical</code> (normalized entity form) - <code>llm_meta</code>: <code>{provider, model, prompt_version, timestamp}</code></p>"},{"location":"llm_integration/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"llm_integration/#core-metrics-from-srcevaluationmetricspy","title":"Core Metrics (from <code>src/evaluation/metrics.py</code>)","text":"<ol> <li>IOU (Intersection over Union): Span boundary overlap (0-1)</li> <li>Boundary Precision: Exact match rate + mean/median IOU</li> <li>IOU Delta: Weak \u2192 LLM improvement per span</li> <li>Correction Rate: Categorizes spans as improved/worsened/unchanged</li> <li>Calibration Curve: Confidence reliability (binned IOU vs confidence)</li> <li>Precision/Recall/F1: Standard NER metrics vs gold standard</li> </ol>"},{"location":"llm_integration/#stratification","title":"Stratification","text":"<p>Analyze performance across: - Label: SYMPTOM vs PRODUCT - Confidence: Bucketed by score (e.g., 0.6-0.7, 0.7-0.8) - Span Length: Short (&lt;10 chars) vs medium vs long (&gt;30 chars)</p>"},{"location":"llm_integration/#example-results-test-fixtures","title":"Example Results (Test Fixtures)","text":"<pre><code>Overall Metrics:\n- IOU Improvement: +13.4% (0.882 \u2192 1.000)\n- Exact Match Rate: 66.7% \u2192 100.0%\n- Correction Rate: 100% improved (2/2 modified spans)\n- F1 Score: 1.000 (perfect precision/recall)\n\nCorrections:\n- \"severe burning sensation\" \u2192 \"burning sensation\" (+18.2% IOU)\n- \"mild redness\" \u2192 \"redness\" (+8.3% IOU)\n</code></pre>"},{"location":"llm_integration/#visualization-outputs","title":"Visualization Outputs","text":"<p>Run <code>plot_llm_metrics.py</code> to generate: 1. IOU Uplift (<code>iou_uplift.png</code>): Weak vs LLM distributions 2. Calibration Curve (<code>calibration_curve.png</code>): Confidence reliability 3. Correction Rate (<code>correction_rate.png</code>): Pie + bar chart breakdown 4. P/R/F1 Comparison (<code>prf_comparison.png</code>): Side-by-side weak vs LLM 5. Stratified Label (<code>stratified_label.png</code>): F1 by entity type 6. Stratified Confidence (<code>stratified_confidence.png</code>): IOU delta across buckets</p> <p>All plots are publication-quality (300 DPI default), colorblind-safe palette.</p>"},{"location":"llm_integration/#cost-management","title":"Cost Management","text":""},{"location":"llm_integration/#tips-for-reducing-costs","title":"Tips for Reducing Costs","text":"<ol> <li>Use Caching: Automatic caching prevents duplicate calls</li> <li>Start with Stub Mode: Develop/test without API costs</li> <li>Smaller Models: Use <code>gpt-3.5-turbo</code> or <code>claude-haiku</code> for initial runs</li> <li>Batch Processing: Amortize setup overhead across multiple texts</li> <li>Confidence Filtering: Only refine mid-confidence spans (0.55\u20130.75)</li> <li>Sample First: Evaluate on 100 examples before full dataset</li> </ol>"},{"location":"llm_integration/#approximate-costs-as-of-november-2025","title":"Approximate Costs (as of November 2025)","text":"Provider Model Input Cost Output Cost Est. Cost/1K Complaints OpenAI gpt-4-turbo $10 / 1M $30 / 1M ~$5-10 OpenAI gpt-3.5-turbo $0.50 / 1M $1.50 / 1M ~$0.25-0.50 Anthropic claude-3-5-sonnet $3 / 1M $15 / 1M ~$2-5 Anthropic claude-3-haiku $0.25 / 1M $1.25 / 1M ~$0.20-0.40 <p>Assumes ~200 tokens per complaint (100 input, 100 output). Check provider websites for current pricing.</p>"},{"location":"llm_integration/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"llm_integration/#current-prompts-prompts","title":"Current Prompts (<code>prompts/</code>)","text":"<ol> <li>boundary_refine.txt: Tightens span boundaries</li> <li>Removes adjectives (severe, mild, slight)</li> <li>Removes determiners (the, a, an)</li> <li> <p>Preserves core symptom/product text</p> </li> <li> <p>negation_check.txt: Validates negation flags</p> </li> <li>Confirms \"no burning\" is correctly negated</li> <li> <p>Detects false positives (\"burn-free\" vs \"burning\")</p> </li> <li> <p>synonym_expand.txt: Normalizes to canonical forms</p> </li> <li>Maps \"itching\" \u2192 \"Pruritus\"</li> <li>Maps \"redness\" \u2192 \"Erythema\"</li> <li>Provides synonym lists for fuzzy matching</li> </ol>"},{"location":"llm_integration/#template-placeholders","title":"Template Placeholders","text":"<ul> <li><code>{{text}}</code>: Full input text</li> <li><code>{{candidates}}</code>: JSON list of weak spans</li> <li><code>{{knowledge}}</code>: Optional lexicon context</li> <li><code>{{confidence_threshold}}</code>: Minimum score to keep suggestions</li> </ul>"},{"location":"llm_integration/#versioning","title":"Versioning","text":"<p>Prompts are versioned (<code>v1</code>, <code>v2</code>) and stored in <code>llm_meta.prompt_version</code> for reproducibility. When iterating, create new versions and compare via evaluation harness.</p>"},{"location":"llm_integration/#data-formats","title":"Data Formats","text":""},{"location":"llm_integration/#input-weak-labels","title":"Input (Weak Labels)","text":"<pre><code>{\n  \"id\": \"complaint_001\",\n  \"text\": \"Patient reports severe burning sensation.\",\n  \"spans\": [\n    {\n      \"start\": 15,\n      \"end\": 43,\n      \"text\": \"severe burning sensation\",\n      \"label\": \"SYMPTOM\",\n      \"confidence\": 0.85,\n      \"source\": \"weak\"\n    }\n  ]\n}\n</code></pre>"},{"location":"llm_integration/#output-llm-refined","title":"Output (LLM Refined)","text":"<pre><code>{\n  \"id\": \"complaint_001\",\n  \"text\": \"Patient reports severe burning sensation.\",\n  \"spans\": [\n    {\n      \"start\": 22,\n      \"end\": 40,\n      \"text\": \"burning sensation\",\n      \"label\": \"SYMPTOM\",\n      \"confidence\": 0.92,\n      \"source\": \"llm_refine\",\n      \"canonical\": \"Burning Sensation\",\n      \"llm_meta\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4-turbo\",\n        \"prompt_version\": \"v1\",\n        \"timestamp\": \"2025-11-27T10:15:30Z\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"llm_integration/#testing","title":"Testing","text":""},{"location":"llm_integration/#run-test-suite","title":"Run Test Suite","text":"<pre><code># All LLM tests\npytest tests/test_llm_agent.py -v\n\n# With coverage\npytest tests/test_llm_agent.py --cov=src.llm_agent --cov-report=term-missing\n\n# Evaluation harness tests\npytest tests/test_evaluate_llm.py -v\n</code></pre> <p>Coverage: 15/15 LLM agent tests passing, 27/27 evaluation tests passing (100%).</p>"},{"location":"llm_integration/#integration-tests","title":"Integration Tests","text":"<pre><code># End-to-end workflow\npytest tests/integration/test_llm_workflow.py -v\n</code></pre>"},{"location":"llm_integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llm_integration/#openai-package-not-installed","title":"\"openai package not installed\"","text":"<pre><code>pip install -r requirements-llm.txt\n</code></pre>"},{"location":"llm_integration/#openai_api_key-environment-variable-not-set","title":"\"OPENAI_API_KEY environment variable not set\"","text":"<pre><code>$env:OPENAI_API_KEY = \"sk-your-key-here\"\n</code></pre>"},{"location":"llm_integration/#api-error-429-too-many-requests","title":"\"API error: 429 Too Many Requests\"","text":"<ul> <li>Rate limit exceeded</li> <li>Retry logic handles automatically (exponential backoff)</li> <li>Reduce request frequency or upgrade API tier</li> </ul>"},{"location":"llm_integration/#cache-file-corruption","title":"Cache file corruption","text":"<pre><code># Clear cache and regenerate\nRemove-Item data\\annotation\\exports\\llm_cache.jsonl\n</code></pre>"},{"location":"llm_integration/#import-errors-with-tenacity","title":"Import errors with tenacity","text":"<pre><code>pip install tenacity&gt;=8.0.0\n</code></pre>"},{"location":"llm_integration/#limitations-future-work","title":"Limitations &amp; Future Work","text":""},{"location":"llm_integration/#current-limitations","title":"Current Limitations","text":"<ul> <li>LLM may over-correct colloquial phrasing (monitor via <code>correction_rate</code> worsened %)</li> <li>Calibration curve requires \u226550 spans per bucket for statistical reliability</li> <li>Visualization requires optional dependencies (<code>matplotlib</code>, <code>seaborn</code>)</li> <li>No batch API support yet (processes sequentially)</li> </ul>"},{"location":"llm_integration/#roadmap","title":"Roadmap","text":"<ul> <li> Batch API integration for cost/speed optimization</li> <li> Active learning: prioritize high-disagreement spans for annotation</li> <li> Multi-turn refinement: iterative boundary correction</li> <li> Ensemble approaches: combine multiple model outputs</li> <li> Custom fine-tuned models for domain-specific terminology</li> </ul>"},{"location":"llm_integration/#references","title":"References","text":""},{"location":"llm_integration/#provider-documentation","title":"Provider Documentation","text":"<ul> <li>OpenAI API</li> <li>Azure OpenAI Service</li> <li>Anthropic Claude API</li> </ul>"},{"location":"llm_integration/#related-guides","title":"Related Guides","text":"<ul> <li>Production Workflow - End-to-end annotation pipeline</li> <li>Annotation Guide - Label Studio setup and best practices</li> <li>Evaluation Workflow - Metrics and reporting</li> </ul>"},{"location":"llm_integration/#research-papers","title":"Research Papers","text":"<ul> <li>Ratner et al. (2017): \"Snorkel: Rapid Training Data Creation with Weak Supervision\"</li> <li>Zhou &amp; Li (2021): \"A Survey on Neural Weak Supervision\"</li> <li>Wei et al. (2022): \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"</li> </ul> <p>Next Steps: 1. Evaluate LLM refinement on sample batch (100 complaints) 2. Measure IOU uplift and correction rate 3. Tune confidence threshold based on precision/recall 4. Scale to full dataset once satisfied with metrics 5. Use LLM suggestions to guide annotation priorities</p>"},{"location":"overview/","title":"Overview","text":"SpanForge Documentation <p>Core architectural overview for the SpanForge adverse event annotation and NER pipeline.</p>"},{"location":"overview/#overview","title":"Overview","text":"<p>End-to-end Adverse Event NER pipeline integrating weak labeling, BioBERT embeddings, human annotation, and quality/provenance tracking. Designed to iteratively refine span quality before supervised fine-tuning.</p>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Text \u2192 Weak Labeler \u2192 Weak JSONL \u2192 Label Studio Tasks \u2192 Human Annotation \u2192 Export\n    \u2192 Gold Converter (+canonical +provenance) \u2192 Gold JSONL \u2192 Quality Metrics / Registry\n                                 \u2192 (Planned) BIO Tagging \u2192 Token Classifier \u2192 Evaluation\n</code></pre>"},{"location":"overview/#core-modules","title":"Core Modules","text":"<ul> <li><code>src/config.py</code>: Centralized configuration (model, device, heuristic thresholds, negation window, scorer).</li> <li><code>src/model.py</code>: BioBERT base encoder loader (future token classification head attachment point).</li> <li><code>src/weak_label.py</code>: Lexicon + fuzzy + Jaccard gating, negation detection, confidence scoring.</li> <li><code>src/pipeline.py</code>: Light inference utilities; will wrap supervised prediction later.</li> <li><code>scripts/annotation/*.py</code>: Operational scripts for project init, import, convert, quality, adjudication, registry, CLI.</li> <li><code>scripts/Workbook.ipynb</code>: Educational notebook walking through ingestion \u2192 gold.</li> </ul>"},{"location":"overview/#entity-types","title":"Entity Types","text":"<ul> <li><code>SYMPTOM</code>: Physiological or subjective adverse effects reported by consumer.</li> <li><code>PRODUCT</code>: Product names, formulations, or product category terms referenced.</li> </ul>"},{"location":"overview/#canonical-mapping","title":"Canonical Mapping","text":"<p>Surface forms mapped to canonical normalized terms (symptoms/products) via curated lexicons (<code>data/lexicon/</code>). This stabilizes vocabulary for aggregation and downstream modeling.</p>"},{"location":"overview/#provenance-fields","title":"Provenance Fields","text":"<p>Gold output includes: <code>source</code>, <code>annotator</code>, <code>revision</code>, <code>canonical</code>, optional <code>concept_id</code> for traceability and reproducibility.</p>"},{"location":"overview/#heuristic-summary","title":"Heuristic Summary","text":"<ul> <li>Fuzzy WRatio \u2265 0.88</li> <li>Jaccard token-set \u2265 40</li> <li>Confidence = <code>0.8*fuzzy + 0.2*jaccard</code> (\u22641.0)</li> <li>Negation window: 5 tokens (\u226550% overlap \u2192 negated)</li> <li>Single generic anatomy token skip unless symptom co-occurs</li> <li>Last-token alignment for multi-token fuzzy spans</li> </ul> <p>Details: <code>heuristic.md</code>.</p>"},{"location":"overview/#quality-metrics","title":"Quality Metrics","text":"<p><code>quality_report.py</code> computes span density, label distribution, conflicts, annotator counts; planned kappa &amp; drift metrics.</p>"},{"location":"overview/#design-principles","title":"Design Principles","text":"<ul> <li>Pure functions; explicit dependency passing</li> <li>Reproducible thresholds in config (tunable)</li> <li>Incremental, test-supported evolution (weak \u2192 gold \u2192 supervised)</li> <li>Clear audit trail via registry and provenance fields</li> </ul>"},{"location":"overview/#privacy-compliance","title":"Privacy &amp; Compliance","text":"<p>No raw proprietary complaints committed. Annotation performed locally with telemetry disabled. Only de-identified / approved text permitted in repository.</p>"},{"location":"overview/#roadmap-condensed","title":"Roadmap (Condensed)","text":"<ol> <li>Lexicon &amp; Weak Labeling (complete / iterative)</li> <li>Annotation &amp; Curation (in progress)</li> <li>Gold Assembly &amp; Expansion</li> <li>Token Classification Fine-Tune</li> <li>Domain Adaptation (MLM)</li> <li>Baseline &amp; Evaluation (RoBERTa)</li> <li>Active Learning Loop</li> </ol>"},{"location":"overview/#references","title":"References","text":"<p>See <code>annotation_guide.md</code>, <code>tutorial_labeling.md</code>, and <code>heuristic.md</code> for deeper detail.</p>"},{"location":"phase_6_checklist/","title":"Phase 6 Quick Start Checklist","text":"<p>Goal: Assemble 500-1000 gold standard annotations Status: \u23f8\ufe0f READY TO START Estimated Time: 20-30 hours annotation + 5-10 hours QA</p>"},{"location":"phase_6_checklist/#phase-6-objectives","title":"\ud83c\udfaf Phase 6 Objectives","text":"<ul> <li> Batch 001: 100 cosmetics complaints (calibration batch)</li> <li> Batch 002-005: 400 additional complaints (scale to 500 total)</li> <li> Optional Batches 006-010: Scale to 1000 total</li> <li> Inter-annotator agreement &gt;0.75 (IOU \u2265 0.5)</li> <li> Canonical coverage &gt;90%</li> <li> All batches registered with provenance</li> </ul>"},{"location":"phase_6_checklist/#prerequisites-complete","title":"\u2705 Prerequisites (Complete)","text":"<p>\u2705 Weak labeling infrastructure (531/531 tests passing) \u2705 LLM refinement agent (OpenAI/Anthropic/stub support) \u2705 Evaluation metrics (10 functions implemented) \u2705 Annotation scripts (12 scripts in <code>scripts/annotation/</code>) \u2705 Documentation (<code>docs/annotation_guide.md</code>, <code>docs/production_workflow.md</code>) \u2705 CAERS data pipeline (666K+ complaints available) \u2705 Batch preparation script (NEW: <code>prepare_production_batch.py</code>) \u2705 Phase 6 implementation guide (NEW: <code>docs/phase_6_gold_standard.md</code>)</p>"},{"location":"phase_6_checklist/#implementation-checklist","title":"\ud83d\udccb Implementation Checklist","text":""},{"location":"phase_6_checklist/#week-1-batch-001-calibration","title":"Week 1: Batch 001 (Calibration)","text":""},{"location":"phase_6_checklist/#day-1-preparation","title":"Day 1: Preparation","text":"<ul> <li> Download CAERS cosmetics data (1000 complaints)   <pre><code>python scripts/caers/download_caers.py --output data/caers/cosmetics_1000.jsonl --categories cosmetics --limit 1000 --min-spans 1\n</code></pre></li> <li> Prepare batch 001 (100 tasks, stratified sampling)   <pre><code>python scripts/annotation/prepare_production_batch.py --input data/caers/cosmetics_1000.jsonl --output data/annotation/exports/batch_001.jsonl --n-tasks 100 --strategy stratified --min-spans 1 --max-text-len 500 --stats-output data/annotation/reports/batch_001_stats.json\n</code></pre></li> <li> Review batch statistics   <pre><code>Get-Content data/annotation/reports/batch_001_stats.json | ConvertFrom-Json | ConvertTo-Json -Depth 10\n</code></pre></li> </ul>"},{"location":"phase_6_checklist/#day-2-optional-llm-refinement","title":"Day 2: Optional LLM Refinement","text":"<ul> <li> Option A: Use OpenAI GPT-4 (requires API key + budget)   <pre><code>$env:OPENAI_API_KEY = \"sk-...\"\npython scripts/annotation/refine_llm.py --input data/annotation/exports/batch_001.jsonl --output data/annotation/exports/batch_001_refined.jsonl --provider openai --model gpt-4o-mini --temperature 0.1\n</code></pre></li> <li> Option B: Use Anthropic Claude (requires API key + budget)   <pre><code>$env:ANTHROPIC_API_KEY = \"sk-ant-...\"\npython scripts/annotation/refine_llm.py --input data/annotation/exports/batch_001.jsonl --output data/annotation/exports/batch_001_refined.jsonl --provider anthropic --model claude-3-5-sonnet-20241022 --temperature 0.1\n</code></pre></li> <li> Option C: Skip LLM (use weak labels directly)   <pre><code>Copy-Item data/annotation/exports/batch_001.jsonl data/annotation/exports/batch_001_refined.jsonl\n</code></pre></li> </ul>"},{"location":"phase_6_checklist/#day-3-4-human-annotation-3-5-hours","title":"Day 3-4: Human Annotation (3-5 hours)","text":"<ul> <li> Initialize Label Studio project (first time only)   <pre><code>python scripts/annotation/init_label_studio_project.py --project-name \"SpanForge Gold Standard\" --config data/annotation/config/label_config.xml\n</code></pre></li> <li> Import batch to Label Studio   <pre><code>python scripts/annotation/import_weak_to_labelstudio.py --input data/annotation/exports/batch_001_refined.jsonl --project-id 1 --confidence-filter 0.6\n</code></pre></li> <li> Launch Label Studio: <code>label-studio start</code></li> <li> Navigate to http://localhost:8080</li> <li> Annotate 100 tasks (~2-3 min/task = 3-5 hours total)</li> <li>\u2705 Correct span boundaries</li> <li>\u2705 Add missing spans</li> <li>\u2705 Remove false positives</li> <li>\u2705 Verify negation flags</li> <li>\u2705 Confirm canonical mappings</li> </ul>"},{"location":"phase_6_checklist/#day-5-export-validation-1-2-hours","title":"Day 5: Export &amp; Validation (1-2 hours)","text":"<ul> <li> Export from Label Studio UI: Settings \u2192 Export \u2192 JSON</li> <li> Convert to gold format   <pre><code>python scripts/annotation/convert_labelstudio.py --input data/annotation/exports/label_studio_export.json --output data/annotation/exports/batch_001_gold.jsonl --consensus majority --min-agree 1\n</code></pre></li> <li> Generate quality report   <pre><code>python scripts/annotation/quality_report.py --gold data/annotation/exports/batch_001_gold.jsonl --output data/annotation/reports/batch_001_quality.md\n</code></pre></li> <li> Evaluate weak \u2192 LLM \u2192 gold   <pre><code>python scripts/annotation/evaluate_llm_refinement.py --weak data/annotation/exports/batch_001.jsonl --refined data/annotation/exports/batch_001_refined.jsonl --gold data/annotation/exports/batch_001_gold.jsonl --output data/annotation/reports/batch_001_evaluation.json --markdown --stratify label confidence\n</code></pre></li> <li> Generate visualizations   <pre><code>python scripts/annotation/plot_llm_metrics.py --report data/annotation/reports/batch_001_evaluation.json --output-dir data/annotation/plots/batch_001/ --formats png pdf --plots all\n</code></pre></li> <li> Review metrics (target: IOU +8-15%, F1 &gt;0.85, exact match 70-85%)</li> </ul>"},{"location":"phase_6_checklist/#day-6-register-reflect-30-min","title":"Day 6: Register &amp; Reflect (30 min)","text":"<ul> <li> Register batch in provenance log   <pre><code>python scripts/annotation/register_batch.py --gold-file data/annotation/exports/batch_001_gold.jsonl --batch-id batch_001 --n-tasks 100 --annotators \"your_name\" --revision 1 --notes \"Initial calibration batch - cosmetics complaints\"\n</code></pre></li> <li> Review quality report for edge cases</li> <li> Update <code>docs/annotation_guide.md</code> with lessons learned</li> <li> Commit to git   <pre><code>git add data/annotation/exports/batch_001_gold.jsonl data/annotation/reports/ data/annotation/plots/\ngit commit -m \"feat(annotation): complete batch 001 gold standard (100 tasks)\"\ngit push origin main\n</code></pre></li> </ul>"},{"location":"phase_6_checklist/#week-2-3-batch-002-005-scale-to-500","title":"Week 2-3: Batch 002-005 (Scale to 500)","text":""},{"location":"phase_6_checklist/#batch-002-cosmetics-refined-guidelines","title":"Batch 002 (Cosmetics - Refined Guidelines)","text":"<ul> <li> Prepare batch 002 (100 tasks, exclude batch 001 IDs)</li> <li> Optional LLM refinement</li> <li> Annotate (3-5 hours)</li> <li> Export, validate, evaluate</li> <li> Register batch</li> </ul>"},{"location":"phase_6_checklist/#batch-003-supplements-category-expansion","title":"Batch 003 (Supplements - Category Expansion)","text":"<ul> <li> Download supplements data   <pre><code>python scripts/caers/download_caers.py --output data/caers/supplements_1000.jsonl --categories supplements --limit 1000 --min-spans 1\n</code></pre></li> <li> Prepare batch 003 (100 tasks)</li> <li> Optional LLM refinement</li> <li> Annotate (3-5 hours)</li> <li> Export, validate, evaluate</li> <li> Register batch</li> </ul>"},{"location":"phase_6_checklist/#batch-004-personal-care-diverse-language","title":"Batch 004 (Personal Care - Diverse Language)","text":"<ul> <li> Download personal care data</li> <li> Prepare batch 004 (100 tasks)</li> <li> Optional LLM refinement</li> <li> Annotate (3-5 hours)</li> <li> Export, validate, evaluate</li> <li> Register batch</li> </ul>"},{"location":"phase_6_checklist/#batch-005-mixed-balanced-sampling","title":"Batch 005 (Mixed - Balanced Sampling)","text":"<ul> <li> Prepare batch 005 (100 tasks, <code>--strategy balanced</code>)</li> <li> Optional LLM refinement</li> <li> Annotate (3-5 hours)</li> <li> Export, validate, evaluate</li> <li> Register batch</li> </ul>"},{"location":"phase_6_checklist/#week-4-quality-assurance-phase-6-completion","title":"Week 4: Quality Assurance &amp; Phase 6 Completion","text":""},{"location":"phase_6_checklist/#aggregate-analysis","title":"Aggregate Analysis","text":"<ul> <li> Combine all gold batches   <pre><code>Get-Content data/annotation/exports/batch_00*.jsonl | Out-File data/annotation/exports/gold_all_500.jsonl\n</code></pre></li> <li> Generate combined quality report   <pre><code>python scripts/annotation/quality_report.py --gold data/annotation/exports/gold_all_500.jsonl --output data/annotation/reports/gold_all_500_quality.md\n</code></pre></li> <li> Calculate inter-annotator agreement (if multiple annotators)</li> <li> Verify canonical coverage &gt;90%</li> <li> Check label distribution (target: 85% SYMPTOM / 15% PRODUCT)</li> </ul>"},{"location":"phase_6_checklist/#phase-6-completion-criteria","title":"Phase 6 Completion Criteria","text":"<ul> <li> \u2705 Total gold tasks \u2265500 (1000 ideal)</li> <li> \u2705 Inter-annotator agreement &gt;0.75 (IOU \u2265 0.5)</li> <li> \u2705 Canonical coverage &gt;90%</li> <li> \u2705 All batches registered in <code>registry.csv</code></li> <li> \u2705 Quality reports reviewed and approved</li> <li> \u2705 Annotation guidelines finalized</li> <li> \u2705 All integrity tests passing   <pre><code>pytest tests/test_curation_integrity.py -v\n</code></pre></li> </ul>"},{"location":"phase_6_checklist/#documentation-handoff","title":"Documentation &amp; Handoff","text":"<ul> <li> Update <code>README.md</code> with Phase 6 completion status</li> <li> Commit all gold files, reports, plots</li> <li> Tag release: <code>git tag v0.6.0 -m \"Phase 6 Complete: Gold Standard Assembly\"</code></li> <li> Push tag: <code>git push origin v0.6.0</code></li> <li> Prepare for Phase 7 (Token Classification Fine-Tuning)</li> </ul>"},{"location":"phase_6_checklist/#optional-scale-to-1000-batches-006-010","title":"\ud83d\ude80 Optional: Scale to 1000 (Batches 006-010)","text":"<p>If time and resources allow, continue to 1000 total annotations:</p> <ul> <li> Batch 006-010: 500 additional tasks (100 each)</li> <li> Maintain stratification and category balance</li> <li> Monitor annotator throughput and agreement</li> <li> Adjust guidelines based on cumulative quality reports</li> </ul> <p>Benefits of 1000 annotations: - More robust model training - Better evaluation of rare labels - Stronger baseline for active learning - Publication-ready dataset size</p>"},{"location":"phase_6_checklist/#success-metrics-dashboard","title":"\ud83d\udcca Success Metrics Dashboard","text":"Metric Target Current Status Batch 001 100 tasks 0 \ud83d\udd34 Not started Batch 002 100 tasks 0 \ud83d\udd34 Not started Batch 003 100 tasks 0 \ud83d\udd34 Not started Batch 004 100 tasks 0 \ud83d\udd34 Not started Batch 005 100 tasks 0 \ud83d\udd34 Not started Total Gold 500 0 \ud83d\udd34 Not started SYMPTOM Spans 400-900 0 \ud83d\udd34 Not started PRODUCT Spans 100-300 0 \ud83d\udd34 Not started Agreement (IOU) &gt;0.75 - \ud83d\udfe1 Pending Canonical Coverage &gt;90% - \ud83d\udfe1 Pending Avg Time/Task &lt;3 min - \ud83d\udfe1 Pending <p>Update this dashboard after each batch!</p>"},{"location":"phase_6_checklist/#troubleshooting-quick-links","title":"\ud83d\udee0\ufe0f Troubleshooting Quick Links","text":"<ul> <li>Low agreement? \u2192 See <code>docs/phase_6_gold_standard.md</code> \u00a7 Troubleshooting</li> <li>Slow annotation? \u2192 Reduce task length, add glossary</li> <li>High false positives? \u2192 Adjust weak labeling thresholds</li> <li>PII concerns? \u2192 Use <code>--deidentify</code> flag</li> <li>Script errors? \u2192 Check <code>scripts/annotation/cli.py --help</code></li> </ul>"},{"location":"phase_6_checklist/#documentation-references","title":"\ud83d\udcda Documentation References","text":"<ul> <li>Full Guide: <code>docs/phase_6_gold_standard.md</code></li> <li>Annotation Rules: <code>docs/annotation_guide.md</code></li> <li>Production Workflow: <code>docs/production_workflow.md</code></li> <li>Tutorial Notebook: <code>scripts/AnnotationWalkthrough.ipynb</code></li> <li>CLI Reference: <code>scripts/annotation/cli.py --help</code></li> </ul>"},{"location":"phase_6_checklist/#next-phase-preview","title":"\ud83c\udf93 Next Phase Preview","text":"<p>Phase 7: Token Classification Fine-Tuning - Convert gold JSONL to BIO-tagged format - Add classification head to BioBERT - Fine-tune 3-5 epochs (learning rate 2e-5) - Target: F1 &gt;0.87 (SYMPTOM), &gt;0.82 (PRODUCT) - See: <code>docs/phase_7_training.md</code> (to be created after Phase 6 complete)</p> <p>Document Version: 1.0 Created: November 28, 2025 Next Update: After Batch 001 complete Owner: SpanForge Project Team</p>"},{"location":"phase_6_gold_standard/","title":"Phase 6: Gold Standard Assembly Guide","text":"<p>Status: ACTIVE Version: 1.0 Date: November 28, 2025 Prerequisites: Phases 1-5 Complete (Weak Labeling, LLM Refinement, Annotation Infrastructure)</p>"},{"location":"phase_6_gold_standard/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Objectives</li> <li>Prerequisites Checklist</li> <li>Implementation Steps</li> <li>Quality Assurance</li> <li>Success Metrics</li> <li>Troubleshooting</li> <li>Next Steps (Phase 7)</li> </ol>"},{"location":"phase_6_gold_standard/#overview","title":"Overview","text":"<p>Phase 6 focuses on assembling high-quality gold standard annotations through systematic human curation of weak labels and LLM-refined spans. This gold standard dataset will serve as:</p> <ul> <li>Training data for supervised BioBERT fine-tuning (Phase 7)</li> <li>Evaluation benchmark for measuring model performance</li> <li>Quality reference for ongoing active learning iterations</li> </ul>"},{"location":"phase_6_gold_standard/#key-activities","title":"Key Activities","text":"<ol> <li>\u2705 Batch Preparation: Sample representative complaints using stratified sampling</li> <li>\u2705 Weak Label Generation: Apply lexicon-based NER to sampled texts</li> <li>\u2705 LLM Refinement: Enhance weak labels with LLM boundary correction (optional)</li> <li>\u2705 Human Annotation: Expert review and correction in Label Studio</li> <li>\u2705 Consensus &amp; Adjudication: Resolve annotator disagreements</li> <li>\u2705 Gold Export: Convert validated annotations to training-ready JSONL</li> <li>\u2705 Quality Validation: Verify integrity, coverage, and agreement metrics</li> </ol>"},{"location":"phase_6_gold_standard/#objectives","title":"Objectives","text":""},{"location":"phase_6_gold_standard/#primary-goals","title":"Primary Goals","text":"<ol> <li>Assemble 500-1000 gold-annotated complaints</li> <li>Target: 500 minimum, 1000 ideal for robust training</li> <li>Balanced across span densities and product categories</li> <li> <p>Representative of FDA CAERS complaint language</p> </li> <li> <p>Achieve &gt;0.75 inter-annotator agreement (IOU \u2265 0.5)</p> </li> <li>Measured on overlapping annotation batches</li> <li>Cohen's kappa for label agreement</li> <li> <p>Boundary precision &gt;80% exact match</p> </li> <li> <p>Maintain &gt;90% canonical coverage</p> </li> <li>All SYMPTOM spans map to lexicon entries</li> <li>Consistent terminology across annotators</li> <li>Clear handling of negation and ambiguity</li> </ol>"},{"location":"phase_6_gold_standard/#secondary-goals","title":"Secondary Goals","text":"<ul> <li>Document annotation guidelines and edge cases</li> <li>Build annotator calibration dataset (50-100 examples)</li> <li>Establish quality monitoring dashboards</li> <li>Create reproducible annotation SOP</li> </ul>"},{"location":"phase_6_gold_standard/#prerequisites-checklist","title":"Prerequisites Checklist","text":"<p>Before starting Phase 6, ensure:</p> <ul> <li>\u2705 Weak Labeling Module: <code>src/weak_labeling/</code> fully tested (531/531 tests passing)</li> <li>\u2705 LLM Agent: <code>src/llm_agent.py</code> operational (stub/OpenAI/Anthropic support)</li> <li>\u2705 Evaluation Metrics: <code>src/evaluation/metrics.py</code> implemented (10 functions)</li> <li>\u2705 Annotation Scripts: All <code>scripts/annotation/*.py</code> scripts present</li> <li>\u2705 Label Studio Setup: Local instance configured (telemetry disabled)</li> <li>\u2705 Documentation: <code>docs/annotation_guide.md</code>, <code>docs/production_workflow.md</code></li> <li>\u2705 CAERS Data: FDA complaints downloaded and weak-labeled</li> <li>\u2705 Lexicons: Symptom and product lexicons up-to-date</li> </ul>"},{"location":"phase_6_gold_standard/#environment-setup","title":"Environment Setup","text":"<pre><code># Activate environment\nconda activate NER\n\n# Verify all dependencies\npip install -r requirements.txt\npip install -r requirements-llm.txt  # Optional: for LLM refinement\n\n# Check annotation scripts\npython scripts/annotation/cli.py --help\n\n# Verify Label Studio (if using)\nlabel-studio --version\n</code></pre>"},{"location":"phase_6_gold_standard/#implementation-steps","title":"Implementation Steps","text":""},{"location":"phase_6_gold_standard/#step-1-prepare-initial-batch-n100","title":"Step 1: Prepare Initial Batch (n=100)","text":"<p>Purpose: Create first annotation batch with stratified sampling</p> <pre><code># Download CAERS cosmetics data (if not already done)\npython scripts/caers/download_caers.py `\n  --output data/caers/cosmetics_1000.jsonl `\n  --categories cosmetics `\n  --limit 1000 `\n  --min-spans 1\n\n# Prepare annotation batch\npython scripts/annotation/prepare_production_batch.py `\n  --input data/caers/cosmetics_1000.jsonl `\n  --output data/annotation/exports/batch_001.jsonl `\n  --n-tasks 100 `\n  --strategy stratified `\n  --min-spans 1 `\n  --max-text-len 500 `\n  --stats-output data/annotation/reports/batch_001_stats.json\n\n# Review batch statistics\nGet-Content data/annotation/reports/batch_001_stats.json | ConvertFrom-Json | ConvertTo-Json -Depth 10\n</code></pre> <p>Expected Output: - <code>batch_001.jsonl</code>: 100 tasks with weak labels - Stratified by span density (low/medium/high) - Avg 2-3 spans per task - Avg text length 150-250 characters</p>"},{"location":"phase_6_gold_standard/#step-2-optional-llm-refinement","title":"Step 2: Optional LLM Refinement","text":"<p>Purpose: Improve boundary precision before human annotation</p> <pre><code># Refine with OpenAI GPT-4 (requires OPENAI_API_KEY)\npython scripts/annotation/refine_llm.py `\n  --input data/annotation/exports/batch_001.jsonl `\n  --output data/annotation/exports/batch_001_refined.jsonl `\n  --provider openai `\n  --model gpt-4o-mini `\n  --temperature 0.1\n\n# OR use stub mode (no API calls)\npython scripts/annotation/refine_llm.py `\n  --input data/annotation/exports/batch_001.jsonl `\n  --output data/annotation/exports/batch_001_refined.jsonl `\n  --provider stub\n</code></pre> <p>Decision Matrix: - Use LLM: If boundary precision &lt;70%, budget allows API costs - Skip LLM: If weak labels already high quality, cost-sensitive, offline workflow</p>"},{"location":"phase_6_gold_standard/#step-3-import-to-label-studio","title":"Step 3: Import to Label Studio","text":"<p>Purpose: Load tasks into annotation interface</p> <pre><code># Initialize Label Studio project (first time only)\npython scripts/annotation/init_label_studio_project.py `\n  --project-name \"SpanForge Gold Standard\" `\n  --config data/annotation/config/label_config.xml\n\n# Import batch tasks\npython scripts/annotation/import_weak_to_labelstudio.py `\n  --input data/annotation/exports/batch_001_refined.jsonl `\n  --project-id 1 `\n  --confidence-filter 0.6\n</code></pre> <p>Alternative: Manual annotation without Label Studio (see <code>docs/annotation_guide.md</code>)</p>"},{"location":"phase_6_gold_standard/#step-4-human-annotation","title":"Step 4: Human Annotation","text":"<p>Purpose: Expert review and span correction</p> <p>Workflow: 1. Launch Label Studio: <code>label-studio start</code> 2. Navigate to project: http://localhost:8080 3. Review each task:    - \u2705 Correct span boundaries (remove extra words, fix punctuation)    - \u2705 Add missing spans (false negatives)    - \u2705 Remove incorrect spans (false positives)    - \u2705 Verify negation flags    - \u2705 Confirm canonical mappings 4. Submit completed tasks</p> <p>Guidelines (see <code>docs/annotation_guide.md</code> for full details): - Include full clinical phrase (e.g., \"severe burning sensation\" not just \"burning\") - Exclude trailing punctuation - Mark negated spans but don't delete them - Prefer specific terms over generic anatomy - When uncertain, add note in task comments</p> <p>Time Estimate: 2-3 minutes per task (100 tasks = 3-5 hours)</p>"},{"location":"phase_6_gold_standard/#step-5-export-convert","title":"Step 5: Export &amp; Convert","text":"<p>Purpose: Extract validated annotations from Label Studio</p> <pre><code># Export from Label Studio UI: Settings \u2192 Export \u2192 JSON\n\n# Convert to SpanForge gold format\npython scripts/annotation/convert_labelstudio.py `\n  --input data/annotation/exports/label_studio_export.json `\n  --output data/annotation/exports/batch_001_gold.jsonl `\n  --consensus majority `\n  --min-agree 2\n</code></pre> <p>Validation Checks: - All tasks have at least 1 annotator - Span boundaries align with text slices - Canonical fields populated - No duplicate spans</p>"},{"location":"phase_6_gold_standard/#step-6-quality-assurance","title":"Step 6: Quality Assurance","text":"<p>Purpose: Verify annotation quality and consistency</p> <pre><code># Generate quality report\npython scripts/annotation/quality_report.py `\n  --gold data/annotation/exports/batch_001_gold.jsonl `\n  --output data/annotation/reports/batch_001_quality.md\n\n# Compare weak vs LLM vs gold\npython scripts/annotation/evaluate_llm_refinement.py `\n  --weak data/annotation/exports/batch_001.jsonl `\n  --refined data/annotation/exports/batch_001_refined.jsonl `\n  --gold data/annotation/exports/batch_001_gold.jsonl `\n  --output data/annotation/reports/batch_001_evaluation.json `\n  --markdown `\n  --stratify label confidence span_length\n\n# Generate visualizations\npython scripts/annotation/plot_llm_metrics.py `\n  --report data/annotation/reports/batch_001_evaluation.json `\n  --output-dir data/annotation/plots/batch_001/ `\n  --formats png pdf `\n  --dpi 300 `\n  --plots all\n</code></pre> <p>Review Metrics: - IOU Improvement: +8-15% (weak \u2192 LLM vs gold) - Exact Match Rate: 70-85% (LLM boundaries vs gold) - Correction Rate: &gt;60% improved, &lt;10% worsened - F1 Score: &gt;0.85 (LLM precision/recall vs gold) - Inter-Annotator Agreement: &gt;0.75 (if multiple annotators)</p>"},{"location":"phase_6_gold_standard/#step-7-adjudication-if-needed","title":"Step 7: Adjudication (If Needed)","text":"<p>Purpose: Resolve annotator disagreements</p> <pre><code># Identify conflicts\npython scripts/annotation/adjudicate.py `\n  --gold data/annotation/exports/batch_001_gold.jsonl `\n  --output data/annotation/conflicts/batch_001_conflicts.json `\n  --strategy flag\n\n# Manual review of conflicts\n# Edit conflicts JSON to mark resolutions\n\n# Apply resolutions\npython scripts/annotation/adjudicate.py `\n  --gold data/annotation/exports/batch_001_gold.jsonl `\n  --conflicts data/annotation/conflicts/batch_001_conflicts_resolved.json `\n  --output data/annotation/exports/batch_001_gold_adjudicated.jsonl `\n  --strategy resolve\n</code></pre>"},{"location":"phase_6_gold_standard/#step-8-register-batch","title":"Step 8: Register Batch","text":"<p>Purpose: Track batch provenance and metadata</p> <pre><code>python scripts/annotation/register_batch.py `\n  --gold-file data/annotation/exports/batch_001_gold.jsonl `\n  --batch-id batch_001 `\n  --n-tasks 100 `\n  --annotators \"annotator_1,annotator_2\" `\n  --revision 1 `\n  --notes \"Initial gold standard batch - cosmetics complaints\"\n</code></pre> <p>Registry Entry (appended to <code>data/annotation/registry.csv</code>): <pre><code>timestamp,batch_id,gold_file,n_tasks,annotators,revision,notes\n2025-11-28T14:30:00,batch_001,data/annotation/exports/batch_001_gold.jsonl,100,annotator_1;annotator_2,1,Initial gold standard batch - cosmetics complaints\n</code></pre></p>"},{"location":"phase_6_gold_standard/#step-9-iterate-repeat-for-additional-batches","title":"Step 9: Iterate (Repeat for Additional Batches)","text":"<p>Purpose: Build up to 500-1000 gold annotations</p> <p>Recommended Batch Schedule: 1. Batch 001 (n=100): Cosmetics, initial calibration 2. Batch 002 (n=100): Cosmetics, refined guidelines 3. Batch 003 (n=100): Supplements, category expansion 4. Batch 004 (n=100): Personal care, diverse language 5. Batches 005-010 (n=100 each): Scale to 1000 total</p> <p>Iteration Strategy: - Review quality reports after each batch - Update <code>docs/annotation_guide.md</code> with new edge cases - Calibrate annotators on disagreements - Adjust weak labeling thresholds if needed - Track time-to-annotate and adjust task complexity</p>"},{"location":"phase_6_gold_standard/#quality-assurance","title":"Quality Assurance","text":""},{"location":"phase_6_gold_standard/#integrity-checks","title":"Integrity Checks","text":"<p>Run automated validation on all gold batches:</p> <pre><code># Test curation integrity\npytest tests/test_curation_integrity.py -v\n\n# Expected: All spans validate, text alignment correct, no orphaned canonicals\n</code></pre>"},{"location":"phase_6_gold_standard/#manual-spot-checks","title":"Manual Spot Checks","text":"<p>Periodically review: - 10 random tasks from each batch - All tasks with &gt;5 disagreements - Tasks with 0 spans (verify true negatives) - Tasks flagged in quality reports</p>"},{"location":"phase_6_gold_standard/#dashboard-monitoring","title":"Dashboard Monitoring","text":"<p>Track cumulative metrics: - Total gold annotations: 0 \u2192 500 \u2192 1000 - Avg spans per task: 2-3 stable - Label distribution: 85% SYMPTOM / 15% PRODUCT target - Annotator throughput: 2-3 min/task stable - Agreement trends: improving with calibration</p>"},{"location":"phase_6_gold_standard/#success-metrics","title":"Success Metrics","text":""},{"location":"phase_6_gold_standard/#phase-6-complete-when","title":"Phase 6 Complete When:","text":"<p>\u2705 Quantity: \u2265500 gold-annotated complaints (1000 ideal) \u2705 Quality: Inter-annotator agreement &gt;0.75 (IOU \u2265 0.5) \u2705 Coverage: &gt;90% canonical mapping for symptom spans \u2705 Balance: Stratified by span density and product category \u2705 Documentation: Annotation guidelines finalized with examples \u2705 Provenance: All batches registered in <code>registry.csv</code> \u2705 Validation: All integrity tests passing</p>"},{"location":"phase_6_gold_standard/#quantitative-targets","title":"Quantitative Targets","text":"Metric Target Current Status Total Gold Tasks 500-1000 0 \ud83d\udd34 Not started Inter-Annotator Agreement (IOU) &gt;0.75 - \ud83d\udfe1 Pending Canonical Coverage &gt;90% - \ud83d\udfe1 Pending SYMPTOM Spans 400-900 0 \ud83d\udd34 Not started PRODUCT Spans 100-300 0 \ud83d\udd34 Not started Avg Time/Task &lt;3 min - \ud83d\udfe1 Pending"},{"location":"phase_6_gold_standard/#troubleshooting","title":"Troubleshooting","text":""},{"location":"phase_6_gold_standard/#issue-low-inter-annotator-agreement-060","title":"Issue: Low Inter-Annotator Agreement (&lt;0.60)","text":"<p>Causes: - Unclear boundary rules - Ambiguous symptom definitions - Inconsistent negation handling</p> <p>Solutions: - Conduct calibration session with annotators - Add examples to <code>docs/annotation_guide.md</code> - Create practice batch with known-correct annotations - Use adjudication to establish precedents</p>"},{"location":"phase_6_gold_standard/#issue-high-false-positive-rate-in-weak-labels","title":"Issue: High False Positive Rate in Weak Labels","text":"<p>Causes: - Fuzzy threshold too low (0.88 \u2192 0.92) - Jaccard threshold too low (40 \u2192 50) - Lexicon contains non-specific terms</p> <p>Solutions: - Adjust thresholds in <code>src/weak_labeling/labeler.py</code> - Refine lexicons to remove ambiguous terms - Use LLM refinement to filter low-confidence spans</p>"},{"location":"phase_6_gold_standard/#issue-slow-annotation-throughput-5-mintask","title":"Issue: Slow Annotation Throughput (&gt;5 min/task)","text":"<p>Causes: - Tasks too long (&gt;500 chars) - Too many pre-annotated spans (&gt;10) - Complex medical terminology</p> <p>Solutions: - Reduce <code>--max-text-len</code> in batch preparation - Filter to simpler language (Flesch-Kincaid &gt;8th grade) - Provide terminology glossary - Split long tasks into shorter segments</p>"},{"location":"phase_6_gold_standard/#issue-pii-exposure-in-raw-complaints","title":"Issue: PII Exposure in Raw Complaints","text":"<p>Causes: - CAERS data contains user-submitted PII - De-identification patterns missed edge cases</p> <p>Solutions: - Use <code>--deidentify</code> flag in batch preparation - Manual review of high-risk fields (age, gender, dates) - Keep raw CAERS data in <code>data/raw/</code> (gitignored) - Audit exported gold files before committing</p>"},{"location":"phase_6_gold_standard/#next-steps-phase-7","title":"Next Steps (Phase 7)","text":"<p>Once Phase 6 is complete with \u2265500 gold annotations:</p>"},{"location":"phase_6_gold_standard/#phase-7-token-classification-fine-tuning","title":"Phase 7: Token Classification Fine-Tuning","text":"<ol> <li>Prepare Training Data: Convert gold JSONL to BIO-tagged format</li> <li>Split Dataset: 80% train / 10% validation / 10% test</li> <li>Add Classification Head: Extend BioBERT with token classification layer</li> <li>Fine-Tune: 3-5 epochs with learning rate 2e-5</li> <li>Evaluate: Compare weak labels vs fine-tuned model</li> <li>Iterate: Active learning to identify hard examples</li> </ol> <p>Target Metrics (Phase 7): - Precision: &gt;0.90 (SYMPTOM), &gt;0.85 (PRODUCT) - Recall: &gt;0.85 (SYMPTOM), &gt;0.80 (PRODUCT) - F1: &gt;0.87 (SYMPTOM), &gt;0.82 (PRODUCT)</p> <p>See: <code>docs/phase_7_training.md</code> (to be created)</p>"},{"location":"phase_6_gold_standard/#appendix-a-cli-quick-reference","title":"Appendix A: CLI Quick Reference","text":"<pre><code># Prepare batch\npython scripts/annotation/prepare_production_batch.py --input CAERS.jsonl --output batch.jsonl --n-tasks 100\n\n# LLM refinement\npython scripts/annotation/refine_llm.py --input batch.jsonl --output refined.jsonl --provider openai\n\n# Convert Label Studio export\npython scripts/annotation/convert_labelstudio.py --input export.json --output gold.jsonl --consensus majority\n\n# Quality report\npython scripts/annotation/quality_report.py --gold gold.jsonl --output report.md\n\n# Evaluation\npython scripts/annotation/evaluate_llm_refinement.py --weak batch.jsonl --refined refined.jsonl --gold gold.jsonl --output eval.json --markdown\n\n# Visualize\npython scripts/annotation/plot_llm_metrics.py --report eval.json --output-dir plots/ --plots all\n\n# Register batch\npython scripts/annotation/register_batch.py --gold-file gold.jsonl --batch-id batch_001 --n-tasks 100 --annotators \"user1,user2\"\n</code></pre>"},{"location":"phase_6_gold_standard/#appendix-b-annotation-workflow-diagram","title":"Appendix B: Annotation Workflow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    PHASE 6 WORKFLOW                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 1: Batch Preparation\n    \u251c\u2500\u2192 Download CAERS data (cosmetics, supplements, etc.)\n    \u251c\u2500\u2192 Filter by quality (min spans, text length)\n    \u251c\u2500\u2192 Stratified sampling (span density, category)\n    \u2514\u2500\u2192 Output: batch_XXX.jsonl (100 tasks)\n         \u2502\n         \u2193\nStep 2: Weak Labeling (Already Done)\n    \u251c\u2500\u2192 Lexicon matching (fuzzy + Jaccard)\n    \u251c\u2500\u2192 Negation detection (bidirectional window)\n    \u2514\u2500\u2192 Confidence scoring\n         \u2502\n         \u2193\nStep 3: LLM Refinement (Optional)\n    \u251c\u2500\u2192 Boundary correction (remove adjectives)\n    \u251c\u2500\u2192 Negation validation\n    \u2514\u2500\u2192 Canonical normalization\n         \u2502\n         \u2193\nStep 4: Human Annotation\n    \u251c\u2500\u2192 Load into Label Studio\n    \u251c\u2500\u2192 Expert review + correction\n    \u251c\u2500\u2192 Add missing, remove false positives\n    \u2514\u2500\u2192 Mark negation, verify canonicals\n         \u2502\n         \u2193\nStep 5: Export &amp; Convert\n    \u251c\u2500\u2192 Export from Label Studio (JSON)\n    \u251c\u2500\u2192 Convert to SpanForge format (JSONL)\n    \u2514\u2500\u2192 Apply consensus strategy\n         \u2502\n         \u2193\nStep 6: Quality Validation\n    \u251c\u2500\u2192 Integrity checks (span alignment, canonicals)\n    \u251c\u2500\u2192 Agreement metrics (IOU, kappa)\n    \u251c\u2500\u2192 Evaluation report (weak \u2192 LLM \u2192 gold)\n    \u2514\u2500\u2192 Visualizations (IOU uplift, calibration)\n         \u2502\n         \u2193\nStep 7: Adjudication (If Conflicts)\n    \u251c\u2500\u2192 Identify disagreements\n    \u251c\u2500\u2192 Manual resolution\n    \u2514\u2500\u2192 Update gold file\n         \u2502\n         \u2193\nStep 8: Register Batch\n    \u251c\u2500\u2192 Add to registry.csv\n    \u251c\u2500\u2192 Record provenance (annotators, date)\n    \u2514\u2500\u2192 Archive reports\n         \u2502\n         \u2193\nStep 9: Iterate (Repeat Until 500-1000 Gold)\n    \u2514\u2500\u2192 Next batch with refined guidelines\n</code></pre>"},{"location":"phase_6_gold_standard/#appendix-c-file-locations","title":"Appendix C: File Locations","text":"<pre><code>data/\n\u251c\u2500\u2500 annotation/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 label_config.xml          # Label Studio configuration\n\u2502   \u251c\u2500\u2500 exports/\n\u2502   \u2502   \u251c\u2500\u2500 batch_001.jsonl            # Weak labels (input)\n\u2502   \u2502   \u251c\u2500\u2500 batch_001_refined.jsonl    # LLM refined (optional)\n\u2502   \u2502   \u251c\u2500\u2500 batch_001_gold.jsonl       # Gold standard (output)\n\u2502   \u2502   \u2514\u2500\u2500 *.json                     # Label Studio exports\n\u2502   \u251c\u2500\u2500 reports/\n\u2502   \u2502   \u251c\u2500\u2500 batch_001_stats.json       # Batch statistics\n\u2502   \u2502   \u251c\u2500\u2500 batch_001_quality.md       # Quality report\n\u2502   \u2502   \u2514\u2500\u2500 batch_001_evaluation.json  # Evaluation metrics\n\u2502   \u251c\u2500\u2500 plots/\n\u2502   \u2502   \u2514\u2500\u2500 batch_001/                 # Visualization outputs\n\u2502   \u251c\u2500\u2500 conflicts/\n\u2502   \u2502   \u2514\u2500\u2500 batch_001_conflicts.json   # Adjudication workspace\n\u2502   \u2514\u2500\u2500 registry.csv                   # Batch provenance log\n\u2502\n\u251c\u2500\u2500 caers/\n\u2502   \u251c\u2500\u2500 raw/                           # Downloaded CAERS CSV (gitignored)\n\u2502   \u2514\u2500\u2500 *.jsonl                        # Processed weak-labeled complaints\n\u2502\n\u2514\u2500\u2500 lexicon/\n    \u251c\u2500\u2500 symptoms.csv                   # Symptom lexicon\n    \u2514\u2500\u2500 products.csv                   # Product lexicon\n</code></pre> <p>Document Version: 1.0 Last Updated: November 28, 2025 Next Review: After Batch 001 complete Owner: SpanForge Project Team</p>"},{"location":"production_workflow/","title":"Production Evaluation Workflow Guide","text":"<p>Version: 1.0 Purpose: Step-by-step instructions for running evaluation on real annotation batches Audience: Annotators, project managers, NER practitioners</p>"},{"location":"production_workflow/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Prerequisites</li> <li>Workflow Steps</li> <li>Data Validation</li> <li>CLI Execution Examples</li> <li>Result Interpretation</li> <li>Iteration Strategy</li> <li>Troubleshooting</li> </ol>"},{"location":"production_workflow/#overview","title":"Overview","text":"<p>This guide walks through the complete production evaluation workflow for measuring annotation quality on real biomedical complaints data.</p>"},{"location":"production_workflow/#workflow-phases","title":"Workflow Phases","text":"<pre><code>Phase 1: Batch Preparation\n   \u2193\nPhase 2: Weak Label Generation\n   \u2193\nPhase 3: LLM Refinement (Optional)\n   \u2193\nPhase 4: Human Annotation (Label Studio)\n   \u2193\nPhase 5: Export &amp; Conversion\n   \u2193\nPhase 6: Evaluation &amp; Analysis\n   \u2193\nPhase 7: Iteration &amp; Improvement\n</code></pre>"},{"location":"production_workflow/#key-metrics","title":"Key Metrics","text":"<ul> <li>IOU Improvement: +8-15% target (weak \u2192 LLM vs gold)</li> <li>Exact Match Rate: 70-85% target (LLM boundaries align with gold)</li> <li>Correction Rate: &gt;60% improved, &lt;10% worsened</li> <li>F1 Score: &gt;0.85 target (LLM precision/recall vs gold)</li> </ul>"},{"location":"production_workflow/#prerequisites","title":"Prerequisites","text":""},{"location":"production_workflow/#environment-setup","title":"Environment Setup","text":"<ol> <li> <p>Python Environment:    <pre><code>conda activate NER\npip install -r requirements.txt\npip install -r requirements-llm.txt\npip install -r requirements-viz.txt  # Optional for plots\n</code></pre></p> </li> <li> <p>Label Studio:    <pre><code>pip install label-studio\n\n# Disable telemetry (PowerShell)\n$env:LABEL_STUDIO_DISABLE_TELEMETRY = \"1\"\n</code></pre></p> </li> <li> <p>LLM Provider (if using refinement):    <pre><code># OpenAI\n$env:OPENAI_API_KEY = \"sk-...\"\n\n# Or Azure OpenAI\n$env:AZURE_OPENAI_API_KEY = \"...\"\n$env:AZURE_OPENAI_ENDPOINT = \"https://...\"\n$env:AZURE_OPENAI_DEPLOYMENT = \"gpt-4\"\n\n# Or Anthropic\n$env:ANTHROPIC_API_KEY = \"sk-ant-...\"\n</code></pre></p> </li> </ol>"},{"location":"production_workflow/#data-requirements","title":"Data Requirements","text":"<ul> <li>Raw Complaints: De-identified text files (UTF-8 encoding)</li> <li>Lexicons: <code>data/lexicon/symptoms.csv</code>, <code>data/lexicon/products.csv</code></li> <li>Config: <code>data/annotation/config/label_config.xml</code></li> </ul>"},{"location":"production_workflow/#workflow-steps","title":"Workflow Steps","text":""},{"location":"production_workflow/#step-1-prepare-production-batch","title":"Step 1: Prepare Production Batch","text":"<p>Goal: Select 100 representative complaints for annotation</p> <p>Command: <pre><code>python scripts/annotation/prepare_production_batch.py \\\n  --input data/raw/complaints_pool.txt \\\n  --output data/annotation/batches/batch_001/ \\\n  --batch-size 100 \\\n  --stratify confidence \\\n  --confidence-bins 0.5,0.7,0.85 \\\n  --deidentify\n</code></pre></p> <p>Expected Output: <pre><code>data/annotation/batches/batch_001/\n\u251c\u2500\u2500 manifest.json          # Batch metadata (source, timestamp, counts)\n\u251c\u2500\u2500 tasks.json             # Label Studio import format\n\u251c\u2500\u2500 weak_labels.jsonl      # Weak labels only (baseline)\n\u251c\u2500\u2500 llm_refined.jsonl      # LLM-refined labels (optional)\n\u2514\u2500\u2500 texts.txt              # De-identified complaint texts\n</code></pre></p> <p>Manifest Example: <pre><code>{\n  \"batch_id\": \"batch_001\",\n  \"created_at\": \"2025-11-25T10:30:00Z\",\n  \"source\": \"complaints_pool.txt\",\n  \"total_tasks\": 100,\n  \"stratification\": {\n    \"low_confidence\": 30,    // confidence &lt; 0.7\n    \"medium_confidence\": 50, // 0.7 \u2264 confidence &lt; 0.85\n    \"high_confidence\": 20    // confidence \u2265 0.85\n  },\n  \"llm_refined\": true,\n  \"llm_provider\": \"openai\",\n  \"llm_model\": \"gpt-4\",\n  \"estimated_cost\": \"$3.25\"\n}\n</code></pre></p> <p>Tips: - Use <code>--stratify confidence</code> to ensure diverse difficulty levels - <code>--deidentify</code> removes PII (names, dates, locations) automatically - <code>--llm-refine</code> flag runs LLM refinement during batch prep (saves time)</p>"},{"location":"production_workflow/#step-2-import-to-label-studio","title":"Step 2: Import to Label Studio","text":"<p>Manual Steps:</p> <ol> <li> <p>Launch Label Studio:    <pre><code>label-studio start\n</code></pre>    Opens at http://localhost:8080</p> </li> <li> <p>Create Project:</p> </li> <li>Name: \"Batch 001 - Production NER\"</li> <li> <p>Description: \"100 complaints from complaints_pool.txt\"</p> </li> <li> <p>Import Config:</p> </li> <li>Settings \u2192 Labeling Interface \u2192 Code</li> <li>Copy from <code>data/annotation/config/label_config.xml</code></li> <li> <p>Save</p> </li> <li> <p>Import Tasks:</p> </li> <li>Click \"Import\" button</li> <li>Upload <code>data/annotation/batches/batch_001/tasks.json</code></li> <li>Verify pre-annotations appear (LLM suggestions)</li> </ol> <p>Verification: - Check task count: Should show 100 tasks - Open first task: Verify SYMPTOM/PRODUCT spans visible - Test hotkeys: <code>s</code> for SYMPTOM, <code>p</code> for PRODUCT</p>"},{"location":"production_workflow/#step-3-annotate","title":"Step 3: Annotate","text":"<p>Guidelines: - Follow <code>docs/annotation_guide.md</code> for boundary rules - Use <code>scripts/AnnotationWalkthrough.ipynb</code> for practice examples - Target: 2-3 hours per 100 tasks (experienced annotator)</p> <p>Quality Checks (every 25 tasks): 1. Export current progress 2. Run quick evaluation (see Step 5) 3. Adjust annotation strategy if metrics off-target</p> <p>Common Issues: - Boundary Errors: Review Section 7 of tutorial notebook - Negation Confusion: Annotate symptom only, exclude \"no\"/\"without\" - Anatomy Tokens: Skip single anatomy words unless part of symptom phrase</p>"},{"location":"production_workflow/#step-4-export-from-label-studio","title":"Step 4: Export from Label Studio","text":"<p>Manual Steps:</p> <ol> <li>Complete All Tasks:</li> <li> <p>Verify all 100 tasks submitted (status: \"completed\")</p> </li> <li> <p>Export JSON:</p> </li> <li>Click \"Export\" button</li> <li>Select \"JSON\" format</li> <li> <p>Download file (e.g., <code>project-1-at-2025-11-25-export.json</code>)</p> </li> <li> <p>Save to Data Directory:    <pre><code>Move-Item project-1-at-2025-11-25-export.json `\n  data/annotation/raw/batch_001_export.json\n</code></pre></p> </li> </ol>"},{"location":"production_workflow/#step-5-convert-to-gold-standard","title":"Step 5: Convert to Gold Standard","text":"<p>Command: <pre><code>python scripts/annotation/convert_labelstudio.py \\\n  --input data/annotation/raw/batch_001_export.json \\\n  --output data/gold/batch_001.jsonl \\\n  --source \"batch_001\" \\\n  --annotator \"your_name\" \\\n  --symptom-lexicon data/lexicon/symptoms.csv \\\n  --product-lexicon data/lexicon/products.csv\n</code></pre></p> <p>Expected Output: <pre><code>\u2705 Converted 100 tasks to gold standard\n\u2705 Total spans: 287 (203 SYMPTOM, 84 PRODUCT)\n\u2705 Output: data/gold/batch_001.jsonl\n\nQuality Report:\n  - Average spans per task: 2.87\n  - Canonical coverage: 94.2% (191/203 symptoms in lexicon)\n  - Boundary integrity: 100.0% (all spans valid)\n</code></pre></p> <p>Validation Checks: - Canonical coverage \u226590%: Most symptoms map to lexicon entries - Boundary integrity 100%: All spans have valid start/end positions - Average spans 2-4: Reasonable density for complaints</p>"},{"location":"production_workflow/#step-6-run-evaluation","title":"Step 6: Run Evaluation","text":"<p>Command: <pre><code>python scripts/annotation/cli.py evaluate-llm \\\n  --weak data/annotation/batches/batch_001/weak_labels.jsonl \\\n  --refined data/annotation/batches/batch_001/llm_refined.jsonl \\\n  --gold data/gold/batch_001.jsonl \\\n  --output data/annotation/reports/batch_001_eval.json \\\n  --markdown \\\n  --stratify label confidence span_length\n</code></pre></p> <p>Expected Output: <pre><code>=== Evaluation Summary ===\n\nIOU Improvement:\n  Weak Mean IOU:  0.823\n  LLM Mean IOU:   0.901\n  Delta:          +0.078 (+9.5% improvement)\n\nBoundary Precision:\n  Weak Exact Match:  58.7% (169/288 spans)\n  LLM Exact Match:   73.6% (212/288 spans)\n  LLM Mean IOU:      0.901\n\nCorrection Rate:\n  Total Modified:  119 spans\n  Improved:        78 spans (65.5%)\n  Worsened:        11 spans (9.2%)\n  Unchanged:       30 spans (25.2%)\n\nLLM Performance vs Gold:\n  Precision:  0.883 (25 FP)\n  Recall:     0.912 (22 FN)\n  F1 Score:   0.897\n\nStratified Analysis:\n  By Label:\n    SYMPTOM: F1=0.902 (N=203)\n    PRODUCT: F1=0.881 (N=84)\n\n  By Confidence:\n    Low (0.0-0.7):    IOU delta=+12.3% (N=87)\n    Medium (0.7-0.85): IOU delta=+8.1% (N=144)\n    High (0.85-1.0):   IOU delta=+3.2% (N=57)\n\n\u2705 Report saved: data/annotation/reports/batch_001_eval.json\n\u2705 Markdown saved: data/annotation/reports/batch_001_eval.md\n</code></pre></p> <p>Files Generated: - <code>batch_001_eval.json</code>: Full report with all metrics - <code>batch_001_eval.md</code>: Human-readable summary with tables</p>"},{"location":"production_workflow/#step-7-generate-visualizations","title":"Step 7: Generate Visualizations","text":"<p>Command: <pre><code>python scripts/annotation/cli.py plot-metrics \\\n  --report data/annotation/reports/batch_001_eval.json \\\n  --output-dir data/annotation/plots/batch_001/ \\\n  --formats png pdf \\\n  --dpi 300 \\\n  --plots all\n</code></pre></p> <p>Expected Output: <pre><code>Generated 6 plots in data/annotation/plots/batch_001/:\n  \u2705 iou_uplift.png (Weak vs LLM IOU distribution)\n  \u2705 calibration_curve.png (Confidence reliability)\n  \u2705 correction_rate.png (Improved/worsened breakdown)\n  \u2705 prf_comparison.png (Precision/Recall/F1 comparison)\n  \u2705 stratified_label.png (F1 by entity type)\n  \u2705 stratified_confidence.png (IOU delta by confidence bucket)\n</code></pre></p> <p>Use Cases: - Presentations: High-DPI PNG for slides - Reports: PDF for professional documentation - Analysis: Identify patterns (e.g., LLM over-corrects low-confidence spans)</p>"},{"location":"production_workflow/#data-validation","title":"Data Validation","text":""},{"location":"production_workflow/#pre-evaluation-checks","title":"Pre-Evaluation Checks","text":"<p>Run these checks before evaluation to avoid errors:</p> <p>1. File Existence: <pre><code>Test-Path data/annotation/batches/batch_001/weak_labels.jsonl\nTest-Path data/annotation/batches/batch_001/llm_refined.jsonl\nTest-Path data/gold/batch_001.jsonl\n</code></pre></p> <p>2. JSONL Format Validation: <pre><code>import json\nfrom pathlib import Path\n\ndef validate_jsonl(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f, 1):\n            try:\n                rec = json.loads(line)\n                assert 'id' in rec, f\"Line {i}: Missing 'id' field\"\n                assert 'text' in rec, f\"Line {i}: Missing 'text' field\"\n                assert 'spans' in rec, f\"Line {i}: Missing 'spans' field\"\n                for span in rec['spans']:\n                    assert 'start' in span and 'end' in span, f\"Line {i}: Invalid span\"\n            except Exception as e:\n                print(f\"\u274c Error at line {i}: {e}\")\n                return False\n    print(f\"\u2705 {path} is valid JSONL\")\n    return True\n\nvalidate_jsonl('data/gold/batch_001.jsonl')\n</code></pre></p> <p>3. Span Integrity Test: <pre><code># Verify all spans have valid start/end positions\nwith open('data/gold/batch_001.jsonl', 'r') as f:\n    for line in f:\n        rec = json.loads(line)\n        text = rec['text']\n        for span in rec['spans']:\n            start, end = span['start'], span['end']\n            extracted = text[start:end]\n            assert extracted == span['text'], \\\n                f\"Span mismatch: '{span['text']}' != '{extracted}'\"\nprint(\"\u2705 All spans have valid boundaries\")\n</code></pre></p>"},{"location":"production_workflow/#cli-execution-examples","title":"CLI Execution Examples","text":""},{"location":"production_workflow/#example-1-quick-evaluation-no-stratification","title":"Example 1: Quick Evaluation (No Stratification)","text":"<pre><code>python scripts/annotation/cli.py evaluate-llm \\\n  --weak data/annotation/batches/batch_001/weak_labels.jsonl \\\n  --refined data/annotation/batches/batch_001/llm_refined.jsonl \\\n  --gold data/gold/batch_001.jsonl \\\n  --output data/annotation/reports/batch_001_quick.json\n</code></pre> <p>Use Case: Fast quality check during annotation (no stratified analysis)</p>"},{"location":"production_workflow/#example-2-full-evaluation-with-all-stratifications","title":"Example 2: Full Evaluation with All Stratifications","text":"<pre><code>python scripts/annotation/cli.py evaluate-llm \\\n  --weak data/annotation/batches/batch_001/weak_labels.jsonl \\\n  --refined data/annotation/batches/batch_001/llm_refined.jsonl \\\n  --gold data/gold/batch_001.jsonl \\\n  --output data/annotation/reports/batch_001_full.json \\\n  --markdown \\\n  --stratify label confidence span_length\n</code></pre> <p>Use Case: Comprehensive analysis for final batch report</p>"},{"location":"production_workflow/#example-3-confidence-only-stratification","title":"Example 3: Confidence-Only Stratification","text":"<pre><code>python scripts/annotation/cli.py evaluate-llm \\\n  --weak data/annotation/batches/batch_001/weak_labels.jsonl \\\n  --refined data/annotation/batches/batch_001/llm_refined.jsonl \\\n  --gold data/gold/batch_001.jsonl \\\n  --output data/annotation/reports/batch_001_conf.json \\\n  --stratify confidence\n</code></pre> <p>Use Case: Focus on LLM performance by confidence bucket (optimize refinement threshold)</p>"},{"location":"production_workflow/#example-4-visualization-only","title":"Example 4: Visualization Only","text":"<pre><code># Assumes evaluation already run\npython scripts/annotation/cli.py plot-metrics \\\n  --report data/annotation/reports/batch_001_eval.json \\\n  --output-dir data/annotation/plots/batch_001/ \\\n  --formats png \\\n  --plots iou calibration\n</code></pre> <p>Use Case: Generate specific plots for presentation (skip full suite)</p>"},{"location":"production_workflow/#result-interpretation","title":"Result Interpretation","text":""},{"location":"production_workflow/#target-metrics-healthy-annotation-quality","title":"Target Metrics (Healthy Annotation Quality)","text":"Metric Target Red Flag (&lt;) Interpretation IOU Improvement +8-15% +5% LLM refinement provides meaningful boundary correction Exact Match Rate 70-85% 60% LLM boundaries align well with gold standard Correction Rate (Improved) &gt;60% 50% Majority of LLM changes improve weak labels Correction Rate (Worsened) &lt;10% &gt;15% LLM rarely introduces errors F1 Score (LLM vs Gold) &gt;0.85 &lt;0.75 High precision and recall Canonical Coverage &gt;90% &lt;80% Most symptoms map to lexicon entries"},{"location":"production_workflow/#interpretation-guide","title":"Interpretation Guide","text":""},{"location":"production_workflow/#1-iou-improvement-below-target-5","title":"1. IOU Improvement Below Target (+5%)","text":"<p>Possible Causes: - Weak labels already high quality (lexicon well-tuned) - LLM over-correcting (removing valid multi-word terms) - Gold standard boundaries inconsistent</p> <p>Actions: - Review worsened spans (correction rate breakdown) - Check if LLM removing important context (e.g., \"burning sensation\" \u2192 \"burning\") - Calibrate lexicon thresholds (increase fuzzy threshold to 0.90?)</p>"},{"location":"production_workflow/#2-high-worsened-rate-10","title":"2. High Worsened Rate (&gt;10%)","text":"<p>Possible Causes: - LLM prompt too aggressive (removing necessary words) - LLM hallucinating boundaries (rare but possible) - Annotator inconsistency (gold standard variability)</p> <p>Actions: - Inspect worsened spans manually:   <pre><code># Load evaluation report\nimport json\nwith open('data/annotation/reports/batch_001_eval.json', 'r') as f:\n    report = json.load(f)\n\n# Find worsened spans\ncorrections = report['overall']['correction_details']\nworsened = [c for c in corrections if c['category'] == 'worsened']\n\nfor span in worsened[:10]:\n    print(f\"Weak:  '{span['weak_text']}'\")\n    print(f\"LLM:   '{span['llm_text']}'\")\n    print(f\"Gold:  '{span['gold_text']}'\")\n    print(f\"IOU:   {span['weak_iou']:.3f} \u2192 {span['llm_iou']:.3f} (\u0394={span['iou_delta']:.3f})\")\n    print()\n</code></pre> - Adjust LLM prompt (reduce aggressiveness) - Re-annotate sample tasks for consistency check</p>"},{"location":"production_workflow/#3-low-canonical-coverage-80","title":"3. Low Canonical Coverage (&lt;80%)","text":"<p>Possible Causes: - Lexicon incomplete (missing colloquial terms) - Annotators using non-canonical synonyms - Domain drift (new symptoms not in lexicon)</p> <p>Actions: - Extract missing terms:   <pre><code># Find spans not in lexicon\nimport pandas as pd\n\nsymptoms = pd.read_csv('data/lexicon/symptoms.csv')\ncanonical_set = set(symptoms['Canonical Term'].str.lower())\n\nwith open('data/gold/batch_001.jsonl', 'r') as f:\n    for line in f:\n        rec = json.loads(line)\n        for span in rec['spans']:\n            if span['label'] == 'SYMPTOM':\n                term = span['text'].lower()\n                if term not in canonical_set:\n                    print(f\"Missing: '{term}'\")\n</code></pre> - Update lexicon with new entries - Re-run weak labeling with expanded lexicon</p>"},{"location":"production_workflow/#4-low-f1-score-075","title":"4. Low F1 Score (&lt;0.75)","text":"<p>Possible Causes: - High false positive rate (LLM over-generating spans) - High false negative rate (LLM missing valid spans) - Inconsistent annotation guidelines</p> <p>Actions: - Check precision vs recall:   - Low precision (many FP): LLM too liberal; increase confidence threshold   - Low recall (many FN): LLM too conservative; review skipped spans - Stratify by confidence:   <pre><code>python scripts/annotation/cli.py evaluate-llm ... --stratify confidence\n</code></pre>   If low-confidence spans drag down F1, filter them out during import</p>"},{"location":"production_workflow/#iteration-strategy","title":"Iteration Strategy","text":""},{"location":"production_workflow/#after-first-batch-100-tasks","title":"After First Batch (100 tasks)","text":"<p>Goals: 1. Identify systematic errors (boundary, negation, anatomy) 2. Refine prompts/lexicons based on worsened spans 3. Calibrate confidence thresholds for next batch</p> <p>Workflow:</p> <pre><code># 1. Analyze worsened spans\nimport json\nfrom collections import Counter\n\nwith open('data/annotation/reports/batch_001_eval.json', 'r') as f:\n    report = json.load(f)\n\ncorrections = report['overall']['correction_details']\nworsened = [c for c in corrections if c['category'] == 'worsened']\n\n# Common patterns\nllm_changes = [(c['weak_text'], c['llm_text']) for c in worsened]\nchange_patterns = Counter([\n    'removed_adjective' if 'severe' in weak or 'mild' in weak else\n    'truncated_compound' if len(llm.split()) &lt; len(weak.split()) else\n    'other'\n    for weak, llm in llm_changes\n])\n\nprint(\"Worsened Patterns:\")\nfor pattern, count in change_patterns.most_common():\n    print(f\"  {pattern}: {count}\")\n</code></pre> <p>Example Output: <pre><code>Worsened Patterns:\n  truncated_compound: 7  \u2190 LLM removing important words\n  removed_adjective: 3   \u2190 Expected (but check if gold includes adjectives)\n  other: 1\n</code></pre></p> <p>Action: If <code>truncated_compound</code> high, adjust LLM prompt: <pre><code># In src/llm_agent.py, update prompt:\n\"Preserve multi-word medical terms even if they include anatomical references. \nOnly remove intensity adjectives (severe, mild, slight).\"\n</code></pre></p>"},{"location":"production_workflow/#after-third-batch-300-tasks-total","title":"After Third Batch (300 tasks total)","text":"<p>Goals: 1. Measure inter-batch consistency 2. Estimate final model performance (extrapolate F1) 3. Decide on batch size for remaining data</p> <p>Workflow:</p> <pre><code># Compare metrics across batches\nimport pandas as pd\n\nbatches = ['batch_001', 'batch_002', 'batch_003']\nmetrics = []\n\nfor batch in batches:\n    with open(f'data/annotation/reports/{batch}_eval.json', 'r') as f:\n        report = json.load(f)\n        overall = report['overall']\n        metrics.append({\n            'batch': batch,\n            'iou_improvement': overall['iou_improvement_pct'],\n            'f1': overall['llm_prf']['f1'],\n            'worsened_pct': overall['correction_rate']['worsened_pct']\n        })\n\ndf = pd.DataFrame(metrics)\nprint(df)\nprint(f\"\\nMean F1: {df['f1'].mean():.3f} (\u00b1{df['f1'].std():.3f})\")\n</code></pre> <p>Example Output: <pre><code>       batch  iou_improvement    f1  worsened_pct\n0  batch_001            9.5  0.897           9.2\n1  batch_002           11.2  0.903           7.8\n2  batch_003            8.7  0.891          10.1\n\nMean F1: 0.897 (\u00b10.006)  \u2190 Stable performance, ready to scale\n</code></pre></p> <p>Decision: - If F1 stable (std &lt;0.01): Scale to 500-task batches - If F1 volatile (std &gt;0.02): Continue 100-task batches, investigate variability</p>"},{"location":"production_workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"production_workflow/#issue-1-evaluation-script-fails-with-mismatched-ids","title":"Issue 1: Evaluation Script Fails with \"Mismatched IDs\"","text":"<p>Error Message: <pre><code>ValueError: IDs in weak labels do not match gold labels\n</code></pre></p> <p>Cause: Batch preparation and conversion used different ID schemes</p> <p>Solution: <pre><code># Ensure IDs consistent across files\n# In prepare_production_batch.py:\ntask_id = f\"batch_{batch_num:03d}_task_{idx:03d}\"\n\n# In convert_labelstudio.py:\n# Preserve original task IDs from Label Studio export\ngold_id = original_task['data']['id']  # Not generated fresh\n</code></pre></p>"},{"location":"production_workflow/#issue-2-calibration-curve-shows-poor-confidence-reliability","title":"Issue 2: Calibration Curve Shows Poor Confidence Reliability","text":"<p>Symptom: Calibration curve plot shows large gap between expected and observed IOU</p> <p>Cause: Weak label confidence scores not calibrated (fuzzy + jaccard heuristic inaccurate)</p> <p>Solution: <pre><code># Recalibrate confidence formula in src/weak_label.py:\n# Current: 0.8*fuzzy + 0.2*jaccard\n# Tune weights based on evaluation data:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Collect (confidence, IOU) pairs from batch evaluations\nX = np.array([span['confidence'] for span in all_spans]).reshape(-1, 1)\ny = np.array([span['iou_vs_gold'] for span in all_spans])\n\nmodel = LinearRegression().fit(X, y)\nprint(f\"Calibrated weights: {model.coef_}, intercept: {model.intercept_}\")\n\n# Update confidence formula with learned weights\n</code></pre></p>"},{"location":"production_workflow/#issue-3-llm-refinement-hangs-on-api-calls","title":"Issue 3: LLM Refinement Hangs on API Calls","text":"<p>Symptom: Script timeout after 60 seconds, no LLM response</p> <p>Cause: API key invalid, rate limit hit, or network issue</p> <p>Solution: <pre><code># Test API connectivity\npython -c \"\nfrom openai import OpenAI\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model='gpt-4',\n    messages=[{'role': 'user', 'content': 'test'}],\n    max_tokens=10\n)\nprint(response.choices[0].message.content)\n\"\n\n# If fails, check:\n# 1. API key valid: echo $env:OPENAI_API_KEY\n# 2. Rate limit: Reduce batch size or add delay\n# 3. Network: Test curl https://api.openai.com/v1/models\n</code></pre></p>"},{"location":"production_workflow/#issue-4-markdown-report-missing-stratification-tables","title":"Issue 4: Markdown Report Missing Stratification Tables","text":"<p>Symptom: <code>batch_001_eval.md</code> shows overall metrics but no stratified breakdowns</p> <p>Cause: <code>--stratify</code> flag not used during evaluation</p> <p>Solution: <pre><code># Re-run evaluation with stratification\npython scripts/annotation/cli.py evaluate-llm \\\n  --weak ... \\\n  --refined ... \\\n  --gold ... \\\n  --output batch_001_eval.json \\\n  --markdown \\\n  --stratify label confidence span_length  # \u2190 Add this\n</code></pre></p>"},{"location":"production_workflow/#summary","title":"Summary","text":""},{"location":"production_workflow/#checklist-first-production-batch","title":"Checklist: First Production Batch","text":"<ul> <li> Batch Preparation: 100 tasks stratified by confidence</li> <li> Label Studio Import: Config loaded, tasks imported with pre-annotations</li> <li> Annotation: All 100 tasks completed (2-3 hours)</li> <li> Export &amp; Convert: Gold JSONL with canonical coverage &gt;90%</li> <li> Evaluation: IOU improvement &gt;8%, F1 &gt;0.85, worsened rate &lt;10%</li> <li> Visualization: 6 plots generated (IOU, calibration, correction, P/R/F1, stratified)</li> <li> Iteration: Worsened spans analyzed, prompts/lexicons refined if needed</li> </ul>"},{"location":"production_workflow/#next-steps","title":"Next Steps","text":"<ol> <li>Annotate Batches 2-3 (200 more tasks):</li> <li>Validate consistency (compare F1 across batches)</li> <li> <p>Refine guidelines based on common errors</p> </li> <li> <p>Scale to 500 Tasks:</p> </li> <li>Once metrics stable, increase batch size</li> <li> <p>Target: 1,000 gold annotations for fine-tuning</p> </li> <li> <p>Fine-Tune BioBERT:</p> </li> <li>Train token classification head on gold data</li> <li>Evaluate on held-out test set (20% of gold annotations)</li> <li>Compare to weak/LLM baselines</li> </ol> <p>Questions? See <code>docs/annotation_guide.md</code> for boundary rules, <code>docs/llm_evaluation.md</code> for metric definitions, or open a GitHub issue.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with SpanForge in 5 minutes.</p>"},{"location":"quickstart/#basic-entity-detection","title":"Basic Entity Detection","text":""},{"location":"quickstart/#1-load-lexicons","title":"1. Load Lexicons","text":"<pre><code>from pathlib import Path\nfrom src.weak_label import load_symptom_lexicon, load_product_lexicon\n\n# Load symptom and product lexicons\nsymptom_lex = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\"))\nproduct_lex = load_product_lexicon(Path(\"data/lexicon/products.csv\"))\n\nprint(f\"Loaded {len(symptom_lex)} symptoms, {len(product_lex)} products\")\n</code></pre>"},{"location":"quickstart/#2-detect-entities","title":"2. Detect Entities","text":"<pre><code>from src.weak_label import weak_label\n\ntext = \"Patient developed severe rash and itching after using the moisturizer\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text:20} | {span.label:10} | conf={span.confidence:.2f} | negated={span.negated}\")\n</code></pre> <p>Output: <pre><code>severe rash          | SYMPTOM    | conf=1.00 | negated=False\nitching              | SYMPTOM    | conf=1.00 | negated=False\nmoisturizer          | PRODUCT    | conf=1.00 | negated=False\n</code></pre></p>"},{"location":"quickstart/#3-negation-detection","title":"3. Negation Detection","text":"<pre><code>text = \"No irritation from the face wash\"\nspans = weak_label(text, symptom_lex, product_lex)\n\nfor span in spans:\n    print(f\"{span.text}: negated={span.negated}\")\n</code></pre> <p>Output: <pre><code>irritation: negated=True\nface wash: negated=False\n</code></pre></p>"},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":""},{"location":"quickstart/#process-multiple-documents","title":"Process Multiple Documents","text":"<pre><code>from src.weak_label import weak_label_batch\n\ntexts = [\n    \"Severe headache after using the serum\",\n    \"No side effects, works great\",\n    \"Mild dryness but no redness\"\n]\n\n# Batch process\nall_spans = weak_label_batch(texts, symptom_lex, product_lex)\n\nfor i, (text, spans) in enumerate(zip(texts, all_spans), 1):\n    print(f\"\\nDocument {i}: {text}\")\n    print(f\"Found {len(spans)} entities:\")\n    for span in spans:\n        print(f\"  - {span.text} ({span.label})\")\n</code></pre>"},{"location":"quickstart/#save-to-jsonl","title":"Save to JSONL","text":"<pre><code>from src.weak_label import persist_weak_labels_jsonl\nfrom pathlib import Path\n\n# Persist results\noutput_path = Path(\"data/output/results.jsonl\")\npersist_weak_labels_jsonl(texts, all_spans, output_path)\n\nprint(f\"Saved to {output_path}\")\n</code></pre>"},{"location":"quickstart/#pipeline-inference","title":"Pipeline Inference","text":""},{"location":"quickstart/#full-biobert-weak-labeling","title":"Full BioBERT + Weak Labeling","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\n    \"Itching and redness from the facial cream\",\n    \"No adverse effects noted\"\n]\n\n# Run full pipeline\nresults = simple_inference(texts, persist_path=\"output.jsonl\")\n\nfor result in results:\n    print(f\"Token count: {result['token_count']}\")\n    print(f\"Detected spans: {len(result['weak_spans'])}\")\n\n    for span in result['weak_spans']:\n        print(f\"  {span['text']} ({span['label']})\")\n</code></pre>"},{"location":"quickstart/#configuration","title":"Configuration","text":""},{"location":"quickstart/#custom-parameters","title":"Custom Parameters","text":"<pre><code>from src.config import AppConfig\n\n# Create custom config\nconfig = AppConfig(\n    negation_window=7,  # Extend negation window\n    fuzzy_scorer=\"jaccard\",  # Use Jaccard instead of WRatio\n    device=\"cpu\"  # Force CPU\n)\n\n# Use in weak labeling\nfrom src.weak_label import match_symptoms\n\nspans = match_symptoms(\n    text=\"Patient has severe itching\",\n    lexicon=symptom_lex,\n    negation_window=config.negation_window,\n    scorer=config.fuzzy_scorer\n)\n</code></pre>"},{"location":"quickstart/#working-with-spans","title":"Working with Spans","text":""},{"location":"quickstart/#filter-by-confidence","title":"Filter by Confidence","text":"<pre><code># Get high-confidence spans only\nhigh_conf_spans = [s for s in spans if s.confidence &gt;= 0.9]\n\nprint(f\"High confidence: {len(high_conf_spans)}/{len(spans)}\")\n</code></pre>"},{"location":"quickstart/#group-by-label","title":"Group by Label","text":"<pre><code>from collections import defaultdict\n\nby_label = defaultdict(list)\nfor span in spans:\n    by_label[span.label].append(span)\n\nprint(f\"Symptoms: {len(by_label['SYMPTOM'])}\")\nprint(f\"Products: {len(by_label['PRODUCT'])}\")\n</code></pre>"},{"location":"quickstart/#check-for-negated-entities","title":"Check for Negated Entities","text":"<pre><code>negated = [s for s in spans if s.negated]\naffirmed = [s for s in spans if not s.negated]\n\nprint(f\"Affirmed: {len(affirmed)}\")\nprint(f\"Negated: {len(negated)}\")\n</code></pre>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#entity-co-occurrence","title":"Entity Co-occurrence","text":"<pre><code>def find_symptom_product_pairs(text, symptom_lex, product_lex):\n    \"\"\"Find symptoms mentioned with products.\"\"\"\n    spans = weak_label(text, symptom_lex, product_lex)\n\n    symptoms = [s for s in spans if s.label == \"SYMPTOM\" and not s.negated]\n    products = [s for s in spans if s.label == \"PRODUCT\"]\n\n    if symptoms and products:\n        return [(s.text, p.text) for s in symptoms for p in products]\n    return []\n\n# Example\ntext = \"Developed rash and itching from the hydra cream\"\npairs = find_symptom_product_pairs(text, symptom_lex, product_lex)\nprint(f\"Symptom-Product pairs: {pairs}\")\n# Output: [('rash', 'hydra cream'), ('itching', 'hydra cream')]\n</code></pre>"},{"location":"quickstart/#confidence-threshold","title":"Confidence Threshold","text":"<pre><code>def filter_confident_spans(spans, threshold=0.85):\n    \"\"\"Keep only high-confidence, non-negated spans.\"\"\"\n    return [\n        s for s in spans \n        if s.confidence &gt;= threshold and not s.negated\n    ]\n\nconfident = filter_confident_spans(spans, threshold=0.9)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Detailed parameter tuning</li> <li>User Guide: Weak Labeling - Advanced techniques</li> <li>User Guide: Negation - Negation patterns</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"tutorial_labeling/","title":"Tutorial (Labeling)","text":"SpanForge Labeling Tutorial <p>Step-by-step workflow for converting weak labels into high-quality gold adverse event spans using Label Studio.</p>"},{"location":"tutorial_labeling/#labeling-tutorial-label-studio","title":"Labeling Tutorial (Label Studio)","text":"<p>Walk-through for non-technical users to curate SYMPTOM and PRODUCT spans.</p>"},{"location":"tutorial_labeling/#1-environment-setup","title":"1. Environment Setup","text":"<p><pre><code>setx LABEL_STUDIO_DISABLE_TELEMETRY 1   # one-time (optional)\nconda activate NER\nlabel-studio start --no-browser\n</code></pre> Open http://localhost:8080 in a browser.</p>"},{"location":"tutorial_labeling/#2-obtain-api-token","title":"2. Obtain API Token","text":"<ul> <li>Click user avatar \u2192 Account Settings \u2192 Enable legacy API tokens (Org tab if required).</li> <li>Generate token; copy for import scripts.</li> </ul>"},{"location":"tutorial_labeling/#3-create-project","title":"3. Create Project","text":"<p>Use included config file: <pre><code>python scripts/annotation/init_label_studio_project.py --name \"Adverse Event NER\"\n</code></pre> Alternatively create manually in UI, paste XML from <code>data/annotation/config/label_config.xml</code>.</p>"},{"location":"tutorial_labeling/#4-generate-import-weak-tasks","title":"4. Generate &amp; Import Weak Tasks","text":"<p>Weak label JSONL (from notebook or pipeline) lives at <code>data/output/&lt;batch&gt;_weak.jsonl</code>. <pre><code>python scripts/annotation/cli.py import-weak `\n  --weak data/output/workflow_demo_weak.jsonl `\n  --out data/annotation/exports/tasks.json `\n  --push --project-id &lt;PROJECT_ID&gt;\n</code></pre> Tasks appear in the project task list.</p>"},{"location":"tutorial_labeling/#5-annotating","title":"5. Annotating","text":"<p>For each task: 1. Read entire complaint. 2. Highlight SYMPTOM or PRODUCT phrase. 3. Select label from interface. 4. Adjust existing weak suggestions (delete incorrect, expand truncated, add missing). 5. Save/Submit.</p>"},{"location":"tutorial_labeling/#boundary-checklist","title":"Boundary Checklist","text":"<ul> <li>Include severity modifiers (\"severe rash\").</li> <li>Exclude trailing punctuation.</li> <li>Preserve internal spacing/casing.</li> <li>Keep negated symptoms (e.g., \"no irritation\")\u2014negation handled later.</li> </ul>"},{"location":"tutorial_labeling/#keyboard-shortcuts-default","title":"Keyboard Shortcuts (Default)","text":"Action Shortcut Select label Click or numeric index Undo last span Ctrl+Z Redo Ctrl+Y"},{"location":"tutorial_labeling/#6-export-annotations","title":"6. Export Annotations","text":"<p>In project: Export \u2192 JSON. Save to <code>data/annotation/raw/&lt;export_name&gt;.json</code>.</p>"},{"location":"tutorial_labeling/#7-convert-to-gold","title":"7. Convert to Gold","text":"<pre><code>python scripts/annotation/convert_labelstudio.py `\n  --input data/annotation/raw/&lt;export_name&gt;.json `\n  --output data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --source &lt;batch_id&gt; `\n  --annotator &lt;your_name&gt; `\n  --symptom-lexicon data/lexicon/symptoms.csv `\n  --product-lexicon data/lexicon/products.csv\n</code></pre>"},{"location":"tutorial_labeling/#8-quality-report","title":"8. Quality Report","text":"<p><pre><code>python scripts/annotation/cli.py quality `\n  --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --out data/annotation/reports/&lt;batch&gt;_quality.json\n</code></pre> Review conflicts and distributions.</p>"},{"location":"tutorial_labeling/#9-register-batch","title":"9. Register Batch","text":"<pre><code>python scripts/annotation/cli.py register `\n  --batch-id &lt;batch_id&gt; `\n  --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl `\n  --annotators &lt;comma_names&gt; `\n  --revision 1\n</code></pre>"},{"location":"tutorial_labeling/#10-adjudication-if-needed","title":"10. Adjudication (If Needed)","text":"<p>Conflicts listed in quality report can be processed: <pre><code>python scripts/annotation/adjudicate.py --gold data/annotation/exports/&lt;batch&gt;_gold.jsonl --out data/annotation/exports/&lt;batch&gt;_gold_resolved.jsonl\n</code></pre></p>"},{"location":"tutorial_labeling/#troubleshooting","title":"Troubleshooting","text":"Issue Resolution 400 import error Ensure tasks POST body is raw array, not wrapped object. Empty task list Verify project ID &amp; API token validity. Token rejected Legacy tokens may need enabling in Org settings. Wrong character indices Confirm spans copy exact text; use notebook integrity cell."},{"location":"tutorial_labeling/#best-practices","title":"Best Practices","text":"<ul> <li>Annotate daily small batches (\u226450) for consistency.</li> <li>Log notes in registry for threshold changes.</li> <li>Periodically re-run quality reports to monitor drift.</li> </ul>"},{"location":"tutorial_labeling/#next-steps","title":"Next Steps","text":"<p>After several batches: perform consensus, finalize label inventory, generate BIO tags, and begin supervised fine-tuning.</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>All notable changes to SpanForge will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"about/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"about/changelog/#planned","title":"Planned","text":"<ul> <li>Gold standard dataset (500+ annotated samples)</li> <li>Token classification fine-tuning</li> <li>Model evaluation framework</li> <li>Active learning pipeline</li> <li>Model deployment and production monitoring</li> </ul>"},{"location":"about/changelog/#060-2025-11-27","title":"[0.6.0] - 2025-11-27","text":""},{"location":"about/changelog/#changed-phase-6-preparation-pre-release-cleanup","title":"Changed - Phase 6 Preparation (Pre-release Cleanup)","text":"<ul> <li>Modular Package Testing: Comprehensive test suite for <code>weak_labeling</code> package</li> <li>176 new tests across 5 modules (confidence, matchers, validators, negation, labeler)</li> <li>Coverage improvement: 76% \u2192 84% overall (+8 percentage points)</li> <li>Module-specific gains: confidence.py 16%\u2192100%, negation.py 68%\u219290%, validators.py 46%\u219285%</li> <li>104 passing tests (59% pass rate due to API signature mismatches in 72 tests)</li> <li>Zero regressions in original 344 tests (100% backward compatibility)</li> <li>Documentation Consolidation: Reduced active docs from 41 to 25 files (39% reduction)</li> <li>Archived 16 historical files (phase summaries, development reports) to <code>docs/archive/</code></li> <li>Consolidated 4 LLM docs into single <code>llm_integration.md</code> (14.5 KB comprehensive guide)</li> <li>Updated navigation: \"Annotation &amp; Evaluation\" (10 items) \u2192 \"Annotation &amp; LLM\" (4 items)</li> <li>Zero information loss, all content preserved or improved</li> <li>Version Alignment: Fixed submodule versioning inconsistency</li> <li>Removed independent <code>__version__ = \"0.2.0\"</code> from <code>src/weak_labeling/__init__.py</code></li> <li>Unified versioning: All components now reference parent package version (0.6.0)</li> <li>Updated version in 3 locations: <code>src/__init__.py</code>, <code>VERSION</code>, <code>pyproject.toml</code></li> </ul>"},{"location":"about/changelog/#fixed","title":"Fixed","text":"<ul> <li>Resolved version inconsistency between main package (0.5.0) and weak_labeling submodule (0.2.0)</li> <li>Documentation build warnings reduced (archived files no longer in navigation)</li> </ul>"},{"location":"about/changelog/#internal","title":"Internal","text":"<ul> <li>Created <code>docs/archive/</code> for historical development artifacts</li> <li>Test infrastructure enhancements: 520 total tests (448 passing, 86% pass rate)</li> <li>Code formatting applied via isort across codebase</li> </ul>"},{"location":"about/changelog/#050-2025-11-25","title":"[0.5.0] - 2025-11-25","text":""},{"location":"about/changelog/#added-phase-5-annotation-curation-infrastructure","title":"Added - Phase 5: Annotation &amp; Curation Infrastructure","text":"<ul> <li>LLM Refinement System: Multi-provider support for automated label refinement</li> <li>OpenAI (GPT-4, GPT-4o-mini), Azure OpenAI, Anthropic (Claude 3.5 Sonnet)</li> <li>Boundary correction (removes adjectives: \"severe burning\" \u2192 \"burning\")</li> <li>Canonical normalization (maps colloquial terms to medical lexicon)</li> <li>Negation validation with span-level accuracy checks</li> <li>Exponential backoff retry (3 attempts), caching, structured output validation</li> <li>15 tests covering all providers and edge cases</li> <li>Evaluation Harness: Comprehensive metrics for measuring annotation quality</li> <li>10 evaluation functions: IOU, boundary precision, IOU delta, correction rate, calibration curve, stratification (confidence/label/length), precision/recall/F1</li> <li>3-way comparison: weak labels \u2192 LLM refinement \u2192 gold standard</li> <li>Stratified analysis by entity label, confidence bucket, span length</li> <li>27 tests with 100% coverage (overlap, boundary, correction, calibration, P/R/F1)</li> <li>Evaluation Script: Production CLI tool (<code>evaluate_llm_refinement.py</code>)</li> <li>JSON + Markdown report generation</li> <li>Configurable stratification (label, confidence, span_length)</li> <li>Detailed correction breakdown (improved/worsened/unchanged spans)</li> <li>Fixture-based testing with real weak/LLM/gold data</li> <li>Visualization Tools: Publication-quality plots (<code>plot_llm_metrics.py</code>)</li> <li>6 plot types: IOU uplift, calibration curve, correction rate, P/R/F1 comparison, stratified label/confidence analysis</li> <li>Multiple formats (PNG, PDF, SVG, JPG) at 300 DPI</li> <li>Colorblind-safe palette with annotated deltas</li> <li>Optional dependencies (matplotlib, seaborn, numpy) in <code>requirements-viz.txt</code></li> <li>Label Studio Configuration: Production-ready annotation interface</li> <li>Enhanced <code>label_config.xml</code> with hotkeys (s/p), word granularity, colorblind-safe colors (green/blue)</li> <li>Optional negation tracking (commented, ready to enable)</li> <li>Configuration documentation (<code>data/annotation/config/README.md</code>, 100+ lines)</li> <li>API import examples, customization guide, troubleshooting</li> <li>Annotation Tutorial: Interactive Jupyter notebook (<code>AnnotationWalkthrough.ipynb</code>)</li> <li>7 sections: Introduction, data prep, LLM demo, Label Studio setup, 5 practice examples, export/evaluation, common mistakes</li> <li>5 practice examples with correct/incorrect annotations (boundary, negation, anatomy, multi-word, conjunctions)</li> <li>Medical term glossary (itching\u2192pruritus, redness\u2192erythema, dyspnea\u2192shortness of breath)</li> <li>Boundary decision tree flowchart</li> <li>Visualization of weak label quality (confidence distributions, label counts)</li> <li>Production Workflow Guide: Complete evaluation workflow documentation</li> <li><code>docs/production_workflow.md</code> (450+ lines, 8 sections)</li> <li>7-step workflow: batch prep \u2192 Label Studio \u2192 annotation \u2192 export \u2192 conversion \u2192 evaluation \u2192 visualization</li> <li>Data validation scripts (JSONL format, span integrity)</li> <li>Result interpretation guide with 6 target metrics (IOU +8-15%, F1 &gt;0.85, worsened &lt;10%)</li> <li>Iteration strategies (after 100/300 tasks)</li> <li>Troubleshooting (4 common issues: mismatched IDs, calibration, API hangs, missing stratification)</li> <li>Comprehensive Documentation: 2,000+ lines of new documentation</li> <li><code>docs/llm_evaluation.md</code> (520 lines): Evaluation metrics reference</li> <li><code>docs/phase_4.5_summary.md</code> (330 lines): LLM refinement deliverables</li> <li><code>docs/phase_5_plan.md</code> (330 lines): Implementation plan with timeline, costs, ROI</li> <li><code>docs/production_evaluation.md</code> (450 lines): Real-world usage with optimization strategies</li> <li><code>docs/llm_providers.md</code>: Provider comparison and setup guide</li> <li>CLI Integration: Unified annotation workflow commands</li> <li><code>cli.py evaluate-llm</code>: Routes to evaluation script with stratification</li> <li><code>cli.py plot-metrics</code>: Generates visualization suite</li> <li>8 total subcommands (bootstrap, import-weak, quality, adjudicate, register, refine-llm, evaluate-llm, plot-metrics)</li> </ul>"},{"location":"about/changelog/#test-coverage","title":"Test Coverage","text":"<ul> <li>Total Tests: 296 (99.3% passing; 1 flaky performance test)</li> <li>171 core + edge cases + integration tests (from Phase 4)</li> <li>15 LLM agent tests (provider validation, boundary correction, negation, caching)</li> <li>27 evaluation harness tests (metrics, stratification, calibration)</li> <li>Test Fixtures: 3 JSONL files for annotation evaluation</li> <li><code>weak_baseline.jsonl</code>: Original weak labels with confidence scores</li> <li><code>gold_with_llm_refined.jsonl</code>: LLM-refined suggestions + gold spans</li> <li><code>gold_standard.jsonl</code>: Human-annotated ground truth</li> </ul>"},{"location":"about/changelog/#performance-benchmarks-from-test-fixtures","title":"Performance Benchmarks (from test fixtures)","text":"<ul> <li>IOU Improvement: +13.4% (0.882 \u2192 1.000) weak vs LLM</li> <li>Exact Match Rate: 66.7% \u2192 100.0% (LLM boundaries align with gold)</li> <li>Correction Rate: 100% improved (2/2 modified spans)</li> <li>F1 Score: 1.000 (perfect precision/recall on fixtures)</li> <li>Boundary Corrections: \"severe burning sensation\" \u2192 \"burning sensation\" (+18.2% IOU)</li> </ul>"},{"location":"about/changelog/#cost-analysis","title":"Cost Analysis","text":"<ul> <li>LLM Refinement (100 tasks):</li> <li>OpenAI GPT-4: $1.20</li> <li>Azure OpenAI GPT-4: $1.20</li> <li>Anthropic Claude 3.5 Sonnet: $0.16 (10x cheaper)</li> <li>OpenAI GPT-4o-mini: $0.15</li> <li>ROI: 2,186% (GPT-4) to 30,600% (GPT-4o-mini) return on LLM investment</li> <li>Calculation: (F1 improvement \u00d7 annotation cost) / LLM cost</li> <li>Assumes $60-90 annotation labor per 100 tasks</li> </ul>"},{"location":"about/changelog/#changed","title":"Changed","text":"<ul> <li>Updated <code>.github/copilot-instructions.md</code> with Phase 5 progress</li> <li>Enhanced project structure with <code>src/evaluation/</code> module</li> <li>Added <code>requirements-llm.txt</code> (openai, anthropic, tenacity)</li> <li>Added <code>requirements-viz.txt</code> (matplotlib, seaborn, numpy - optional)</li> </ul>"},{"location":"about/changelog/#documentation","title":"Documentation","text":"<ul> <li>\u2705 Phase 5 implementation complete (Options 1 &amp; 2)</li> <li>\u2705 Label Studio configuration with hotkeys and colorblind-safe design</li> <li>\u2705 Interactive tutorial notebook for annotators</li> <li>\u2705 Production workflow guide with CLI examples</li> <li>\u2705 Comprehensive evaluation and visualization tools</li> <li>\u2705 2,000+ lines of new documentation</li> </ul>"},{"location":"about/changelog/#pending-phase-5-continuation","title":"Pending (Phase 5 continuation)","text":"<ul> <li>Batch preparation script (<code>prepare_production_batch.py</code>): Stratified sampling, de-identification, manifest generation</li> <li>Conversion script (<code>convert_labelstudio.py</code>): Label Studio JSON \u2192 gold JSONL, provenance tracking</li> <li>First production batch (100 tasks) with real annotation data</li> </ul>"},{"location":"about/changelog/#040-2025-01-xx","title":"[0.4.0] - 2025-01-XX","text":""},{"location":"about/changelog/#added","title":"Added","text":"<ul> <li>Documentation Infrastructure: Complete MkDocs setup with Material theme</li> <li>Homepage with architecture diagrams, performance metrics, quick start</li> <li>5 API reference pages (config, model, weak_label, pipeline, llm_agent)</li> <li>Installation guide with troubleshooting</li> <li>Quick start tutorial with code examples</li> <li>User guides: weak labeling, negation, pipeline, annotation</li> <li>Development guide: testing infrastructure</li> <li>About pages: roadmap, changelog</li> <li>Docstrings: Comprehensive Google-style docstrings for all core modules</li> <li><code>src/config.py</code>: Module, class, function docstrings with examples</li> <li><code>src/model.py</code>: Detailed function docstrings with parameter/return docs</li> <li><code>src/pipeline.py</code>: Complete pipeline documentation</li> <li><code>src/llm_agent.py</code>: Enhanced class and method documentation</li> <li>Type Hints: Explicit type annotations throughout codebase</li> <li><code>Optional</code>, <code>AutoTokenizer</code>, <code>AutoModel</code>, <code>BatchEncoding</code> types</li> <li>Consistent typing for all function signatures</li> <li>CI/CD: GitHub Actions workflows and pre-commit hooks</li> <li><code>test.yml</code>: 6 configurations (Ubuntu/Windows \u00d7 Python 3.9/3.10/3.11)</li> <li><code>pre-commit.yml</code>: Automated code quality checks</li> <li>Pre-commit hooks for pytest, formatting</li> <li>Configuration: Consolidated pyproject.toml</li> <li>Black, isort, pytest, coverage configurations</li> <li>Tool configurations centralized</li> </ul>"},{"location":"about/changelog/#changed_1","title":"Changed","text":"<ul> <li>Updated README with CI/CD badges and documentation links</li> <li>Enhanced project structure documentation</li> </ul>"},{"location":"about/changelog/#documentation_1","title":"Documentation","text":"<ul> <li>\u2705 MkDocs Material theme with light/dark mode</li> <li>\u2705 Auto-generated API reference using mkdocstrings</li> <li>\u2705 Comprehensive user guides (weak labeling, negation, pipeline, annotation)</li> <li>\u2705 Testing guide with 144-test infrastructure documentation</li> <li>\u2705 Roadmap with 12 phases (4 complete, 8 planned)</li> </ul>"},{"location":"about/changelog/#030-2025-01-15","title":"[0.3.0] - 2025-01-15","text":""},{"location":"about/changelog/#added_1","title":"Added","text":"<ul> <li>Test Infrastructure: Comprehensive test suite with 144 tests</li> <li>16 core functionality tests</li> <li>98 edge case tests (unicode, emoji, negation, boundaries, anatomy, validation)</li> <li>26 integration tests (pipeline, scale, performance)</li> <li>4 curation tests (Label Studio export validation)</li> <li>Test Composition Pattern: Base classes with shared fixtures</li> <li><code>WeakLabelTestBase</code> for common setup</li> <li>Eliminates test code duplication</li> <li>Easy extension for new test categories</li> <li>Edge Case Coverage: Extensive validation</li> <li>Unicode and emoji handling (12 tests)</li> <li>Negation patterns (24 tests)</li> <li>Boundary conditions (18 tests)</li> <li>Anatomy filter (15 tests)</li> <li>Validation and errors (29 tests)</li> <li>Integration Tests: End-to-end validation</li> <li>Pipeline inference (11 tests)</li> <li>Scale and batch processing (9 tests)</li> <li>Performance benchmarks (6 tests)</li> </ul>"},{"location":"about/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Bidirectional Negation: Expanded negation token list</li> <li>Added: \"non\", \"non-\", \"free of\", \"absence of\", \"ruled out\", \"r/o\"</li> <li>Improved backward negation detection</li> <li>Fixed negation window boundary conditions</li> <li>Unicode Handling: Robust emoji and special character support</li> <li>Emoji within text doesn't break span detection</li> <li>Medical symbols (\u2265, \u00b1, \u00b0) handled correctly</li> <li>Accented characters in symptoms preserved</li> <li>Validation Fixes: Improved error handling</li> <li>Empty lexicon handling</li> <li>Empty text handling</li> <li>Confidence score clamping [0, 1]</li> <li>Boundary checks for span offsets</li> </ul>"},{"location":"about/changelog/#performance","title":"Performance","text":"<ul> <li>Established performance benchmarks:</li> <li><code>match_symptoms</code>: ~10ms/text (CPU)</li> <li><code>simple_inference</code>: ~200ms/text (CPU), ~50ms/text (GPU)</li> <li>Batch processing: ~5s for 32 texts (CPU), ~1s (GPU)</li> </ul>"},{"location":"about/changelog/#test-results","title":"Test Results","text":"<ul> <li>144/144 tests passing (100% pass rate)</li> <li>Test coverage: ~87% overall, ~94% core modules</li> </ul>"},{"location":"about/changelog/#020-2024-12-20","title":"[0.2.0] - 2024-12-20","text":""},{"location":"about/changelog/#added_2","title":"Added","text":"<ul> <li>Bidirectional Negation Detection: Forward and backward negation windows</li> <li>Forward: Negation cue \u2192 [window] \u2192 span</li> <li>Backward: Span \u2192 [window] \u2192 negation cue</li> <li>Configurable window size (default: 5 tokens)</li> <li>Last-Token Alignment Filter: Prevents partial-word matches</li> <li>Multi-token fuzzy matches must end at token boundaries</li> <li>Reduces false positives from substring matches</li> <li>Anatomy Singleton Filter: Rejects generic anatomy mentions</li> <li>Single anatomy tokens (skin, eye, face) rejected unless symptom co-occurs</li> <li>List of 30+ anatomy terms</li> <li>Reduces false positives by ~15%</li> <li>Emoji and Unicode Handling: Robust text processing</li> <li>Emoji within text doesn't break tokenization</li> <li>Unicode medical symbols supported</li> <li>Preserves span offsets with multi-byte characters</li> <li>Confidence Scoring: Weighted fuzzy + Jaccard scores</li> <li>Formula: 0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score</li> <li>Clamped to [0, 1] range</li> <li>Well-calibrated for active learning thresholds</li> <li>Expanded Negation Tokens: 22 negation cues</li> <li>Clinical: denies, denied, negative, absent, unremarkable</li> <li>Rule-out: rule out, ruled out, r/o</li> <li>Temporal: no longer, ceased</li> </ul>"},{"location":"about/changelog/#changed_2","title":"Changed","text":"<ul> <li>Negation Window: Default increased from 3 to 5 tokens</li> <li>Better balance of precision and recall</li> <li>Configurable via <code>AppConfig.negation_window</code></li> <li>Fuzzy Threshold: Default remains 88.0 (WRatio)</li> <li>Well-calibrated after evaluation</li> <li>Jaccard Threshold: Default remains 40.0</li> <li>Effective quality gate for fuzzy matches</li> </ul>"},{"location":"about/changelog/#performance_1","title":"Performance","text":"<ul> <li>Negation recall improved ~30% with bidirectional detection</li> <li>False positives reduced ~15% with anatomy filter</li> <li>Emoji handling prevents span breakage in ~5% of texts</li> </ul>"},{"location":"about/changelog/#010-2024-11-15","title":"[0.1.0] - 2024-11-15","text":""},{"location":"about/changelog/#added_3","title":"Added","text":"<ul> <li>Initial Release: Core weak labeling functionality</li> <li>BioBERT Integration: Model and tokenizer loading</li> <li>Default: <code>dmis-lab/biobert-base-cased-v1.1</code></li> <li>Singleton pattern for efficient caching</li> <li>Auto-detects CUDA availability</li> <li>Fuzzy Matching: RapidFuzz WRatio-based entity detection</li> <li>Default threshold: 88.0</li> <li>Handles typos and misspellings</li> <li>N-gram tokenization (1-6 tokens)</li> <li>Jaccard Token-Set Filter: Quality gate for fuzzy matches</li> <li>Default threshold: 40.0</li> <li>Prevents false positives from short common words</li> <li>Forward Negation Detection: Basic negation scope</li> <li>Window: 3 tokens (initial default)</li> <li>Standard negation cues: no, not, none, never, without</li> <li>Lexicons: Initial symptom and product lexicons</li> <li><code>data/lexicon/symptoms.csv</code>: MedDRA-derived symptom terms</li> <li><code>data/lexicon/products.csv</code>: Product names and brands</li> <li>Pipeline: End-to-end inference workflow</li> <li><code>simple_inference()</code> function</li> <li>Optional JSONL export</li> <li>Batch processing support</li> <li>Configuration: Centralized config management</li> <li>Pydantic BaseSettings</li> <li>Environment variable support</li> <li>Device auto-detection (CUDA/CPU)</li> <li>LLM Agent Stub: Experimental refinement pipeline</li> <li>Stub implementation for future LLM integration</li> <li>Data structures for span refinement suggestions</li> </ul>"},{"location":"about/changelog/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>model_name</code>: BioBERT model identifier</li> <li><code>max_seq_len</code>: Maximum sequence length (default: 256)</li> <li><code>device</code>: Computation device (auto-detect)</li> <li><code>seed</code>: Random seed (default: 42)</li> <li><code>negation_window</code>: Negation scope (default: 3)</li> <li><code>fuzzy_scorer</code>: Matching algorithm (default: \"wratio\")</li> </ul>"},{"location":"about/changelog/#dependencies","title":"Dependencies","text":"<ul> <li><code>transformers&gt;=4.30.0</code>: HuggingFace transformers</li> <li><code>torch&gt;=2.0.0</code>: PyTorch</li> <li><code>rapidfuzz&gt;=3.0.0</code>: Fuzzy string matching</li> <li><code>pydantic&gt;=2.0.0</code>: Configuration management</li> <li><code>pandas&gt;=2.0.0</code>: Data processing</li> </ul>"},{"location":"about/changelog/#version-naming","title":"Version Naming","text":"<ul> <li>Major (X.0.0): Breaking changes, major milestones (e.g., supervised model release)</li> <li>Minor (0.X.0): New features, enhancements (e.g., new filters, annotation tools)</li> <li>Patch (0.0.X): Bug fixes, documentation updates</li> </ul>"},{"location":"about/changelog/#upcoming-releases","title":"Upcoming Releases","text":""},{"location":"about/changelog/#v050-planned-q1-2025","title":"v0.5.0 (Planned: Q1 2025)","text":"<ul> <li>Label Studio annotation workflow</li> <li>Weak label export/import scripts</li> <li>Consensus and adjudication tools</li> <li>Quality assurance metrics (inter-annotator agreement)</li> <li>Annotation tutorial and guidelines</li> </ul>"},{"location":"about/changelog/#v100-planned-q2-2025","title":"v1.0.0 (Planned: Q2 2025)","text":"<ul> <li>Fine-tuned BioBERT token classification model</li> <li>Gold standard dataset (500+ annotations)</li> <li>Model evaluation framework</li> <li>Training and inference scripts</li> <li>Production-ready NER pipeline</li> </ul>"},{"location":"about/changelog/#links","title":"Links","text":"<ul> <li>Repository: GitHub</li> <li>Documentation: MkDocs Site</li> <li>Issues: Issue Tracker</li> <li>Roadmap: Project Roadmap</li> </ul> <p>Keep this changelog updated with each release or significant change.</p>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#project-license","title":"Project License","text":"<p>SpanForge is released under the MIT License.</p>"},{"location":"about/license/#mit-license","title":"MIT License","text":"<pre><code>MIT License\n\nCopyright (c) 2024-2025 SpanForge Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"about/license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>SpanForge depends on several third-party libraries, each with their own licenses:</p>"},{"location":"about/license/#core-dependencies","title":"Core Dependencies","text":""},{"location":"about/license/#pytorch","title":"PyTorch","text":"<ul> <li>License: BSD-3-Clause</li> <li>Copyright: Facebook, Inc. and its affiliates</li> <li>Link: https://github.com/pytorch/pytorch/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#transformers-huggingface","title":"Transformers (HuggingFace)","text":"<ul> <li>License: Apache License 2.0</li> <li>Copyright: The HuggingFace Inc. team</li> <li>Link: https://github.com/huggingface/transformers/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#rapidfuzz","title":"RapidFuzz","text":"<ul> <li>License: MIT License</li> <li>Copyright: Max Bachmann</li> <li>Link: https://github.com/maxbachmann/RapidFuzz/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#pydantic","title":"Pydantic","text":"<ul> <li>License: MIT License</li> <li>Copyright: Samuel Colvin</li> <li>Link: https://github.com/pydantic/pydantic/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#pandas","title":"Pandas","text":"<ul> <li>License: BSD 3-Clause License</li> <li>Copyright: AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team</li> <li>Link: https://github.com/pandas-dev/pandas/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#development-dependencies","title":"Development Dependencies","text":""},{"location":"about/license/#pytest","title":"pytest","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/pytest-dev/pytest/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#black","title":"Black","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/psf/black/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#isort","title":"isort","text":"<ul> <li>License: MIT License</li> <li>Link: https://github.com/PyCQA/isort/blob/main/LICENSE</li> </ul>"},{"location":"about/license/#documentation-dependencies","title":"Documentation Dependencies","text":""},{"location":"about/license/#mkdocs","title":"MkDocs","text":"<ul> <li>License: BSD 2-Clause License</li> <li>Link: https://github.com/mkdocs/mkdocs/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#mkdocs-material","title":"MkDocs Material","text":"<ul> <li>License: MIT License</li> <li>Copyright: Martin Donath</li> <li>Link: https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#mkdocstrings","title":"mkdocstrings","text":"<ul> <li>License: ISC License</li> <li>Link: https://github.com/mkdocstrings/mkdocstrings/blob/master/LICENSE</li> </ul>"},{"location":"about/license/#model-licenses","title":"Model Licenses","text":""},{"location":"about/license/#biobert","title":"BioBERT","text":"<p>SpanForge uses BioBERT (<code>dmis-lab/biobert-base-cased-v1.1</code>) as the default model.</p> <ul> <li>License: Apache License 2.0</li> <li>Authors: DMIS Lab, Korea University</li> <li>Citation:   <pre><code>@article{lee2020biobert,\n  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\n  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},\n  journal={Bioinformatics},\n  volume={36},\n  number={4},\n  pages={1234--1240},\n  year={2020},\n  publisher={Oxford University Press}\n}\n</code></pre></li> <li>Link: https://github.com/dmis-lab/biobert</li> <li>HuggingFace: https://huggingface.co/dmis-lab/biobert-base-cased-v1.1</li> </ul> <p>Note: When using BioBERT in research, please cite the original paper.</p>"},{"location":"about/license/#lexicon-licenses","title":"Lexicon Licenses","text":""},{"location":"about/license/#meddra","title":"MedDRA","text":"<p>The symptom lexicon (<code>data/lexicon/symptoms.csv</code>) is derived from MedDRA (Medical Dictionary for Regulatory Activities).</p> <ul> <li>License: MedDRA\u00ae trademark is registered by the International Federation of Pharmaceutical Manufacturers and Associations (IFPMA) on behalf of the International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH).</li> <li>Usage: MedDRA content is subject to licensing agreements. The derived lexicon in SpanForge uses publicly available information and does not redistribute proprietary MedDRA data.</li> <li>Link: https://www.meddra.org/</li> </ul> <p>Important: If you use MedDRA-derived data in production or research, ensure compliance with MedDRA licensing terms.</p>"},{"location":"about/license/#custom-lexicons","title":"Custom Lexicons","text":"<p>Product lexicon (<code>data/lexicon/products.csv</code>) is compiled from public sources and does not contain proprietary information.</p>"},{"location":"about/license/#data-privacy","title":"Data Privacy","text":""},{"location":"about/license/#user-data","title":"User Data","text":"<ul> <li>SpanForge does NOT collect or transmit user data</li> <li>All processing is local (no external API calls except HuggingFace model downloads)</li> <li>Label Studio telemetry is disabled (via <code>LABEL_STUDIO_DISABLE_TELEMETRY=1</code>)</li> </ul>"},{"location":"about/license/#complaints-data","title":"Complaints Data","text":"<ul> <li>Raw complaint texts are NOT included in the repository</li> <li>Example data is synthetic or de-identified</li> <li>Users must ensure compliance with privacy regulations (HIPAA, GDPR) when processing real data</li> </ul>"},{"location":"about/license/#contributing","title":"Contributing","text":"<p>By contributing to SpanForge, you agree that your contributions will be licensed under the MIT License.</p> <p>See Contributing Guide for details.</p>"},{"location":"about/license/#disclaimer","title":"Disclaimer","text":"<p>NO WARRANTY</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. THE AUTHORS DISCLAIM ALL WARRANTIES, INCLUDING BUT NOT LIMITED TO MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.</p> <p>NO LIABILITY</p> <p>IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY ARISING FROM THE USE OF THIS SOFTWARE.</p> <p>MEDICAL USE</p> <p>THIS SOFTWARE IS FOR RESEARCH PURPOSES ONLY. IT IS NOT INTENDED FOR CLINICAL DIAGNOSIS, TREATMENT, OR REGULATORY DECISION-MAKING. USERS MUST VALIDATE RESULTS INDEPENDENTLY.</p>"},{"location":"about/license/#contact","title":"Contact","text":"<p>For licensing questions or commercial use inquiries, please contact:</p> <ul> <li>GitHub Issues: SpanForge Issues</li> <li>Email: (Add contact email if applicable)</li> </ul> <p>Last updated: January 2025</p>"},{"location":"about/roadmap/","title":"SpanForge Roadmap","text":"<p>Project roadmap and feature planning for biomedical NER pipeline.</p>"},{"location":"about/roadmap/#project-status","title":"Project Status","text":"<p>Current Phase: Documentation &amp; Infrastructure (Phase 4 Complete) \u2705 Test Coverage: 144/144 tests passing (100%) CI/CD: Active GitHub Actions workflows Next Phase: Annotation &amp; Curation (Phase 5)</p>"},{"location":"about/roadmap/#completed-phases","title":"Completed Phases","text":""},{"location":"about/roadmap/#phase-1-bootstrap-lexicon","title":"Phase 1: Bootstrap &amp; Lexicon \u2705","text":"<p>Completed: November 2024</p> <p>Objectives: - Repository setup and project structure - BioBERT model loading - Initial lexicon-based weak labeling - Core functionality implementation</p> <p>Deliverables: - \u2705 <code>src/config.py</code> - Configuration management - \u2705 <code>src/model.py</code> - BioBERT loader - \u2705 <code>src/weak_label.py</code> - Basic fuzzy matching - \u2705 <code>src/pipeline.py</code> - End-to-end inference - \u2705 <code>data/lexicon/symptoms.csv</code> - Symptom lexicon (MedDRA-derived) - \u2705 <code>data/lexicon/products.csv</code> - Product lexicon - \u2705 <code>requirements.txt</code> - Dependencies</p> <p>Key Achievements: - Fuzzy matching with RapidFuzz (WRatio \u226588) - Jaccard token-set filtering (\u226540) - Basic negation detection (forward only) - JSONL persistence</p>"},{"location":"about/roadmap/#phase-2-weak-label-refinement","title":"Phase 2: Weak Label Refinement \u2705","text":"<p>Completed: December 2024</p> <p>Objectives: - Improve weak labeling accuracy - Add advanced filters and heuristics - Expand negation detection</p> <p>Deliverables: - \u2705 Bidirectional negation (forward + backward windows) - \u2705 Last-token alignment filter - \u2705 Anatomy singleton filter - \u2705 Emoji and unicode handling - \u2705 Confidence scoring (0.8\u00d7fuzzy + 0.2\u00d7jaccard) - \u2705 Expanded negation token list (22 cues)</p> <p>Key Achievements: - Negation recall improved by ~30% (backward detection) - Emoji handling prevents span breakage - Anatomy filter reduces false positives by ~15% - Confidence scores well-calibrated for active learning</p>"},{"location":"about/roadmap/#phase-3-test-infrastructure-edge-cases","title":"Phase 3: Test Infrastructure &amp; Edge Cases \u2705","text":"<p>Completed: January 2025 (Commit 8307485)</p> <p>Objectives: - Comprehensive test coverage - Edge case validation - Test composition patterns</p> <p>Deliverables: - \u2705 144 tests (16 core, 98 edge cases, 26 integration, 4 curation) - \u2705 Test base classes with composition - \u2705 Unicode and emoji edge case tests - \u2705 Negation pattern tests (24 tests) - \u2705 Boundary condition tests (18 tests) - \u2705 Anatomy filter tests (15 tests) - \u2705 Validation and error tests (29 tests) - \u2705 Scale and performance tests (15 tests)</p> <p>Key Achievements: - 100% test pass rate - Edge cases documented and validated - Test composition eliminates duplication - Performance benchmarks established</p>"},{"location":"about/roadmap/#phase-4-cicd-integration","title":"Phase 4: CI/CD Integration \u2705","text":"<p>Completed: January 2025 (Commit dbc2ad8)</p> <p>Objectives: - Automated testing on push/PR - Pre-commit hooks - Configuration management - Documentation infrastructure</p> <p>Deliverables: - \u2705 GitHub Actions workflows (test.yml, pre-commit.yml) - \u2705 6 CI configurations (2 OS \u00d7 3 Python versions) - \u2705 Pre-commit hooks (pytest, formatting) - \u2705 pyproject.toml configuration - \u2705 README updates with badges - \u2705 MkDocs infrastructure with Material theme - \u2705 Comprehensive docstrings and type hints</p> <p>Key Achievements: - CI/CD pipeline fully automated - Pre-commit hooks enforce quality - Test matrix covers Python 3.9-3.11 on Ubuntu/Windows - Professional documentation site</p>"},{"location":"about/roadmap/#current-phase","title":"Current Phase","text":""},{"location":"about/roadmap/#phase-5-annotation-curation","title":"Phase 5: Annotation &amp; Curation \ud83d\udea7","text":"<p>Status: Planned (In Progress: Documentation Complete) Target: Q1 2025</p> <p>Objectives: - Integrate Label Studio for human annotation - Build annotation workflow and tooling - Implement provenance tracking - Quality assurance and agreement metrics</p> <p>Planned Deliverables:</p>"},{"location":"about/roadmap/#label-studio-setup","title":"Label Studio Setup","text":"<ul> <li> <code>data/annotation/config/label_config.xml</code> - Label config (SYMPTOM, PRODUCT)</li> <li> <code>scripts/annotation/init_label_studio_project.py</code> - Project bootstrap</li> <li> Telemetry disabled (privacy-safe setup)</li> </ul>"},{"location":"about/roadmap/#importexport-pipeline","title":"Import/Export Pipeline","text":"<ul> <li> <code>scripts/annotation/import_weak_to_labelstudio.py</code> - Weak label import</li> <li> <code>scripts/annotation/convert_labelstudio.py</code> - Export to gold JSONL</li> <li> Consensus/adjudication logic (majority vote, longest span)</li> </ul>"},{"location":"about/roadmap/#quality-assurance","title":"Quality Assurance","text":"<ul> <li> <code>scripts/annotation/quality_report.py</code> - Per-annotator stats, agreement</li> <li> <code>scripts/annotation/adjudicate.py</code> - Conflict resolution</li> <li> Inter-annotator agreement (IOU \u22650.5, Cohen's kappa)</li> </ul>"},{"location":"about/roadmap/#provenance-registry","title":"Provenance &amp; Registry","text":"<ul> <li> <code>scripts/annotation/register_batch.py</code> - Track batches, annotators</li> <li> <code>data/annotation/registry.csv</code> - Batch metadata</li> <li> Conflict collection (<code>data/annotation/conflicts/</code>)</li> </ul>"},{"location":"about/roadmap/#documentation","title":"Documentation","text":"<ul> <li>\u2705 <code>docs/annotation_guide.md</code> - Annotation guidelines (COMPLETE)</li> <li> <code>docs/tutorial_labeling.md</code> - Step-by-step tutorial</li> <li> <code>scripts/AnnotationWalkthrough.ipynb</code> - Interactive tutorial</li> <li> Glossary of symptom synonyms</li> </ul>"},{"location":"about/roadmap/#cli-tooling","title":"CLI Tooling","text":"<ul> <li> <code>scripts/annotation/cli.py</code> - Unified CLI (<code>bootstrap</code>, <code>import-weak</code>, <code>export-convert</code>, <code>quality</code>, <code>adjudicate</code>, <code>register</code>)</li> </ul> <p>Success Criteria: - &lt;5% conflicting overlaps after consensus - \u226590% canonical coverage - Annotator agreement (IOU \u22650.5) &gt;0.75 - Clean gold JSONL passes integrity tests</p> <p>Risks &amp; Mitigations: - Annotator overlap confusion \u2192 Visual examples &amp; highlight guidance - Bias from pre-annotated spans \u2192 Option to hide weak spans - Inconsistent boundaries \u2192 Boundary rules + integrity tests - Privacy concerns \u2192 Local-only data, telemetry disabled</p>"},{"location":"about/roadmap/#upcoming-phases","title":"Upcoming Phases","text":""},{"location":"about/roadmap/#phase-6-gold-standard-assembly","title":"Phase 6: Gold Standard Assembly","text":"<p>Status: Planned Target: Q1-Q2 2025</p> <p>Objectives: - Curate high-quality gold annotations (\u2265500 samples) - Define label schema (<code>labels.json</code>) - Split train/dev/test sets - Establish evaluation baselines</p> <p>Planned Work: - Annotate 500-1000 complaint texts - Consensus annotation (2-3 annotators/task) - Adjudication of conflicts - Dataset splits (70/15/15 train/dev/test) - Baseline weak labeling evaluation (P/R/F1)</p> <p>Deliverables: - <code>data/gold/train.jsonl</code> - Training set - <code>data/gold/dev.jsonl</code> - Development set - <code>data/gold/test.jsonl</code> - Test set (held out) - <code>data/labels.json</code> - Label schema (SYMPTOM, PRODUCT, O) - Evaluation metrics: precision, recall, F1 per label</p>"},{"location":"about/roadmap/#phase-7-token-classification-fine-tune","title":"Phase 7: Token Classification Fine-Tune","text":"<p>Status: Planned Target: Q2 2025</p> <p>Objectives: - Add classification head to BioBERT - Fine-tune on gold annotations - Hyperparameter tuning - Model evaluation and selection</p> <p>Planned Work: - <code>AutoModelForTokenClassification</code> wrapper - Training script with AdamW, learning rate scheduling - Hyperparameter search (LR, batch size, epochs) - Evaluation on dev/test sets - Model checkpointing and versioning</p> <p>Deliverables: - <code>src/trainer.py</code> - Training loop - <code>models/biobert-ner-v1/</code> - Fine-tuned checkpoint - <code>scripts/train.py</code> - Training CLI - <code>scripts/evaluate.py</code> - Evaluation script - Training logs and metrics</p> <p>Expected Metrics: - SYMPTOM: P ~85%, R ~80%, F1 ~82% - PRODUCT: P ~90%, R ~85%, F1 ~87% - Macro F1: ~84%</p>"},{"location":"about/roadmap/#phase-8-domain-adaptation","title":"Phase 8: Domain Adaptation","text":"<p>Status: Planned Target: Q2-Q3 2025</p> <p>Objectives: - Continued MLM pre-training on complaints corpus - Adapt BioBERT to colloquial language - Compare adapted vs. base BioBERT</p> <p>Planned Work: - De-identify complaint corpus (\u226510k texts) - Masked language modeling (MLM) on complaints - 1-3 epochs, batch size 16-32 - Evaluation: perplexity, downstream NER F1</p> <p>Deliverables: - <code>scripts/pretrain_mlm.py</code> - MLM training script - <code>models/biobert-complaints-adapted/</code> - Adapted checkpoint - Perplexity comparison report - NER performance before/after adaptation</p> <p>Expected Gains: - Perplexity reduction: ~10-15% - NER F1 improvement: ~2-4% - Better handling of misspellings, colloquialisms</p>"},{"location":"about/roadmap/#phase-9-baseline-comparison","title":"Phase 9: Baseline Comparison","text":"<p>Status: Planned Target: Q3 2025</p> <p>Objectives: - Train RoBERTa-base for comparison - Benchmark against BioBERT - Analyze trade-offs (biomedical vs. general domain)</p> <p>Planned Work: - Fine-tune <code>roberta-base</code> on same gold data - Compare BioBERT vs. RoBERTa on dev/test - Error analysis (medical terms, colloquialisms)</p> <p>Deliverables: - <code>models/roberta-ner-v1/</code> - RoBERTa checkpoint - Comparison report (P/R/F1, inference speed) - Error analysis notebook</p> <p>Expected Results: - BioBERT: Better on medical terms - RoBERTa: Better on colloquial language - Final choice: Ensemble or BioBERT-adapted</p>"},{"location":"about/roadmap/#phase-10-evaluation-calibration","title":"Phase 10: Evaluation &amp; Calibration","text":"<p>Status: Planned Target: Q3 2025</p> <p>Objectives: - Comprehensive error analysis - Confidence calibration - Threshold tuning for production</p> <p>Planned Work: - Error categorization (false positives, false negatives) - Confidence calibration curves - Threshold optimization (maximize F1 or balance P/R) - Per-label performance analysis</p> <p>Deliverables: - <code>docs/error_analysis.md</code> - Error taxonomy - Calibration plots - Optimal threshold recommendations - Per-label performance breakdown</p>"},{"location":"about/roadmap/#phase-11-educational-docs-expansion","title":"Phase 11: Educational Docs Expansion","text":"<p>Status: Partially Complete Target: Q4 2025</p> <p>Objectives: - Comprehensive user and developer docs - API reference - Tutorials and walkthroughs</p> <p>Planned Work: - \u2705 <code>docs/overview.md</code> - Architecture overview (COMPLETE) - \u2705 <code>docs/annotation_guide.md</code> - Annotation guidelines (COMPLETE) - \u2705 <code>docs/user-guide/weak-labeling.md</code> - Weak labeling guide (COMPLETE) - \u2705 <code>docs/user-guide/negation.md</code> - Negation guide (COMPLETE) - \u2705 <code>docs/user-guide/pipeline.md</code> - Pipeline guide (COMPLETE) - \u2705 <code>docs/development/testing.md</code> - Testing guide (COMPLETE) - [ ] <code>docs/heuristic.md</code> - Heuristic tuning guide - [ ] <code>docs/model_strategy.md</code> - Model selection guide - [ ] <code>docs/deployment.md</code> - Production deployment</p>"},{"location":"about/roadmap/#phase-12-continuous-improvement-active-learning","title":"Phase 12: Continuous Improvement &amp; Active Learning","text":"<p>Status: Planned Target: Ongoing (Q4 2025+)</p> <p>Objectives: - Active learning pipeline - Model monitoring and retraining - Feedback loop for continuous improvement</p> <p>Planned Work: - Active learning: prioritize uncertain samples - Human-in-the-loop annotation for edge cases - Periodic model retraining (monthly/quarterly) - Performance monitoring dashboard</p> <p>Deliverables: - <code>scripts/active_learning.py</code> - Uncertainty sampling - Monitoring dashboard (Streamlit or Grafana) - Retraining automation scripts - Performance drift detection</p>"},{"location":"about/roadmap/#feature-wishlist","title":"Feature Wishlist","text":""},{"location":"about/roadmap/#near-term-2025","title":"Near-Term (2025)","text":"<ul> <li> Multi-language support (Spanish, French)</li> <li> Relation extraction (symptom-product links)</li> <li> Severity classification (mild/moderate/severe)</li> <li> Temporal extraction (onset, duration)</li> </ul>"},{"location":"about/roadmap/#long-term-2026","title":"Long-Term (2026+)","text":"<ul> <li> Real-time inference API (FastAPI + Docker)</li> <li> Web-based annotation interface (custom UI)</li> <li> Integration with FAERS database</li> <li> Ensemble models (BioBERT + RoBERTa + ClinicalBERT)</li> <li> Zero-shot entity recognition (GPT-4 integration)</li> </ul>"},{"location":"about/roadmap/#contribution-opportunities","title":"Contribution Opportunities","text":"<p>Looking for contributors in:</p> <ol> <li>Annotation - Help curate gold standard dataset</li> <li>Documentation - Expand tutorials and examples</li> <li>Testing - Add edge cases and integration tests</li> <li>Feature Development - Implement roadmap items</li> <li>Research - Experiment with new models/techniques</li> </ol> <p>See Contributing Guide for details.</p>"},{"location":"about/roadmap/#version-history","title":"Version History","text":"Version Date Phase Highlights v0.1.0 Nov 2024 Phase 1 Initial release, weak labeling v0.2.0 Dec 2024 Phase 2 Bidirectional negation, filters v0.3.0 Jan 2025 Phase 3 144 tests, 100% pass rate v0.4.0 Jan 2025 Phase 4 CI/CD, MkDocs, docstrings v0.5.0 Q1 2025 Phase 5 Label Studio, annotation (planned) v1.0.0 Q2 2025 Phase 7 Fine-tuned NER model (planned)"},{"location":"about/roadmap/#contact-feedback","title":"Contact &amp; Feedback","text":"<p>Questions or suggestions? Open an issue on GitHub: SpanForge Issues</p> <p>Want to contribute? See Contributing Guide</p> <p>Last updated: January 2025</p>"},{"location":"api/config/","title":"Configuration API","text":"<p>Configuration management for SpanForge using Pydantic BaseSettings.</p>"},{"location":"api/config/#src.config","title":"src.config","text":"<p>Configuration management for SpanForge.</p> <p>This module provides centralized configuration using Pydantic BaseSettings, allowing both default values and environment variable overrides.</p>"},{"location":"api/config/#src.config.AppConfig","title":"AppConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Application configuration with defaults and environment override support.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>HuggingFace model identifier for BioBERT base model</p> <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length for tokenization</p> <code>device</code> <code>str</code> <p>Computation device ('cuda' or 'cpu'), auto-detected if available</p> <code>seed</code> <code>int</code> <p>Random seed for reproducibility across runs</p> <code>negation_window</code> <code>int</code> <p>Number of tokens after negation cue to mark as negated</p> <code>fuzzy_scorer</code> <code>str</code> <p>Fuzzy matching algorithm ('wratio' or 'jaccard')</p> <code>llm_enabled</code> <code>bool</code> <p>Enable experimental LLM refinement pipeline</p> <code>llm_provider</code> <code>str</code> <p>LLM provider name ('stub', 'openai', 'azure', 'anthropic')</p> <code>llm_model</code> <code>str</code> <p>LLM model identifier</p> <code>llm_temperature</code> <code>float</code> <p>Temperature for LLM sampling (0.0-2.0, lower = more deterministic)</p> <code>llm_min_confidence</code> <code>float</code> <p>Minimum confidence threshold for LLM suggestions</p> <code>llm_cache_path</code> <code>str</code> <p>Path to LLM response cache file</p> <code>llm_prompt_version</code> <code>str</code> <p>Version identifier for prompt templates</p> Source code in <code>src\\config.py</code> <pre><code>class AppConfig(BaseSettings):\n    \"\"\"Application configuration with defaults and environment override support.\n\n    Attributes:\n        model_name: HuggingFace model identifier for BioBERT base model\n        max_seq_len: Maximum sequence length for tokenization\n        device: Computation device ('cuda' or 'cpu'), auto-detected if available\n        seed: Random seed for reproducibility across runs\n        negation_window: Number of tokens after negation cue to mark as negated\n        fuzzy_scorer: Fuzzy matching algorithm ('wratio' or 'jaccard')\n        llm_enabled: Enable experimental LLM refinement pipeline\n        llm_provider: LLM provider name ('stub', 'openai', 'azure', 'anthropic')\n        llm_model: LLM model identifier\n        llm_temperature: Temperature for LLM sampling (0.0-2.0, lower = more deterministic)\n        llm_min_confidence: Minimum confidence threshold for LLM suggestions\n        llm_cache_path: Path to LLM response cache file\n        llm_prompt_version: Version identifier for prompt templates\n    \"\"\"\n\n    model_name: str = \"dmis-lab/biobert-base-cased-v1.1\"\n    max_seq_len: int = 256\n    device: str = \"cuda\" if (torch and torch.cuda.is_available()) else \"cpu\"\n    seed: int = 42\n    negation_window: int = 5\n    fuzzy_scorer: str = \"wratio\"\n    llm_enabled: bool = False\n    llm_provider: str = \"stub\"\n    llm_model: str = \"gpt-4\"\n    llm_temperature: float = 0.1\n    llm_min_confidence: float = 0.65\n    llm_cache_path: str = \"data/annotation/exports/llm_cache.jsonl\"\n    llm_prompt_version: str = \"v1\"\n</code></pre>"},{"location":"api/config/#src.config.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; AppConfig\n</code></pre> <p>Get application configuration instance.</p> <p>Returns:</p> Type Description <code>AppConfig</code> <p>AppConfig instance with default or environment-overridden values</p> Example <p>config = get_config() print(config.model_name) 'dmis-lab/biobert-base-cased-v1.1'</p> Source code in <code>src\\config.py</code> <pre><code>def get_config() -&gt; AppConfig:\n    \"\"\"Get application configuration instance.\n\n    Returns:\n        AppConfig instance with default or environment-overridden values\n\n    Example:\n        &gt;&gt;&gt; config = get_config()\n        &gt;&gt;&gt; print(config.model_name)\n        'dmis-lab/biobert-base-cased-v1.1'\n    \"\"\"\n    return AppConfig()\n</code></pre>"},{"location":"api/config/#src.config.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int) -&gt; None\n</code></pre> <p>Set random seeds for reproducibility across libraries.</p> <p>Sets seeds for Python random, NumPy, and PyTorch (CPU and CUDA).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Integer seed value for all random number generators</p> required Example <p>set_seed(42)</p> Source code in <code>src\\config.py</code> <pre><code>def set_seed(seed: int) -&gt; None:\n    \"\"\"Set random seeds for reproducibility across libraries.\n\n    Sets seeds for Python random, NumPy, and PyTorch (CPU and CUDA).\n\n    Args:\n        seed: Integer seed value for all random number generators\n\n    Example:\n        &gt;&gt;&gt; set_seed(42)\n        &gt;&gt;&gt; # All subsequent random operations will be reproducible\n    \"\"\"\n    import random\n\n    import numpy as np\n\n    random.seed(seed)\n    np.random.seed(seed)\n    if torch is not None:\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/config/#src.config.set_seed--all-subsequent-random-operations-will-be-reproducible","title":"All subsequent random operations will be reproducible","text":""},{"location":"api/llm_agent/","title":"LLM Agent API","text":"<p>LLM-based span refinement and enhancement (experimental).</p>"},{"location":"api/llm_agent/#src.llm_agent","title":"src.llm_agent","text":"<p>LLM agent for span refinement and enhancement.</p> <p>Provides LLM-based entity span refinement with support for multiple providers. Supports OpenAI, Azure OpenAI, and Anthropic APIs with caching and rate limiting.</p>"},{"location":"api/llm_agent/#src.llm_agent.LLMSuggestion","title":"LLMSuggestion  <code>dataclass</code>","text":"<p>LLM-generated span suggestion with confidence and reasoning.</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Character start position of suggested span</p> <code>end</code> <code>int</code> <p>Character end position of suggested span</p> <code>label</code> <code>str</code> <p>Entity label (SYMPTOM or PRODUCT)</p> <code>negated</code> <code>Optional[bool]</code> <p>Whether the entity is negated (optional)</p> <code>canonical</code> <code>Optional[str]</code> <p>Canonical form of the entity (optional)</p> <code>confidence_reason</code> <code>Optional[str]</code> <p>Textual explanation for confidence score (optional)</p> <code>llm_confidence</code> <code>Optional[float]</code> <p>Confidence score from LLM (optional)</p> Source code in <code>src\\llm_agent.py</code> <pre><code>@dataclass\nclass LLMSuggestion:\n    \"\"\"LLM-generated span suggestion with confidence and reasoning.\n\n    Attributes:\n        start: Character start position of suggested span\n        end: Character end position of suggested span\n        label: Entity label (SYMPTOM or PRODUCT)\n        negated: Whether the entity is negated (optional)\n        canonical: Canonical form of the entity (optional)\n        confidence_reason: Textual explanation for confidence score (optional)\n        llm_confidence: Confidence score from LLM (optional)\n    \"\"\"\n\n    start: int\n    end: int\n    label: str\n    negated: Optional[bool] = None\n    canonical: Optional[str] = None\n    confidence_reason: Optional[str] = None\n    llm_confidence: Optional[float] = None\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent","title":"LLMAgent","text":"<p>LLM agent for entity span refinement with multi-provider support.</p> <p>Provides interface for LLM-based span suggestion and refinement. Supports OpenAI, Azure OpenAI, Anthropic, and stub mode.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>str</code> <p>LLM provider name ('stub', 'openai', 'azure', 'anthropic')</p> <code>model</code> <code>str</code> <p>Model identifier for the LLM</p> <code>min_conf</code> <code>float</code> <p>Minimum confidence threshold for accepting suggestions</p> <code>temperature</code> <code>float</code> <p>Temperature for LLM sampling (0.0-2.0, lower = more deterministic)</p> <code>cache_path</code> <code>Path</code> <p>Path to JSONL cache file for LLM responses</p> <code>_client</code> <code>Any</code> <p>Lazily initialized API client</p> Environment Variables <p>OPENAI_API_KEY: API key for OpenAI provider AZURE_OPENAI_API_KEY: API key for Azure provider AZURE_OPENAI_ENDPOINT: Endpoint URL for Azure provider ANTHROPIC_API_KEY: API key for Anthropic provider</p> Example Source code in <code>src\\llm_agent.py</code> <pre><code>class LLMAgent:\n    \"\"\"LLM agent for entity span refinement with multi-provider support.\n\n    Provides interface for LLM-based span suggestion and refinement.\n    Supports OpenAI, Azure OpenAI, Anthropic, and stub mode.\n\n    Attributes:\n        provider: LLM provider name ('stub', 'openai', 'azure', 'anthropic')\n        model: Model identifier for the LLM\n        min_conf: Minimum confidence threshold for accepting suggestions\n        temperature: Temperature for LLM sampling (0.0-2.0, lower = more deterministic)\n        cache_path: Path to JSONL cache file for LLM responses\n        _client: Lazily initialized API client\n\n    Environment Variables:\n        OPENAI_API_KEY: API key for OpenAI provider\n        AZURE_OPENAI_API_KEY: API key for Azure provider\n        AZURE_OPENAI_ENDPOINT: Endpoint URL for Azure provider\n        ANTHROPIC_API_KEY: API key for Anthropic provider\n\n    Example:\n        &gt;&gt;&gt; # Stub mode (no API calls)\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; suggestions = agent.suggest(template, text, spans, knowledge)\n\n        &gt;&gt;&gt; # OpenAI mode (requires OPENAI_API_KEY)\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ['OPENAI_API_KEY'] = 'sk-...'\n        &gt;&gt;&gt; config = AppConfig(llm_provider='openai', llm_model='gpt-4-turbo')\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; suggestions = agent.suggest(template, text, spans, knowledge)\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize LLM agent with configuration.\"\"\"\n        cfg = get_config()\n        self.provider: str = cfg.llm_provider\n        self.model: str = cfg.llm_model\n        self.min_conf: float = cfg.llm_min_confidence\n        self.temperature: float = cfg.llm_temperature\n        self.cache_path: Path = Path(cfg.llm_cache_path)\n        self._client: Any = None\n        self._cache: Dict[str, str] = {}\n        self._load_cache()\n\n    def _load_cache(self) -&gt; None:\n        \"\"\"Load cached LLM responses from disk.\"\"\"\n        if not self.cache_path.exists():\n            return\n        try:\n            with open(self.cache_path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    entry = json.loads(line.strip())\n                    prompt_hash = str(hash(entry.get(\"prompt\", \"\")))\n                    self._cache[prompt_hash] = entry.get(\"response\", \"{}\")\n        except Exception:\n            pass\n\n    def _save_to_cache(self, prompt: str, response: str) -&gt; None:\n        \"\"\"Save LLM response to cache file.\n\n        Args:\n            prompt: The prompt sent to LLM\n            response: The response received from LLM\n        \"\"\"\n        prompt_hash = str(hash(prompt))\n        self._cache[prompt_hash] = response\n\n        # Append to cache file\n        try:\n            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(self.cache_path, \"a\", encoding=\"utf-8\") as f:\n                entry = {\n                    \"timestamp\": time.time(),\n                    \"provider\": self.provider,\n                    \"model\": self.model,\n                    \"prompt\": prompt,\n                    \"response\": response,\n                }\n                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n        except Exception:\n            pass\n\n    def _get_client(self) -&gt; Any:\n        \"\"\"Lazily initialize and return API client.\n\n        Returns:\n            Initialized API client for the configured provider\n\n        Raises:\n            ImportError: If required SDK not installed\n            ValueError: If API credentials not found or invalid provider\n        \"\"\"\n        if self._client is not None:\n            return self._client\n\n        if self.provider == \"stub\":\n            self._client = None\n            return None\n\n        elif self.provider == \"openai\":\n            try:\n                import openai\n\n                api_key = os.getenv(\"OPENAI_API_KEY\")\n                if not api_key:\n                    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n                self._client = openai.OpenAI(api_key=api_key)\n                return self._client\n            except ImportError:\n                raise ImportError(\"openai package not installed. Run: pip install openai\")\n\n        elif self.provider == \"azure\":\n            try:\n                import openai\n\n                api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n                endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n                if not api_key or not endpoint:\n                    raise ValueError(\"AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT required\")\n                self._client = openai.AzureOpenAI(\n                    api_key=api_key, azure_endpoint=endpoint, api_version=\"2024-02-15-preview\"\n                )\n                return self._client\n            except ImportError:\n                raise ImportError(\"openai package not installed. Run: pip install openai\")\n\n        elif self.provider == \"anthropic\":\n            try:\n                import anthropic\n\n                api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n                if not api_key:\n                    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set\")\n                self._client = anthropic.Anthropic(api_key=api_key)\n                return self._client\n            except ImportError:\n                raise ImportError(\"anthropic package not installed. Run: pip install anthropic\")\n\n        else:\n            raise ValueError(f\"Unknown LLM provider: {self.provider}\")\n\n    def format_prompt(\n        self, template: str, text: str, spans: List[Dict[str, Any]], knowledge: Dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Format prompt template with text, spans, and knowledge.\n\n        Args:\n            template: Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders\n            text: Source text being analyzed\n            spans: List of candidate spans to refine\n            knowledge: Additional domain knowledge dictionary\n\n        Returns:\n            Formatted prompt string ready for LLM\n        \"\"\"\n        candidates = [\n            f\"{s['text']} [{s['start']},{s['end']}] {s['label']} conf={s.get('confidence',1):.2f}\"\n            for s in spans\n        ]\n        return (\n            template.replace(\"{{text}}\", text)\n            .replace(\"{{candidates}}\", \"\\n\".join(candidates))\n            .replace(\"{{knowledge}}\", json.dumps(knowledge, ensure_ascii=False))\n        )\n\n    def _call_openai_api(self, client: Any, prompt: str) -&gt; str:\n        \"\"\"Call OpenAI/Azure API with retry logic.\n\n        Args:\n            client: OpenAI client instance\n            prompt: Prompt to send\n\n        Returns:\n            Response content as string\n\n        Raises:\n            Exception: If API call fails after retries\n        \"\"\"\n        if TENACITY_AVAILABLE:\n            # Define retry decorator dynamically\n            @retry(\n                stop=stop_after_attempt(3),\n                wait=wait_exponential(multiplier=1, min=2, max=10),\n                retry=retry_if_exception_type((Exception,)),\n            )\n            def _api_call():\n                completion = client.chat.completions.create(\n                    model=self.model,\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a medical NER expert. Analyze text and suggest entity spans in JSON format.\",\n                        },\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    temperature=self.temperature,\n                    response_format={\"type\": \"json_object\"},\n                )\n                return completion.choices[0].message.content or \"{}\"\n\n            return _api_call()\n        else:\n            # No retry if tenacity not available\n            completion = client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a medical NER expert. Analyze text and suggest entity spans in JSON format.\",\n                    },\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                temperature=self.temperature,\n                response_format={\"type\": \"json_object\"},\n            )\n            return completion.choices[0].message.content or \"{}\"\n\n    def _call_anthropic_api(self, client: Any, prompt: str) -&gt; str:\n        \"\"\"Call Anthropic API with retry logic.\n\n        Args:\n            client: Anthropic client instance\n            prompt: Prompt to send\n\n        Returns:\n            Response content as string\n\n        Raises:\n            Exception: If API call fails after retries\n        \"\"\"\n        if TENACITY_AVAILABLE:\n\n            @retry(\n                stop=stop_after_attempt(3),\n                wait=wait_exponential(multiplier=1, min=2, max=10),\n                retry=retry_if_exception_type((Exception,)),\n            )\n            def _api_call():\n                message = client.messages.create(\n                    model=self.model,\n                    max_tokens=2048,\n                    temperature=self.temperature,\n                    system=\"You are a medical NER expert. Analyze text and suggest entity spans in JSON format.\",\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                )\n                return message.content[0].text\n\n            return _api_call()\n        else:\n            message = client.messages.create(\n                model=self.model,\n                max_tokens=2048,\n                temperature=self.temperature,\n                system=\"You are a medical NER expert. Analyze text and suggest entity spans in JSON format.\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n            )\n            return message.content[0].text\n\n    def call(self, prompt: str) -&gt; str:\n        \"\"\"Call LLM with formatted prompt.\n\n        Supports caching to avoid redundant API calls. Checks cache first,\n        then calls appropriate provider API if not cached.\n\n        Args:\n            prompt: Formatted prompt string\n\n        Returns:\n            JSON string with LLM response\n\n        Raises:\n            ImportError: If required SDK not installed\n            ValueError: If API credentials missing\n            Exception: If API call fails\n\n        Note:\n            - Stub mode returns empty suggestions for deterministic testing\n            - All providers return JSON with {\"spans\": [...], \"notes\": \"...\"}\n            - Responses are cached to disk for reproducibility\n        \"\"\"\n        # Check cache first\n        prompt_hash = str(hash(prompt))\n        if prompt_hash in self._cache:\n            return self._cache[prompt_hash]\n\n        # Stub mode: return empty JSON (no API call)\n        if self.provider == \"stub\":\n            response = json.dumps({\"spans\": [], \"notes\": \"stub\"})\n            return response\n\n        # Get API client\n        client = self._get_client()\n\n        try:\n            # OpenAI / Azure OpenAI\n            if self.provider in [\"openai\", \"azure\"]:\n                response = self._call_openai_api(client, prompt)\n\n            # Anthropic\n            elif self.provider == \"anthropic\":\n                response = self._call_anthropic_api(client, prompt) if message.content else \"{}\"\n\n            else:\n                response = json.dumps({\"spans\": [], \"notes\": \"unknown_provider\"})\n\n            # Save to cache\n            self._save_to_cache(prompt, response)\n            return response\n\n        except Exception as e:\n            # Return error response instead of raising\n            error_response = json.dumps({\"spans\": [], \"notes\": f\"api_error: {str(e)[:100]}\"})\n            return error_response\n\n    def parse(self, response: str) -&gt; Dict[str, Any]:\n        \"\"\"Parse LLM response JSON string.\n\n        Args:\n            response: JSON string from LLM\n\n        Returns:\n            Parsed dictionary with 'spans' list and optional 'notes'\n        \"\"\"\n        try:\n            data = json.loads(response)\n            if \"spans\" not in data:\n                data[\"spans\"] = []\n            return data\n        except Exception:\n            return {\"spans\": [], \"notes\": \"parse_error\"}\n\n    def suggest(\n        self, template: str, text: str, spans: List[Dict[str, Any]], knowledge: Dict[str, Any]\n    ) -&gt; List[LLMSuggestion]:\n        \"\"\"Generate span suggestions using LLM.\n\n        Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n        Args:\n            template: Prompt template string\n            text: Source text being analyzed\n            spans: List of candidate span dictionaries\n            knowledge: Domain knowledge dictionary\n\n        Returns:\n            List of LLMSuggestion objects meeting confidence threshold\n\n        Example:\n            &gt;&gt;&gt; agent = LLMAgent()\n            &gt;&gt;&gt; template = \"Analyze: {{text}}\\nCandidates: {{candidates}}\"\n            &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})\n            &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode\n            0\n        \"\"\"\n        prompt = self.format_prompt(template, text, spans, knowledge)\n        raw = self.call(prompt)\n        parsed = self.parse(raw)\n        suggestions: List[LLMSuggestion] = []\n        for s in parsed.get(\"spans\", []):\n            try:\n                ls = LLMSuggestion(\n                    start=int(s[\"start\"]),\n                    end=int(s[\"end\"]),\n                    label=str(s[\"label\"]),\n                    negated=bool(s.get(\"negated\", False)),\n                    canonical=s.get(\"canonical\"),\n                    confidence_reason=s.get(\"confidence_reason\"),\n                    llm_confidence=float(s.get(\"llm_confidence\", self.min_conf)),\n                )\n                if ls.llm_confidence &gt;= self.min_conf:\n                    suggestions.append(ls)\n            except Exception:\n                continue\n        return suggestions\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent--stub-mode-no-api-calls","title":"Stub mode (no API calls)","text":"<p>agent = LLMAgent() suggestions = agent.suggest(template, text, spans, knowledge)</p>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent--openai-mode-requires-openai_api_key","title":"OpenAI mode (requires OPENAI_API_KEY)","text":"<p>import os os.environ['OPENAI_API_KEY'] = 'sk-...' config = AppConfig(llm_provider='openai', llm_model='gpt-4-turbo') agent = LLMAgent() suggestions = agent.suggest(template, text, spans, knowledge)</p>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize LLM agent with configuration.</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize LLM agent with configuration.\"\"\"\n    cfg = get_config()\n    self.provider: str = cfg.llm_provider\n    self.model: str = cfg.llm_model\n    self.min_conf: float = cfg.llm_min_confidence\n    self.temperature: float = cfg.llm_temperature\n    self.cache_path: Path = Path(cfg.llm_cache_path)\n    self._client: Any = None\n    self._cache: Dict[str, str] = {}\n    self._load_cache()\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.format_prompt","title":"format_prompt","text":"<pre><code>format_prompt(\n    template: str,\n    text: str,\n    spans: List[Dict[str, Any]],\n    knowledge: Dict[str, Any],\n) -&gt; str\n</code></pre> <p>Format prompt template with text, spans, and knowledge.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders</p> required <code>text</code> <code>str</code> <p>Source text being analyzed</p> required <code>spans</code> <code>List[Dict[str, Any]]</code> <p>List of candidate spans to refine</p> required <code>knowledge</code> <code>Dict[str, Any]</code> <p>Additional domain knowledge dictionary</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string ready for LLM</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def format_prompt(\n    self, template: str, text: str, spans: List[Dict[str, Any]], knowledge: Dict[str, Any]\n) -&gt; str:\n    \"\"\"Format prompt template with text, spans, and knowledge.\n\n    Args:\n        template: Prompt template with {{text}}, {{candidates}}, {{knowledge}} placeholders\n        text: Source text being analyzed\n        spans: List of candidate spans to refine\n        knowledge: Additional domain knowledge dictionary\n\n    Returns:\n        Formatted prompt string ready for LLM\n    \"\"\"\n    candidates = [\n        f\"{s['text']} [{s['start']},{s['end']}] {s['label']} conf={s.get('confidence',1):.2f}\"\n        for s in spans\n    ]\n    return (\n        template.replace(\"{{text}}\", text)\n        .replace(\"{{candidates}}\", \"\\n\".join(candidates))\n        .replace(\"{{knowledge}}\", json.dumps(knowledge, ensure_ascii=False))\n    )\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.call","title":"call","text":"<pre><code>call(prompt: str) -&gt; str\n</code></pre> <p>Call LLM with formatted prompt.</p> <p>Supports caching to avoid redundant API calls. Checks cache first, then calls appropriate provider API if not cached.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Formatted prompt string</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON string with LLM response</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required SDK not installed</p> <code>ValueError</code> <p>If API credentials missing</p> <code>Exception</code> <p>If API call fails</p> Note <ul> <li>Stub mode returns empty suggestions for deterministic testing</li> <li>All providers return JSON with {\"spans\": [...], \"notes\": \"...\"}</li> <li>Responses are cached to disk for reproducibility</li> </ul> Source code in <code>src\\llm_agent.py</code> <pre><code>def call(self, prompt: str) -&gt; str:\n    \"\"\"Call LLM with formatted prompt.\n\n    Supports caching to avoid redundant API calls. Checks cache first,\n    then calls appropriate provider API if not cached.\n\n    Args:\n        prompt: Formatted prompt string\n\n    Returns:\n        JSON string with LLM response\n\n    Raises:\n        ImportError: If required SDK not installed\n        ValueError: If API credentials missing\n        Exception: If API call fails\n\n    Note:\n        - Stub mode returns empty suggestions for deterministic testing\n        - All providers return JSON with {\"spans\": [...], \"notes\": \"...\"}\n        - Responses are cached to disk for reproducibility\n    \"\"\"\n    # Check cache first\n    prompt_hash = str(hash(prompt))\n    if prompt_hash in self._cache:\n        return self._cache[prompt_hash]\n\n    # Stub mode: return empty JSON (no API call)\n    if self.provider == \"stub\":\n        response = json.dumps({\"spans\": [], \"notes\": \"stub\"})\n        return response\n\n    # Get API client\n    client = self._get_client()\n\n    try:\n        # OpenAI / Azure OpenAI\n        if self.provider in [\"openai\", \"azure\"]:\n            response = self._call_openai_api(client, prompt)\n\n        # Anthropic\n        elif self.provider == \"anthropic\":\n            response = self._call_anthropic_api(client, prompt) if message.content else \"{}\"\n\n        else:\n            response = json.dumps({\"spans\": [], \"notes\": \"unknown_provider\"})\n\n        # Save to cache\n        self._save_to_cache(prompt, response)\n        return response\n\n    except Exception as e:\n        # Return error response instead of raising\n        error_response = json.dumps({\"spans\": [], \"notes\": f\"api_error: {str(e)[:100]}\"})\n        return error_response\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; Dict[str, Any]\n</code></pre> <p>Parse LLM response JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>JSON string from LLM</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed dictionary with 'spans' list and optional 'notes'</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def parse(self, response: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse LLM response JSON string.\n\n    Args:\n        response: JSON string from LLM\n\n    Returns:\n        Parsed dictionary with 'spans' list and optional 'notes'\n    \"\"\"\n    try:\n        data = json.loads(response)\n        if \"spans\" not in data:\n            data[\"spans\"] = []\n        return data\n    except Exception:\n        return {\"spans\": [], \"notes\": \"parse_error\"}\n</code></pre>"},{"location":"api/llm_agent/#src.llm_agent.LLMAgent.suggest","title":"suggest","text":"<pre><code>suggest(\n    template: str,\n    text: str,\n    spans: List[Dict[str, Any]],\n    knowledge: Dict[str, Any],\n) -&gt; List[LLMSuggestion]\n</code></pre> <p>Generate span suggestions using LLM.</p> <pre><code>    Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n    Args:\n        template: Prompt template string\n        text: Source text being analyzed\n        spans: List of candidate span dictionaries\n        knowledge: Domain knowledge dictionary\n\n    Returns:\n        List of LLMSuggestion objects meeting confidence threshold\n\n    Example:\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; template = \"Analyze: {{text}}\n</code></pre> <p>Candidates: {{candidates}}\"             &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})             &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode             0</p> Source code in <code>src\\llm_agent.py</code> <pre><code>def suggest(\n    self, template: str, text: str, spans: List[Dict[str, Any]], knowledge: Dict[str, Any]\n) -&gt; List[LLMSuggestion]:\n    \"\"\"Generate span suggestions using LLM.\n\n    Complete workflow: format prompt -&gt; call LLM -&gt; parse response -&gt; filter by confidence.\n\n    Args:\n        template: Prompt template string\n        text: Source text being analyzed\n        spans: List of candidate span dictionaries\n        knowledge: Domain knowledge dictionary\n\n    Returns:\n        List of LLMSuggestion objects meeting confidence threshold\n\n    Example:\n        &gt;&gt;&gt; agent = LLMAgent()\n        &gt;&gt;&gt; template = \"Analyze: {{text}}\\nCandidates: {{candidates}}\"\n        &gt;&gt;&gt; suggestions = agent.suggest(template, \"Test\", [], {})\n        &gt;&gt;&gt; print(len(suggestions))  # 0 in stub mode\n        0\n    \"\"\"\n    prompt = self.format_prompt(template, text, spans, knowledge)\n    raw = self.call(prompt)\n    parsed = self.parse(raw)\n    suggestions: List[LLMSuggestion] = []\n    for s in parsed.get(\"spans\", []):\n        try:\n            ls = LLMSuggestion(\n                start=int(s[\"start\"]),\n                end=int(s[\"end\"]),\n                label=str(s[\"label\"]),\n                negated=bool(s.get(\"negated\", False)),\n                canonical=s.get(\"canonical\"),\n                confidence_reason=s.get(\"confidence_reason\"),\n                llm_confidence=float(s.get(\"llm_confidence\", self.min_conf)),\n            )\n            if ls.llm_confidence &gt;= self.min_conf:\n                suggestions.append(ls)\n        except Exception:\n            continue\n    return suggestions\n</code></pre>"},{"location":"api/model/","title":"Model API","text":"<p>BioBERT model loading and text encoding utilities.</p>"},{"location":"api/model/#src.model","title":"src.model","text":"<p>BioBERT model loading and text encoding utilities.</p> <p>Provides cached model and tokenizer loading with GPU support. Uses singleton pattern to avoid reloading models on repeated calls.</p>"},{"location":"api/model/#src.model.get_tokenizer","title":"get_tokenizer","text":"<pre><code>get_tokenizer(config: AppConfig) -&gt; AutoTokenizer\n</code></pre> <p>Get or load BioBERT tokenizer with caching.</p> <p>Uses singleton pattern to cache tokenizer after first load.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AppConfig</code> <p>Application configuration containing model_name</p> required <p>Returns:</p> Type Description <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance for BioBERT</p> Example <p>config = AppConfig() tokenizer = get_tokenizer(config) tokens = tokenizer.tokenize(\"Patient has itching\")</p> Source code in <code>src\\model.py</code> <pre><code>def get_tokenizer(config: AppConfig) -&gt; AutoTokenizer:\n    \"\"\"Get or load BioBERT tokenizer with caching.\n\n    Uses singleton pattern to cache tokenizer after first load.\n\n    Args:\n        config: Application configuration containing model_name\n\n    Returns:\n        PreTrainedTokenizer instance for BioBERT\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; tokenizer = get_tokenizer(config)\n        &gt;&gt;&gt; tokens = tokenizer.tokenize(\"Patient has itching\")\n    \"\"\"\n    global _tokenizer\n    if _tokenizer is None:\n        # TODO(security): Pin model revision before production (Bandit B615)\n        # Example: AutoTokenizer.from_pretrained(config.model_name, revision=\"3e4b9c6\")\n        _tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    return _tokenizer\n</code></pre>"},{"location":"api/model/#src.model.get_model","title":"get_model","text":"<pre><code>get_model(config: AppConfig) -&gt; AutoModel\n</code></pre> <p>Get or load BioBERT model with caching and device placement.</p> <p>Uses singleton pattern to cache model after first load. Automatically moves model to configured device (CPU/GPU). Sets model to evaluation mode.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AppConfig</code> <p>Application configuration containing model_name and device</p> required <p>Returns:</p> Type Description <code>AutoModel</code> <p>PreTrainedModel instance for BioBERT in eval mode</p> Example <p>config = AppConfig() model = get_model(config)</p> Source code in <code>src\\model.py</code> <pre><code>def get_model(config: AppConfig) -&gt; AutoModel:\n    \"\"\"Get or load BioBERT model with caching and device placement.\n\n    Uses singleton pattern to cache model after first load.\n    Automatically moves model to configured device (CPU/GPU).\n    Sets model to evaluation mode.\n\n    Args:\n        config: Application configuration containing model_name and device\n\n    Returns:\n        PreTrainedModel instance for BioBERT in eval mode\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; model = get_model(config)\n        &gt;&gt;&gt; # Model is on correct device and in eval mode\n    \"\"\"\n    global _model\n    if _model is None:\n        # TODO(security): Pin model revision before production (Bandit B615)\n        _model = AutoModel.from_pretrained(config.model_name)\n        _model.to(config.device)\n        _model.eval()\n    return _model\n</code></pre>"},{"location":"api/model/#src.model.get_model--model-is-on-correct-device-and-in-eval-mode","title":"Model is on correct device and in eval mode","text":""},{"location":"api/model/#src.model.encode_text","title":"encode_text","text":"<pre><code>encode_text(\n    text: str, tokenizer: AutoTokenizer, max_len: int\n) -&gt; BatchEncoding\n</code></pre> <p>Encode text string to model input format.</p> <p>Tokenizes and encodes text with truncation and tensor conversion.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text string to encode</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance</p> required <code>max_len</code> <code>int</code> <p>Maximum sequence length for truncation</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>BatchEncoding with input_ids, attention_mask, and token_type_ids</p> Example <p>config = AppConfig() tokenizer = get_tokenizer(config) encoding = encode_text(\"Test text\", tokenizer, 256) print(encoding.keys()) dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])</p> Source code in <code>src\\model.py</code> <pre><code>def encode_text(text: str, tokenizer: AutoTokenizer, max_len: int) -&gt; BatchEncoding:\n    \"\"\"Encode text string to model input format.\n\n    Tokenizes and encodes text with truncation and tensor conversion.\n\n    Args:\n        text: Input text string to encode\n        tokenizer: PreTrainedTokenizer instance\n        max_len: Maximum sequence length for truncation\n\n    Returns:\n        BatchEncoding with input_ids, attention_mask, and token_type_ids\n\n    Example:\n        &gt;&gt;&gt; config = AppConfig()\n        &gt;&gt;&gt; tokenizer = get_tokenizer(config)\n        &gt;&gt;&gt; encoding = encode_text(\"Test text\", tokenizer, 256)\n        &gt;&gt;&gt; print(encoding.keys())\n        dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n    \"\"\"\n    return tokenizer(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>End-to-end inference pipeline combining BioBERT and weak labeling.</p>"},{"location":"api/pipeline/#src.pipeline","title":"src.pipeline","text":"<p>End-to-end inference pipeline combining BioBERT and weak labeling.</p> <p>Provides batch processing with model inference, weak labeling, and optional persistence to JSONL format.</p>"},{"location":"api/pipeline/#src.pipeline.tokenize_batch","title":"tokenize_batch","text":"<pre><code>tokenize_batch(\n    texts: List[str], tokenizer: AutoTokenizer, max_len: int\n) -&gt; BatchEncoding\n</code></pre> <p>Tokenize batch of texts for model input.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input text strings</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>PreTrainedTokenizer instance</p> required <code>max_len</code> <code>int</code> <p>Maximum sequence length for truncation</p> required <p>Returns:</p> Type Description <code>BatchEncoding</code> <p>BatchEncoding with padded and truncated sequences</p> Source code in <code>src\\pipeline.py</code> <pre><code>def tokenize_batch(texts: List[str], tokenizer: AutoTokenizer, max_len: int) -&gt; BatchEncoding:\n    \"\"\"Tokenize batch of texts for model input.\n\n    Args:\n        texts: List of input text strings\n        tokenizer: PreTrainedTokenizer instance\n        max_len: Maximum sequence length for truncation\n\n    Returns:\n        BatchEncoding with padded and truncated sequences\n    \"\"\"\n    return tokenizer(texts, truncation=True, max_length=max_len, padding=True, return_tensors=\"pt\")\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.predict_tokens","title":"predict_tokens","text":"<pre><code>predict_tokens(\n    model: Any, encodings: BatchEncoding, device: str\n) -&gt; Dict[str, Any]\n</code></pre> <p>Run model inference on encoded batch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>PreTrainedModel instance</p> required <code>encodings</code> <code>BatchEncoding</code> <p>BatchEncoding from tokenizer</p> required <code>device</code> <code>str</code> <p>Device string ('cuda' or 'cpu')</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing last_hidden_state from model output</p> Source code in <code>src\\pipeline.py</code> <pre><code>def predict_tokens(model: Any, encodings: BatchEncoding, device: str) -&gt; Dict[str, Any]:\n    \"\"\"Run model inference on encoded batch.\n\n    Args:\n        model: PreTrainedModel instance\n        encodings: BatchEncoding from tokenizer\n        device: Device string ('cuda' or 'cpu')\n\n    Returns:\n        Dictionary containing last_hidden_state from model output\n    \"\"\"\n    with torch.no_grad():\n        outputs = model(**{k: v.to(device) for k, v in encodings.items()})\n    return {\"last_hidden_state\": outputs.last_hidden_state}\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.postprocess_predictions","title":"postprocess_predictions","text":"<pre><code>postprocess_predictions(\n    batch_tokens: List[List[str]],\n    model_outputs: Dict[str, Any],\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Postprocess model outputs (placeholder for future expansion).</p> <p>Parameters:</p> Name Type Description Default <code>batch_tokens</code> <code>List[List[str]]</code> <p>List of tokenized texts</p> required <code>model_outputs</code> <code>Dict[str, Any]</code> <p>Dictionary containing model predictions</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries with basic token count</p> Note <p>This is a placeholder for future lexicon-based span extraction.</p> Source code in <code>src\\pipeline.py</code> <pre><code>def postprocess_predictions(\n    batch_tokens: List[List[str]], model_outputs: Dict[str, Any]\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Postprocess model outputs (placeholder for future expansion).\n\n    Args:\n        batch_tokens: List of tokenized texts\n        model_outputs: Dictionary containing model predictions\n\n    Returns:\n        List of dictionaries with basic token count\n\n    Note:\n        This is a placeholder for future lexicon-based span extraction.\n    \"\"\"\n    # Placeholder: extend with lexicon match, span extraction, normalization.\n    return [{\"token_count\": len(tokens)} for tokens in batch_tokens]\n</code></pre>"},{"location":"api/pipeline/#src.pipeline.simple_inference","title":"simple_inference","text":"<pre><code>simple_inference(\n    texts: List[str], persist_path: Optional[str] = None\n) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Run end-to-end inference pipeline with weak labeling.</p> <p>Combines BioBERT inference with lexicon-based weak labeling. Optionally persists results to JSONL format.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of input text strings to analyze</p> required <code>persist_path</code> <code>Optional[str]</code> <p>Optional path to save weak labels in JSONL format</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing: - token_count: Number of tokens in text - weak_spans: List of detected symptom/product spans with metadata</p> Example <p>texts = [\"Patient has severe rash and headache\"] results = simple_inference(texts, persist_path=\"output.jsonl\") print(results[0]['weak_spans']) [{'text': 'rash', 'label': 'SYMPTOM', ...}]</p> Source code in <code>src\\pipeline.py</code> <pre><code>def simple_inference(texts: List[str], persist_path: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n    \"\"\"Run end-to-end inference pipeline with weak labeling.\n\n    Combines BioBERT inference with lexicon-based weak labeling.\n    Optionally persists results to JSONL format.\n\n    Args:\n        texts: List of input text strings to analyze\n        persist_path: Optional path to save weak labels in JSONL format\n\n    Returns:\n        List of dictionaries containing:\n            - token_count: Number of tokens in text\n            - weak_spans: List of detected symptom/product spans with metadata\n\n    Example:\n        &gt;&gt;&gt; texts = [\"Patient has severe rash and headache\"]\n        &gt;&gt;&gt; results = simple_inference(texts, persist_path=\"output.jsonl\")\n        &gt;&gt;&gt; print(results[0]['weak_spans'])\n        [{'text': 'rash', 'label': 'SYMPTOM', ...}]\n    \"\"\"\n    config = AppConfig()\n    tokenizer = get_tokenizer(config)\n    model = get_model(config)\n    encodings = tokenize_batch(texts, tokenizer, config.max_seq_len)\n    outputs = predict_tokens(model, encodings, config.device)\n    batch_tokens = [tokenizer.tokenize(t) for t in texts]\n    base = postprocess_predictions(batch_tokens, outputs)\n\n    # Load lexicons\n    symptom_lex_path = Path(\"data/lexicon/symptoms.csv\")\n    product_lex_path = Path(\"data/lexicon/products.csv\")\n    symptom_lexicon = load_symptom_lexicon(symptom_lex_path)\n    product_lexicon = load_product_lexicon(product_lex_path)\n\n    # Weak labeling with config params\n    wl = (\n        weak_label_batch(\n            texts,\n            symptom_lexicon,\n            product_lexicon,\n            negation_window=config.negation_window,\n            scorer=config.fuzzy_scorer,\n        )\n        if (symptom_lexicon or product_lexicon)\n        else [[] for _ in texts]\n    )\n\n    # Persist if requested\n    if persist_path:\n        persist_weak_labels_jsonl(texts, wl, Path(persist_path))\n\n    # Merge\n    for rec, spans in zip(base, wl):\n        rec[\"weak_spans\"] = [\n            {\n                \"text\": s.text,\n                \"start\": s.start,\n                \"end\": s.end,\n                \"label\": s.label,\n                \"canonical\": s.canonical,\n                \"sku\": s.sku,\n                \"category\": s.category,\n                \"confidence\": s.confidence,\n                \"negated\": s.negated,\n            }\n            for s in spans\n        ]\n    return base\n</code></pre>"},{"location":"api/weak_label/","title":"Weak Label API","text":"<p>Lexicon-based weak labeling with fuzzy matching and negation detection.</p>"},{"location":"api/weak_label/#src.weak_label","title":"src.weak_label","text":"<p>Weak labeling for biomedical named entity recognition.</p> <p>DEPRECATION NOTICE: This module has been refactored into <code>src.weak_labeling</code> package. For new code, please use: <code>from src.weak_labeling import weak_label</code> This compatibility module will be maintained through version 0.2.x but may be removed in version 0.3.0.</p> <p>This module implements lexicon-based fuzzy matching with rule-based filters for automated span extraction. Suitable for bootstrapping annotation, active learning, and evaluation baselines.</p> Key Features <ul> <li>Fuzzy matching with RapidFuzz (WRatio \u226588, Jaccard \u226540)</li> <li>Bidirectional negation detection (forward + backward windows)</li> <li>Last-token alignment filter (prevents partial-word matches)</li> <li>Anatomy singleton filter (rejects generic anatomy mentions)</li> <li>Emoji and unicode handling (robust multi-byte support)</li> <li>Confidence scoring (0.8\u00d7fuzzy + 0.2\u00d7jaccard)</li> </ul> Typical Usage <p>from src.weak_label import match_symptoms, load_symptom_lexicon lexicon_entries = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\")) lexicon_terms = [entry.term for entry in lexicon_entries] text = \"Patient has severe itching\" spans = match_symptoms(text, lexicon_terms) print(spans[0][\"text\"], spans[0][\"label\"], spans[0][\"confidence\"]) severe itching SYMPTOM 0.92</p> See Also <ul> <li>User Guide: docs/user-guide/weak-labeling.md</li> <li>Negation Guide: docs/user-guide/negation.md</li> <li>API Reference: docs/api/weak_label.md</li> </ul>"},{"location":"api/weak_label/#src.weak_label.detect_negated_regions","title":"detect_negated_regions","text":"<pre><code>detect_negated_regions(\n    text: str, window: int = 5\n) -&gt; List[Tuple[int, int]]\n</code></pre> <p>Phase 3: Enhanced negation detection with forward/backward windows and prefix matching.</p> <p>Forward window: negation precedes symptom (e.g., \"no itching\") Backward window: negation follows symptom (e.g., \"itching absent\")</p>"},{"location":"api/weak_label/#src.weak_label.match_symptoms","title":"match_symptoms","text":"<pre><code>match_symptoms(\n    text: str,\n    lexicon: List[LexiconEntry],\n    fuzzy_threshold: float = 88.0,\n    max_term_words: int = 6,\n    negation_window: int = 5,\n    scorer: str = \"wratio\",\n) -&gt; List[Span]\n</code></pre> <p>Match symptom entities with negation detection.</p> <p>Wrapper around _match_entities() with negation enabled.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to extract symptoms from.</p> required <code>lexicon</code> <code>List[LexiconEntry]</code> <p>List of symptom lexicon entries.</p> required <code>fuzzy_threshold</code> <code>float</code> <p>Minimum fuzzy match score (0-100).</p> <code>88.0</code> <code>max_term_words</code> <code>int</code> <p>Maximum tokens per candidate phrase.</p> <code>6</code> <code>negation_window</code> <code>int</code> <p>Token window for bidirectional negation.</p> <code>5</code> <code>scorer</code> <code>str</code> <p>Fuzzy scoring method ('wratio' or 'jaccard').</p> <code>'wratio'</code> <p>Returns:</p> Type Description <code>List[Span]</code> <p>List of symptom spans with negation flags.</p>"},{"location":"api/weak_label/#src.weak_label.match_products","title":"match_products","text":"<pre><code>match_products(\n    text: str,\n    lexicon: List[LexiconEntry],\n    fuzzy_threshold: float = 88.0,\n    max_term_words: int = 6,\n    scorer: str = \"wratio\",\n) -&gt; List[Span]\n</code></pre> <p>Match product entities without negation detection.</p> <p>Wrapper around _match_entities() with negation disabled.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to extract products from.</p> required <code>lexicon</code> <code>List[LexiconEntry]</code> <p>List of product lexicon entries.</p> required <code>fuzzy_threshold</code> <code>float</code> <p>Minimum fuzzy match score (0-100).</p> <code>88.0</code> <code>max_term_words</code> <code>int</code> <p>Maximum tokens per candidate phrase.</p> <code>6</code> <code>scorer</code> <code>str</code> <p>Fuzzy scoring method ('wratio' or 'jaccard').</p> <code>'wratio'</code> <p>Returns:</p> Type Description <code>List[Span]</code> <p>List of product spans.</p>"},{"location":"api/weak_label/#src.weak_label.persist_weak_labels_jsonl","title":"persist_weak_labels_jsonl","text":"<pre><code>persist_weak_labels_jsonl(\n    texts: List[str],\n    spans_batch: List[List[Span]],\n    output_path: Path,\n) -&gt; None\n</code></pre> <p>Persist weak labels to JSONL for annotation triage.</p>"},{"location":"development/contributing/","title":"Contributing to SpanForge","text":"<p>Thank you for your interest in contributing to SpanForge! This guide will help you get started.</p>"},{"location":"development/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Development Setup</li> <li>Contribution Workflow</li> <li>Code Standards</li> <li>Testing Requirements</li> <li>Documentation</li> <li>Pull Request Process</li> <li>Contribution Areas</li> <li>Community Guidelines</li> </ul>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9, 3.10, or 3.11</li> <li>Git</li> <li>Virtual environment tool (venv or conda)</li> <li>Familiarity with PyTorch and transformers</li> </ul>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone repository\ngit clone https://github.com/your-org/spanforge.git\ncd spanforge\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or\nvenv\\Scripts\\activate  # Windows\n\n# Install dependencies (dev mode)\npip install -r requirements.txt\npip install -r docs-requirements.txt\npip install pre-commit pytest pytest-cov black isort\n\n# Verify setup\npython scripts/verify_env.py\npytest -v\n</code></pre>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":""},{"location":"development/contributing/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<p>Pre-commit hooks ensure code quality before commits:</p> <pre><code>pre-commit install\n</code></pre> <p>Hooks will now run automatically on <code>git commit</code>: - pytest (all tests must pass) - Code formatting checks</p>"},{"location":"development/contributing/#configure-ide","title":"Configure IDE","text":""},{"location":"development/contributing/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - Python (Microsoft) - Pylance - Jupyter - GitLens</p> <p>Recommended settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\"-v\"],\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": false,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"editor.formatOnSave\": true,\n  \"[python]\": {\n    \"editor.codeActionsOnSave\": {\n      \"source.organizeImports\": true\n    }\n  }\n}\n</code></pre>"},{"location":"development/contributing/#pycharm","title":"PyCharm","text":"<ul> <li>Enable pytest as test runner (Settings \u2192 Tools \u2192 Python Integrated Tools)</li> <li>Configure Black as formatter (Settings \u2192 Tools \u2192 Black)</li> <li>Enable type checking (Settings \u2192 Editor \u2192 Inspections \u2192 Python)</li> </ul>"},{"location":"development/contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"development/contributing/#1-create-issue-optional-but-recommended","title":"1. Create Issue (Optional but Recommended)","text":"<p>Before starting work, create an issue describing: - Problem or feature - Proposed solution - Expected impact</p> <p>Wait for maintainer feedback before starting large changes.</p>"},{"location":"development/contributing/#2-fork-and-branch","title":"2. Fork and Branch","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/your-username/spanforge.git\ncd spanforge\n\n# Add upstream remote\ngit remote add upstream https://github.com/original-org/spanforge.git\n\n# Create feature branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#3-make-changes","title":"3. Make Changes","text":"<ul> <li>Write code following Code Standards</li> <li>Add tests for new functionality</li> <li>Update documentation</li> <li>Commit regularly with clear messages</li> </ul>"},{"location":"development/contributing/#4-test-locally","title":"4. Test Locally","text":"<pre><code># Run all tests\npytest -v\n\n# Run specific test categories\npytest tests/test_weak_label_core.py -v\n\n# Check coverage\npytest --cov=src --cov-report=html\n\n# Build documentation\ncd docs\nmkdocs serve\n</code></pre>"},{"location":"development/contributing/#5-commit","title":"5. Commit","text":"<pre><code># Stage changes\ngit add .\n\n# Commit (pre-commit hooks will run)\ngit commit -m \"feat: add bidirectional negation detection\"\n\n# If hooks fail, fix issues and commit again\n</code></pre>"},{"location":"development/contributing/#6-push-and-create-pr","title":"6. Push and Create PR","text":"<pre><code># Push to your fork\ngit push origin feature/your-feature-name\n\n# Create pull request on GitHub\n</code></pre>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#style-guide","title":"Style Guide","text":"<p>Follow PEP 8 with these specifics:</p> <ul> <li>Line length: 100 characters (not 88 like Black default)</li> <li>Indentation: 4 spaces</li> <li>Quotes: Double quotes <code>\"</code> for strings</li> <li>Imports: Organize with isort (automatic)</li> </ul>"},{"location":"development/contributing/#formatting","title":"Formatting","text":"<p>Use Black for automatic formatting:</p> <pre><code># Format all files\nblack src tests\n\n# Check without formatting\nblack --check src tests\n</code></pre>"},{"location":"development/contributing/#import-organization","title":"Import Organization","text":"<p>Use isort for import sorting:</p> <pre><code># Sort imports\nisort src tests\n\n# Check without sorting\nisort --check-only src tests\n</code></pre> <p>Import order: 1. Standard library 2. Third-party packages 3. Local modules</p> <pre><code># Standard library\nimport os\nimport sys\nfrom typing import List, Optional\n\n# Third-party\nimport torch\nfrom transformers import AutoModel\n\n# Local\nfrom src.config import AppConfig\nfrom src.model import get_model\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Always use type hints for function signatures:</p> <pre><code>from typing import List, Dict, Optional\n\ndef match_symptoms(\n    text: str,\n    lexicon: List[str],\n    fuzzy_threshold: float = 88.0,\n    negation_window: int = 5\n) -&gt; List[Dict[str, any]]:\n    \"\"\"\n    Match symptoms in text using fuzzy matching.\n\n    Args:\n        text: Input text to search\n        lexicon: List of symptom terms\n        fuzzy_threshold: Minimum fuzzy score (0-100)\n        negation_window: Negation scope in tokens\n\n    Returns:\n        List of matched span dictionaries\n    \"\"\"\n    pass\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def example_function(arg1: str, arg2: int = 0) -&gt; bool:\n    \"\"\"\n    Short one-line description.\n\n    Longer description explaining functionality, edge cases,\n    and important details.\n\n    Args:\n        arg1: Description of arg1\n        arg2: Description of arg2 (default: 0)\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When arg1 is empty\n\n    Examples:\n        &gt;&gt;&gt; example_function(\"test\", 5)\n        True\n        &gt;&gt;&gt; example_function(\"\", 0)\n        ValueError: arg1 cannot be empty\n    \"\"\"\n    if not arg1:\n        raise ValueError(\"arg1 cannot be empty\")\n    return len(arg1) &gt; arg2\n</code></pre>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Functions/Variables: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Constants: <code>UPPER_SNAKE_CASE</code></li> <li>Private/Internal: <code>_leading_underscore</code></li> </ul> <pre><code># Good\ndef match_symptoms(text: str) -&gt; List[Dict]:\n    NEGATION_TOKENS = {\"no\", \"not\", \"never\"}\n    _internal_cache = {}\n\nclass WeakLabelMatcher:\n    pass\n</code></pre>"},{"location":"development/contributing/#testing-requirements","title":"Testing Requirements","text":""},{"location":"development/contributing/#test-categories","title":"Test Categories","text":"<p>All contributions must include tests:</p> <ol> <li>Core Tests: Basic functionality</li> <li>Edge Case Tests: Boundary conditions, unicode, errors</li> <li>Integration Tests: End-to-end workflows</li> </ol>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>import pytest\nfrom src.weak_label import match_symptoms\n\ndef test_match_symptoms_basic():\n    \"\"\"Test basic symptom matching.\"\"\"\n    text = \"Patient has severe itching\"\n    lexicon = [\"itching\", \"redness\"]\n\n    spans = match_symptoms(text, lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] in [\"itching\", \"severe itching\"]\n    assert spans[0][\"label\"] == \"SYMPTOM\"\n    assert 0 &lt;= spans[0][\"confidence\"] &lt;= 1.0\n\ndef test_negation_detection():\n    \"\"\"Test negation detection.\"\"\"\n    text = \"No itching reported\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    assert len(spans) == 1\n    assert spans[0].get(\"negated\", False) is True\n</code></pre>"},{"location":"development/contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for \u226590% coverage on new code</li> <li>All functions must be tested</li> <li>Test edge cases: empty inputs, unicode, very long texts</li> <li>Test error handling: invalid inputs, missing files</li> </ul> <pre><code># Check coverage\npytest --cov=src --cov-report=term-missing\n\n# Generate HTML report\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest -v\n\n# Specific file\npytest tests/test_weak_label_core.py -v\n\n# Specific test\npytest tests/test_weak_label_core.py::test_match_symptoms_basic -v\n\n# Pattern matching\npytest -k \"negation\" -v\n\n# Stop on first failure\npytest -x\n\n# Show print statements\npytest -v -s\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#api-documentation","title":"API Documentation","text":"<p>Add docstrings to all public functions/classes:</p> <pre><code>def new_function(arg: str) -&gt; int:\n    \"\"\"\n    One-line summary.\n\n    Detailed explanation of what the function does,\n    including edge cases and important notes.\n\n    Args:\n        arg: Description of argument\n\n    Returns:\n        Description of return value\n\n    Examples:\n        &gt;&gt;&gt; new_function(\"test\")\n        4\n    \"\"\"\n    return len(arg)\n</code></pre>"},{"location":"development/contributing/#user-guides","title":"User Guides","text":"<p>For new features, add user guide documentation:</p> <ul> <li>Create markdown file in <code>docs/user-guide/</code></li> <li>Include examples and use cases</li> <li>Add to navigation in <code>mkdocs.yml</code></li> </ul> <p>Example structure:</p> <pre><code># Feature Name\n\n## Overview\nBrief introduction\n\n## Usage\nBasic examples\n\n## Advanced Usage\nComplex scenarios\n\n## Best Practices\nRecommendations\n\n## Troubleshooting\nCommon issues\n</code></pre>"},{"location":"development/contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Install docs dependencies\npip install -r docs-requirements.txt\n\n# Serve locally\nmkdocs serve\n# Open http://localhost:8000\n\n# Build static site\nmkdocs build\n# Output in site/\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ul> <li> All tests pass locally (<code>pytest -v</code>)</li> <li> Code is formatted (<code>black src tests</code>)</li> <li> Imports are sorted (<code>isort src tests</code>)</li> <li> Documentation is updated</li> <li> Changelog is updated (if applicable)</li> <li> Branch is up-to-date with main</li> </ul>"},{"location":"development/contributing/#pr-title-format","title":"PR Title Format","text":"<p>Use conventional commits format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\nTypes:\n- feat: New feature\n- fix: Bug fix\n- docs: Documentation changes\n- test: Test additions/changes\n- refactor: Code refactoring\n- perf: Performance improvements\n- chore: Maintenance tasks\n\nExamples:\nfeat(weak_label): add bidirectional negation detection\nfix(pipeline): handle empty text input gracefully\ndocs(annotation): add Label Studio tutorial\ntest(negation): add 12 new negation pattern tests\n</code></pre>"},{"location":"development/contributing/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Motivation\nWhy is this change needed?\n\n## Changes\n- Change 1\n- Change 2\n\n## Testing\n- Test 1 added\n- Test 2 updated\n\n## Checklist\n- [ ] Tests pass\n- [ ] Documentation updated\n- [ ] Changelog updated (if applicable)\n</code></pre>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated Checks: CI/CD runs tests on 6 configurations</li> <li>Code Review: Maintainer reviews code, tests, docs</li> <li>Feedback: Address review comments</li> <li>Approval: Maintainer approves and merges</li> </ol> <p>Response time: Expect initial review within 3-5 business days.</p>"},{"location":"development/contributing/#contribution-areas","title":"Contribution Areas","text":""},{"location":"development/contributing/#1-annotation","title":"1. Annotation","text":"<p>Skills: Domain knowledge, attention to detail Tasks: - Annotate complaint texts in Label Studio - Review and correct weak labels - Participate in consensus annotation</p> <p>Impact: Directly improves model quality</p>"},{"location":"development/contributing/#2-documentation","title":"2. Documentation","text":"<p>Skills: Technical writing, markdown Tasks: - Expand user guides and tutorials - Add code examples - Improve API documentation - Write blog posts or walkthroughs</p> <p>Impact: Makes project more accessible</p>"},{"location":"development/contributing/#3-testing","title":"3. Testing","text":"<p>Skills: Python, pytest Tasks: - Add edge case tests - Improve test coverage - Write integration tests - Performance benchmarks</p> <p>Impact: Increases code reliability</p>"},{"location":"development/contributing/#4-feature-development","title":"4. Feature Development","text":"<p>Skills: Python, NLP, ML Tasks: - Implement roadmap features - Add new heuristics or filters - Integrate new models - Build tooling (CLI, API)</p> <p>Impact: Expands functionality</p>"},{"location":"development/contributing/#5-research-experimentation","title":"5. Research &amp; Experimentation","text":"<p>Skills: NLP, ML, evaluation Tasks: - Experiment with new models (RoBERTa, ClinicalBERT) - Tune hyperparameters - Evaluate weak labeling approaches - Write research reports</p> <p>Impact: Advances state-of-the-art</p>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful: Treat all contributors with respect</li> <li>Be constructive: Provide helpful, actionable feedback</li> <li>Be inclusive: Welcome diverse perspectives</li> <li>Be patient: Remember everyone is learning</li> </ul>"},{"location":"development/contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues: Bug reports, feature requests</li> <li>Pull Requests: Code contributions, reviews</li> <li>Discussions: General questions, brainstorming</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - <code>CONTRIBUTORS.md</code> file - Release notes - Documentation acknowledgments</p>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#stuck-ask-for-help","title":"Stuck? Ask for help!","text":"<ul> <li>GitHub Discussions: General questions</li> <li>GitHub Issues: Specific problems</li> <li>Documentation: User guides, API reference</li> </ul>"},{"location":"development/contributing/#useful-resources","title":"Useful Resources","text":"<ul> <li>Roadmap - Project plan</li> <li>Testing Guide - Test infrastructure</li> <li>Annotation Guide - Annotation workflow</li> <li>Weak Labeling Guide - Core functionality</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's license (see <code>LICENSE</code> file).</p> <p>Thank you for contributing to SpanForge! \ud83c\udf89</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Comprehensive guide to SpanForge's test infrastructure.</p>"},{"location":"development/testing/#overview","title":"Overview","text":"<p>SpanForge implements 144 tests across 4 categories:</p> Category Count Purpose Core Tests 16 Basic functionality validation Edge Case Tests 98 Boundary conditions, unicode, negation Integration Tests 26 End-to-end workflows, scale, performance Curation Tests 4 Annotation pipeline, Label Studio export <p>Test Status: \u2705 144/144 passing (100%)</p>"},{"location":"development/testing/#quick-start","title":"Quick Start","text":""},{"location":"development/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code># Run full suite\npytest\n\n# With verbose output\npytest -v\n\n# With coverage\npytest --cov=src --cov-report=html\n</code></pre>"},{"location":"development/testing/#run-specific-categories","title":"Run Specific Categories","text":"<pre><code># Core tests only\npytest tests/test_weak_label_core.py -v\n\n# Edge cases only\npytest tests/test_weak_label_edge.py -v\n\n# Integration tests only\npytest tests/test_integration.py -v\n\n# Curation tests only\npytest tests/test_curation.py -v\n</code></pre>"},{"location":"development/testing/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code># Single test function\npytest tests/test_weak_label_core.py::test_match_symptoms_basic -v\n\n# Pattern matching\npytest -k \"negation\" -v  # All negation tests\npytest -k \"emoji\" -v     # All emoji tests\npytest -k \"unicode\" -v   # All unicode tests\n</code></pre>"},{"location":"development/testing/#test-architecture","title":"Test Architecture","text":""},{"location":"development/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                    # Pytest fixtures\n\u251c\u2500\u2500 test_weak_label_core.py        # 16 core tests\n\u251c\u2500\u2500 test_weak_label_edge.py        # 98 edge case tests\n\u251c\u2500\u2500 test_integration.py            # 26 integration tests\n\u2514\u2500\u2500 test_curation.py               # 4 curation tests\n</code></pre>"},{"location":"development/testing/#test-composition-pattern","title":"Test Composition Pattern","text":"<p>Tests use composition for shared setup:</p> <pre><code># Base test class\nclass WeakLabelTestBase:\n    \"\"\"Shared fixtures and utilities.\"\"\"\n\n    @pytest.fixture\n    def symptom_lexicon(self):\n        return [\"itching\", \"redness\", \"burning sensation\"]\n\n    @pytest.fixture\n    def product_lexicon(self):\n        return [\"Lotion X\", \"Cream Y\"]\n\n# Edge case test class (inherits base)\nclass TestWeakLabelEdgeCases(WeakLabelTestBase):\n    \"\"\"Edge case tests with shared fixtures.\"\"\"\n\n    def test_emoji_handling(self, symptom_lexicon):\n        text = \"Patient has \ud83d\ude0a severe itching\"\n        spans = match_symptoms(text, symptom_lexicon)\n        assert len(spans) &gt; 0\n</code></pre> <p>Benefits: - Avoid duplicate fixture code - Easy to extend with new test categories - Clear inheritance hierarchy</p>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#1-core-tests-16","title":"1. Core Tests (16)","text":"<p>Purpose: Validate basic functionality</p> <p>Examples:</p> <pre><code>def test_match_symptoms_basic(symptom_lexicon):\n    \"\"\"Test basic symptom matching.\"\"\"\n    text = \"Patient has severe itching\"\n    spans = match_symptoms(text, symptom_lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] in [\"itching\", \"severe itching\"]\n    assert spans[0][\"label\"] == \"SYMPTOM\"\n\ndef test_match_products_basic(product_lexicon):\n    \"\"\"Test basic product matching.\"\"\"\n    text = \"Used Lotion X twice daily\"\n    spans = match_products(text, product_lexicon)\n\n    assert len(spans) == 1\n    assert spans[0][\"text\"] == \"Lotion X\"\n    assert spans[0][\"label\"] == \"PRODUCT\"\n\ndef test_negation_forward():\n    \"\"\"Test forward negation detection.\"\"\"\n    text = \"No history of itching\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    assert len(spans) == 1\n    assert spans[0].get(\"negated\", False) is True\n</code></pre> <p>Coverage: - Symptom matching - Product matching - Fuzzy matching - Negation detection (forward/backward) - Confidence scoring</p>"},{"location":"development/testing/#2-edge-case-tests-98","title":"2. Edge Case Tests (98)","text":"<p>Purpose: Validate boundary conditions and special cases</p> <p>Categories:</p>"},{"location":"development/testing/#unicode-emoji-12-tests","title":"Unicode &amp; Emoji (12 tests)","text":"<pre><code>def test_emoji_within_text():\n    \"\"\"Test emoji doesn't break span detection.\"\"\"\n    text = \"Patient has \ud83d\ude0a severe itching and \ud83c\udf21\ufe0f redness\"\n    spans = match_symptoms(text, [\"itching\", \"redness\"])\n    assert len(spans) == 2\n\ndef test_unicode_medical_symbols():\n    \"\"\"Test medical unicode symbols.\"\"\"\n    text = \"Patient has \u22653 episodes of severe itching\"\n    spans = match_symptoms(text, [\"itching\"])\n    assert len(spans) == 1\n</code></pre>"},{"location":"development/testing/#negation-patterns-24-tests","title":"Negation Patterns (24 tests)","text":"<pre><code>def test_bidirectional_negation():\n    \"\"\"Test both forward and backward negation.\"\"\"\n    # Forward\n    text1 = \"No history of itching\"\n    spans1 = match_symptoms(text1, [\"itching\"])\n    assert spans1[0].get(\"negated\", False) is True\n\n    # Backward\n    text2 = \"Itching was denied\"\n    spans2 = match_symptoms(text2, [\"itching\"])\n    assert spans2[0].get(\"negated\", False) is True\n\ndef test_negation_out_of_scope():\n    \"\"\"Test negation beyond window.\"\"\"\n    text = \"Patient denies fever but reports itching\"\n    spans = match_symptoms(text, [\"itching\"], negation_window=5)\n    itching_span = next(s for s in spans if s[\"text\"] == \"itching\")\n    assert itching_span.get(\"negated\", False) is False\n</code></pre>"},{"location":"development/testing/#boundary-cases-18-tests","title":"Boundary Cases (18 tests)","text":"<pre><code>def test_last_token_alignment():\n    \"\"\"Test last-token alignment filter.\"\"\"\n    text = \"Patient has severe itching today\"\n    spans = match_symptoms(text, [\"itch\"])  # Partial match\n    # Should NOT match \"itch\" (doesn't end at token boundary)\n    assert not any(s[\"text\"] == \"itch\" for s in spans)\n\ndef test_sentence_boundary():\n    \"\"\"Test span doesn't cross sentence.\"\"\"\n    text = \"Patient has redness. New sentence with itching.\"\n    spans = match_symptoms(text, [\"redness\", \"itching\"])\n    assert len(spans) == 2\n</code></pre>"},{"location":"development/testing/#anatomy-filter-15-tests","title":"Anatomy Filter (15 tests)","text":"<pre><code>def test_anatomy_singleton_rejection():\n    \"\"\"Test single anatomy token rejected.\"\"\"\n    text = \"Apply to skin twice daily\"\n    spans = match_symptoms(text, [\"skin\"])\n    assert len(spans) == 0  # Generic anatomy alone rejected\n\ndef test_anatomy_with_symptom_keyword():\n    \"\"\"Test anatomy accepted with symptom.\"\"\"\n    text = \"Patient has skin redness\"\n    spans = match_symptoms(text, [\"skin\"])\n    assert len(spans) &gt; 0  # Accepted due to \"redness\" co-occurrence\n</code></pre>"},{"location":"development/testing/#validation-errors-29-tests","title":"Validation &amp; Errors (29 tests)","text":"<pre><code>def test_empty_lexicon():\n    \"\"\"Test empty lexicon handling.\"\"\"\n    text = \"Patient has itching\"\n    spans = match_symptoms(text, [])\n    assert spans == []\n\ndef test_empty_text():\n    \"\"\"Test empty text handling.\"\"\"\n    spans = match_symptoms(\"\", [\"itching\"])\n    assert spans == []\n\ndef test_confidence_bounds():\n    \"\"\"Test confidence clamped to [0, 1].\"\"\"\n    text = \"Patient has severe itching\"\n    spans = match_symptoms(text, [\"severe itching\"])\n    for span in spans:\n        assert 0.0 &lt;= span[\"confidence\"] &lt;= 1.0\n</code></pre>"},{"location":"development/testing/#3-integration-tests-26","title":"3. Integration Tests (26)","text":"<p>Purpose: Validate end-to-end workflows</p> <p>Categories:</p>"},{"location":"development/testing/#pipeline-integration-11-tests","title":"Pipeline Integration (11 tests)","text":"<pre><code>def test_pipeline_end_to_end():\n    \"\"\"Test full pipeline from text to entities.\"\"\"\n    from src.pipeline import simple_inference\n\n    text = \"Patient used Lotion X and experienced severe itching\"\n    results = simple_inference([text])\n\n    assert len(results) == 1\n    assert \"text\" in results[0]\n    assert \"entities\" in results[0]\n    assert len(results[0][\"entities\"]) &gt;= 2  # SYMPTOM + PRODUCT\n\ndef test_pipeline_jsonl_export():\n    \"\"\"Test JSONL persistence.\"\"\"\n    import tempfile\n    from src.pipeline import simple_inference\n\n    texts = [\"Text 1 with itching\", \"Text 2 with redness\"]\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n        output_path = f.name\n\n    results = simple_inference(texts, output_jsonl=output_path)\n\n    # Verify file created\n    with open(output_path) as f:\n        lines = f.readlines()\n    assert len(lines) == len(texts)\n</code></pre>"},{"location":"development/testing/#scale-tests-9-tests","title":"Scale Tests (9 tests)","text":"<pre><code>def test_large_batch_processing():\n    \"\"\"Test batch processing 100+ texts.\"\"\"\n    from src.pipeline import simple_inference\n\n    texts = [f\"Patient {i} has itching\" for i in range(100)]\n    results = simple_inference(texts)\n\n    assert len(results) == 100\n    assert all(\"entities\" in r for r in results)\n\ndef test_long_text_handling():\n    \"\"\"Test very long texts (&gt;1000 chars).\"\"\"\n    text = \"Patient has itching. \" * 50  # ~1000 chars\n    spans = match_symptoms(text, [\"itching\"])\n    assert len(spans) &gt; 0\n</code></pre>"},{"location":"development/testing/#performance-tests-6-tests","title":"Performance Tests (6 tests)","text":"<pre><code>import time\n\ndef test_inference_speed():\n    \"\"\"Test inference time per text.\"\"\"\n    from src.pipeline import simple_inference\n\n    texts = [f\"Patient {i} has itching\" for i in range(10)]\n\n    start = time.time()\n    results = simple_inference(texts)\n    elapsed = time.time() - start\n\n    # Should process &lt;1 sec/text on CPU\n    assert elapsed / len(texts) &lt; 1.0\n</code></pre>"},{"location":"development/testing/#4-curation-tests-4","title":"4. Curation Tests (4)","text":"<p>Purpose: Validate annotation pipeline</p> <pre><code>def test_weak_label_export_format():\n    \"\"\"Test weak labels exportable to Label Studio.\"\"\"\n    from src.pipeline import simple_inference\n    import json\n\n    text = \"Patient has itching\"\n    results = simple_inference([text])\n\n    # Convert to Label Studio format\n    task = {\n        \"data\": {\"text\": results[0][\"text\"]},\n        \"predictions\": [{\n            \"result\": [\n                {\n                    \"value\": {\n                        \"start\": e[\"start\"],\n                        \"end\": e[\"end\"],\n                        \"text\": e[\"text\"],\n                        \"labels\": [e[\"label\"]]\n                    },\n                    \"from_name\": \"label\",\n                    \"to_name\": \"text\",\n                    \"type\": \"labels\"\n                }\n                for e in results[0][\"entities\"]\n            ]\n        }]\n    }\n\n    # Validate JSON serializable\n    json_str = json.dumps(task)\n    assert json_str\n</code></pre>"},{"location":"development/testing/#fixtures","title":"Fixtures","text":""},{"location":"development/testing/#shared-fixtures-conftestpy","title":"Shared Fixtures (conftest.py)","text":"<pre><code>import pytest\n\n@pytest.fixture\ndef symptom_lexicon():\n    \"\"\"Standard symptom lexicon.\"\"\"\n    return [\n        \"itching\", \"redness\", \"burning\", \"swelling\",\n        \"severe itching\", \"burning sensation\", \"dry skin\"\n    ]\n\n@pytest.fixture\ndef product_lexicon():\n    \"\"\"Standard product lexicon.\"\"\"\n    return [\"Lotion X\", \"Cream Y\", \"Soap Z\"]\n\n@pytest.fixture\ndef sample_texts():\n    \"\"\"Sample complaint texts.\"\"\"\n    return [\n        \"Patient has severe itching\",\n        \"No redness reported\",\n        \"Used Lotion X twice daily\"\n    ]\n</code></pre>"},{"location":"development/testing/#test-specific-fixtures","title":"Test-Specific Fixtures","text":"<pre><code>@pytest.fixture\ndef negation_config():\n    \"\"\"Config with extended negation window.\"\"\"\n    from src.config import AppConfig\n    return AppConfig(negation_window=7)\n\n@pytest.fixture\ndef temp_output_dir(tmp_path):\n    \"\"\"Temporary output directory.\"\"\"\n    output_dir = tmp_path / \"output\"\n    output_dir.mkdir()\n    return output_dir\n</code></pre>"},{"location":"development/testing/#assertions-validation","title":"Assertions &amp; Validation","text":""},{"location":"development/testing/#entity-assertions","title":"Entity Assertions","text":"<pre><code>def assert_entity_valid(entity):\n    \"\"\"Validate entity structure.\"\"\"\n    assert \"text\" in entity\n    assert \"start\" in entity\n    assert \"end\" in entity\n    assert \"label\" in entity\n    assert entity[\"label\"] in [\"SYMPTOM\", \"PRODUCT\"]\n    assert 0 &lt;= entity.get(\"confidence\", 0.0) &lt;= 1.0\n\ndef assert_span_bounds(text, entity):\n    \"\"\"Validate span boundaries.\"\"\"\n    assert 0 &lt;= entity[\"start\"] &lt; len(text)\n    assert entity[\"start\"] &lt; entity[\"end\"] &lt;= len(text)\n    assert text[entity[\"start\"]:entity[\"end\"]] == entity[\"text\"]\n</code></pre>"},{"location":"development/testing/#negation-assertions","title":"Negation Assertions","text":"<pre><code>def assert_negated(text, entity_text, lexicon):\n    \"\"\"Assert entity is negated.\"\"\"\n    spans = match_symptoms(text, lexicon)\n    entity = next(s for s in spans if s[\"text\"] == entity_text)\n    assert entity.get(\"negated\", False) is True\n\ndef assert_not_negated(text, entity_text, lexicon):\n    \"\"\"Assert entity is NOT negated.\"\"\"\n    spans = match_symptoms(text, lexicon)\n    entity = next(s for s in spans if s[\"text\"] == entity_text)\n    assert entity.get(\"negated\", False) is False\n</code></pre>"},{"location":"development/testing/#coverage","title":"Coverage","text":""},{"location":"development/testing/#current-coverage-example","title":"Current Coverage (Example)","text":"<pre><code>Name                        Stmts   Miss  Cover\n-----------------------------------------------\nsrc/__init__.py                 0      0   100%\nsrc/config.py                  45      2    96%\nsrc/model.py                   52      3    94%\nsrc/weak_label.py             187     12    94%\nsrc/pipeline.py                78      5    94%\nsrc/llm_agent.py               34     28    18%  # Stub implementation\n-----------------------------------------------\nTOTAL                         396     50    87%\n</code></pre>"},{"location":"development/testing/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code># HTML report\npytest --cov=src --cov-report=html\n\n# Open in browser\nstart htmlcov/index.html  # Windows\nopen htmlcov/index.html   # macOS\n\n# Terminal report\npytest --cov=src --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>SpanForge runs tests on 6 configurations (2 OS \u00d7 3 Python versions):</p> <p>.github/workflows/test.yml:</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest]\n        python-version: ['3.9', '3.10', '3.11']\n\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install pytest pytest-cov\n\n      - name: Run tests\n        run: pytest -v --cov=src --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n</code></pre>"},{"location":"development/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>.pre-commit-config.yaml:</p> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: pytest\n        name: pytest\n        entry: pytest\n        language: system\n        pass_filenames: false\n        always_run: true\n</code></pre> <p>Install hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Now tests run automatically before each commit.</p>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Write tests first (TDD) - define expected behavior</li> <li>Use descriptive names - <code>test_negation_forward_window_5</code> not <code>test1</code></li> <li>One assertion per test - easier to debug failures</li> <li>Use fixtures - avoid duplicate setup code</li> <li>Test edge cases - empty inputs, boundary values, unicode</li> <li>Mock external calls - don't hit HuggingFace API in tests</li> <li>Run locally before push - ensure CI will pass</li> <li>Track coverage - aim for \u226590% on core modules</li> </ol>"},{"location":"development/testing/#debugging-failed-tests","title":"Debugging Failed Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Show print statements\npytest -v -s\n\n# Show locals on failure\npytest -v -l\n\n# Stop on first failure\npytest -x\n</code></pre>"},{"location":"development/testing/#debugging-with-pdb","title":"Debugging with pdb","text":"<pre><code>def test_example():\n    text = \"Patient has itching\"\n    spans = match_symptoms(text, [\"itching\"])\n\n    # Drop into debugger\n    import pdb; pdb.set_trace()\n\n    assert len(spans) == 1\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>import pytest\n\n@pytest.mark.parametrize(\"text,expected\", [\n    (\"No itching\", True),\n    (\"Patient has itching\", False),\n    (\"Itching was denied\", True),\n])\ndef test_negation_parametrized(text, expected):\n    \"\"\"Test negation with multiple cases.\"\"\"\n    spans = match_symptoms(text, [\"itching\"])\n    assert spans[0].get(\"negated\", False) == expected\n</code></pre>"},{"location":"development/testing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"development/testing/#benchmark-suite","title":"Benchmark Suite","text":"<pre><code>import pytest\nimport time\n\n@pytest.mark.benchmark\ndef test_match_symptoms_speed(benchmark):\n    \"\"\"Benchmark symptom matching.\"\"\"\n    text = \"Patient has severe itching and redness\"\n    lexicon = [\"itching\", \"redness\", \"burning\"]\n\n    result = benchmark(match_symptoms, text, lexicon)\n    assert len(result) &gt; 0\n\n# Run benchmarks\npytest -v -m benchmark --benchmark-only\n</code></pre>"},{"location":"development/testing/#expected-performance","title":"Expected Performance","text":"Operation Texts Time (CPU) Time (GPU) <code>match_symptoms</code> 1 ~10ms N/A <code>simple_inference</code> 1 ~200ms ~50ms <code>simple_inference</code> (batch=32) 32 ~5s ~1s"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Development: Contributing - Contribution guidelines</li> <li>User Guide: Weak Labeling - Understand tested features</li> <li>API Reference - Function signatures</li> </ul>"},{"location":"user-guide/annotation/","title":"Annotation Guide","text":"<p>Comprehensive guide to human annotation workflow using Label Studio.</p>"},{"location":"user-guide/annotation/#overview","title":"Overview","text":"<p>SpanForge uses Label Studio for manual annotation and weak label refinement. Annotation workflow:</p> <ol> <li>Export weak labels from pipeline</li> <li>Import to Label Studio as pre-annotations</li> <li>Human annotation - correct, add, remove spans</li> <li>Export gold labels - validated annotations</li> <li>Convert to training format - JSONL with provenance</li> <li>Quality assurance - inter-annotator agreement, coverage</li> </ol> <pre><code>graph LR\n    A[Weak Labels] --&gt; B[Label Studio Import]\n    B --&gt; C[Human Annotation]\n    C --&gt; D[Export]\n    D --&gt; E[Gold JSONL]\n    E --&gt; F[Model Training]\n    E --&gt; G[Quality Reports]</code></pre>"},{"location":"user-guide/annotation/#setup","title":"Setup","text":""},{"location":"user-guide/annotation/#installation","title":"Installation","text":"<pre><code># Install Label Studio\npip install label-studio\n\n# Disable telemetry (privacy)\nsetx LABEL_STUDIO_DISABLE_TELEMETRY 1\n\n# Or for current session only:\n$env:LABEL_STUDIO_DISABLE_TELEMETRY = \"1\"\n</code></pre>"},{"location":"user-guide/annotation/#launch","title":"Launch","text":"<pre><code># Start server (local only)\nlabel-studio start --host localhost --port 8080\n\n# Open browser\n# Navigate to: http://localhost:8080\n</code></pre>"},{"location":"user-guide/annotation/#project-configuration","title":"Project Configuration","text":"<p>Create project with this Label Config (<code>data/annotation/config/label_config.xml</code>):</p> <pre><code>&lt;View&gt;\n  &lt;Text name=\"text\" value=\"$text\"/&gt;\n  &lt;Labels name=\"label\" toName=\"text\"&gt;\n    &lt;Label value=\"SYMPTOM\" background=\"#FF6B6B\"/&gt;\n    &lt;Label value=\"PRODUCT\" background=\"#4ECDC4\"/&gt;\n  &lt;/Labels&gt;\n&lt;/View&gt;\n</code></pre> <p>Label Descriptions: - SYMPTOM (red): Adverse events, symptoms, reactions (e.g., \"itching\", \"severe redness\") - PRODUCT (teal): Product mentions, brand names, drug names (e.g., \"Lotion X\", \"ibuprofen\")</p>"},{"location":"user-guide/annotation/#annotation-workflow","title":"Annotation Workflow","text":""},{"location":"user-guide/annotation/#1-export-weak-labels","title":"1. Export Weak Labels","text":"<pre><code>from src.pipeline import simple_inference\nimport json\n\n# Load texts\ntexts = load_texts(\"complaints.csv\")\n\n# Generate weak labels\nresults = simple_inference(texts)\n\n# Export to JSONL (Label Studio format)\noutput_path = \"data/annotation/exports/weak_labels.jsonl\"\nwith open(output_path, \"w\") as f:\n    for i, result in enumerate(results):\n        task = {\n            \"id\": i,\n            \"data\": {\"text\": result[\"text\"]},\n            \"predictions\": [{\n                \"result\": [\n                    {\n                        \"value\": {\n                            \"start\": entity[\"start\"],\n                            \"end\": entity[\"end\"],\n                            \"text\": entity[\"text\"],\n                            \"labels\": [entity[\"label\"]]\n                        },\n                        \"from_name\": \"label\",\n                        \"to_name\": \"text\",\n                        \"type\": \"labels\"\n                    }\n                    for entity in result[\"entities\"]\n                ]\n            }]\n        }\n        f.write(json.dumps(task) + \"\\n\")\n\nprint(f\"Exported {len(results)} tasks to {output_path}\")\n</code></pre>"},{"location":"user-guide/annotation/#2-import-to-label-studio","title":"2. Import to Label Studio","text":"<ol> <li>Create Project: Click \"Create Project\"</li> <li>Project Name: \"SpanForge Annotation\"</li> <li>Labeling Setup: Paste label config XML (see above)</li> <li>Data Import: </li> <li>Click \"Import\"</li> <li>Select <code>weak_labels.jsonl</code></li> <li>Check \"Treat as pre-annotations\"</li> <li>Save</li> </ol>"},{"location":"user-guide/annotation/#3-annotate-tasks","title":"3. Annotate Tasks","text":""},{"location":"user-guide/annotation/#task-interface","title":"Task Interface","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Task 1 of 100                                     [Skip] [Submit] \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  Patient experienced severe itching after using Lotion X       \u2502\n\u2502                      ^^^^^^^^^^^^^^^^              ^^^^^^^^^    \u2502\n\u2502                      SYMPTOM (weak)                PRODUCT (weak)\u2502\n\u2502                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Labels:                                                        \u2502\n\u2502  \u2610 SYMPTOM   \u2610 PRODUCT                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/annotation/#annotation-actions","title":"Annotation Actions","text":"<p>Select span: 1. Click and drag to highlight text 2. Choose label (SYMPTOM or PRODUCT) 3. Span appears with colored background</p> <p>Edit span: 1. Click existing span 2. Adjust boundaries by dragging edges 3. Or delete and recreate</p> <p>Remove span: 1. Click span to select 2. Press <code>Backspace</code> or click trash icon</p> <p>Keyboard shortcuts: - <code>S</code> - Select SYMPTOM label - <code>P</code> - Select PRODUCT label - <code>Backspace</code> - Delete selected span - <code>Ctrl+Enter</code> - Submit task - <code>Ctrl+Space</code> - Skip task</p>"},{"location":"user-guide/annotation/#4-annotation-guidelines","title":"4. Annotation Guidelines","text":""},{"location":"user-guide/annotation/#boundary-rules","title":"Boundary Rules","text":"<p>\u2713 CORRECT: <pre><code>\"severe burning sensation\"  # Include full phrase\n\"redness and swelling\"      # Include conjunction\n\"dry skin\"                  # Include modifier + noun\n</code></pre></p> <p>\u2717 INCORRECT: <pre><code>\"severe burning sensation.\"  # Exclude punctuation\n\"burning\"                    # Missing \"sensation\" (truncated)\n\" itching \"                  # Exclude leading/trailing spaces\n</code></pre></p>"},{"location":"user-guide/annotation/#negation-policy","title":"Negation Policy","text":"<p>Annotate negated symptoms - mark the span even if negated:</p> <pre><code>\"No history of itching\"\n           ^^^^^^^^ SYMPTOM (annotate, flag as negated)\n</code></pre> <p>Rationale: Teaches model negation scope, improves recall.</p> <p>Flag negated spans (optional): - Add comment: \"NEGATED\" - Or use separate label \"SYMPTOM_NEG\" (requires config change)</p>"},{"location":"user-guide/annotation/#ambiguous-cases","title":"Ambiguous Cases","text":"<p>Generic anatomy (alone): <pre><code>\"Apply to skin twice daily\"\n         ^^^^ DON'T annotate (generic, no symptom)\n\n\"Patient has skin redness\"\n             ^^^^^^^^^^^^ SYMPTOM (symptom phrase)\n</code></pre></p> <p>Product vs. Ingredient: <pre><code>\"Lotion X\"          \u2192 PRODUCT (brand name)\n\"contains retinol\"  \u2192 Don't annotate (ingredient alone)\n\"retinol cream\"     \u2192 PRODUCT (product phrase)\n</code></pre></p> <p>Synonym Preference: <pre><code>\"pruritus\" \u2192 SYMPTOM (medical term, keep canonical)\n\"itching\"  \u2192 SYMPTOM (colloquial, keep canonical)\n# Canonicalization handled downstream\n</code></pre></p>"},{"location":"user-guide/annotation/#common-errors","title":"Common Errors","text":"Error Example Fix Partial span \"severe\" instead of \"severe itching\" Extend to full phrase Trailing punct \"redness.\" (includes period) Exclude punctuation Missed negation Skip \"itching\" in \"No itching\" Annotate, flag negated Anatomy alone Annotate \"skin\" in \"Apply to skin\" Skip unless symptom phrase Overlapping spans \"burning\" + \"burning sensation\" Choose longest, most specific"},{"location":"user-guide/annotation/#5-export-gold-labels","title":"5. Export Gold Labels","text":""},{"location":"user-guide/annotation/#from-label-studio-ui","title":"From Label Studio UI","text":"<ol> <li>Project Dashboard \u2192 Export</li> <li>Format: JSON</li> <li>Download \u2192 <code>annotations.json</code></li> <li>Save to: <code>data/annotation/exports/raw/</code></li> </ol>"},{"location":"user-guide/annotation/#convert-to-training-format","title":"Convert to Training Format","text":"<pre><code>import json\n\n# Load raw export\nwith open(\"data/annotation/exports/raw/annotations.json\") as f:\n    raw_annotations = json.load(f)\n\n# Convert to gold JSONL\ngold_path = \"data/annotation/exports/gold_annotations.jsonl\"\nwith open(gold_path, \"w\") as f:\n    for task in raw_annotations:\n        # Extract annotations\n        annotations = task.get(\"annotations\", [])\n        if not annotations:\n            continue  # Skip unannotated\n\n        # Use first annotation (or consensus if multiple)\n        result = annotations[0][\"result\"]\n\n        # Convert to entity format\n        entities = []\n        for span in result:\n            value = span[\"value\"]\n            entities.append({\n                \"text\": value[\"text\"],\n                \"start\": value[\"start\"],\n                \"end\": value[\"end\"],\n                \"label\": value[\"labels\"][0],\n                \"annotator\": annotations[0].get(\"completed_by\", \"unknown\"),\n                \"timestamp\": annotations[0].get(\"created_at\", \"\"),\n            })\n\n        # Write gold entry\n        gold_entry = {\n            \"id\": task[\"id\"],\n            \"text\": task[\"data\"][\"text\"],\n            \"entities\": entities,\n            \"source\": \"label_studio\",\n            \"batch\": \"batch_001\",\n        }\n        f.write(json.dumps(gold_entry) + \"\\n\")\n\nprint(f\"Converted {len(raw_annotations)} tasks to {gold_path}\")\n</code></pre>"},{"location":"user-guide/annotation/#6-quality-assurance","title":"6. Quality Assurance","text":""},{"location":"user-guide/annotation/#coverage-report","title":"Coverage Report","text":"<pre><code>import pandas as pd\n\n# Load gold annotations\ngold = pd.read_json(\"data/annotation/exports/gold_annotations.jsonl\", lines=True)\n\n# Compute coverage\ntotal_tasks = len(gold)\nwith_symptoms = gold[\"entities\"].apply(lambda e: any(ent[\"label\"] == \"SYMPTOM\" for ent in e)).sum()\nwith_products = gold[\"entities\"].apply(lambda e: any(ent[\"label\"] == \"PRODUCT\" for ent in e)).sum()\nempty = gold[\"entities\"].apply(lambda e: len(e) == 0).sum()\n\nprint(f\"Total tasks: {total_tasks}\")\nprint(f\"Tasks with SYMPTOM: {with_symptoms} ({with_symptoms/total_tasks*100:.1f}%)\")\nprint(f\"Tasks with PRODUCT: {with_products} ({with_products/total_tasks*100:.1f}%)\")\nprint(f\"Tasks with no entities: {empty} ({empty/total_tasks*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/annotation/#inter-annotator-agreement","title":"Inter-Annotator Agreement","text":"<p>For overlapping annotations (multiple annotators per task):</p> <pre><code>from itertools import combinations\n\ndef compute_iou(span1, span2):\n    \"\"\"Compute IOU between two spans.\"\"\"\n    start = max(span1[\"start\"], span2[\"start\"])\n    end = min(span1[\"end\"], span2[\"end\"])\n    intersection = max(0, end - start)\n    union = (span1[\"end\"] - span1[\"start\"]) + (span2[\"end\"] - span2[\"start\"]) - intersection\n    return intersection / union if union &gt; 0 else 0.0\n\ndef compute_agreement(annotations1, annotations2):\n    \"\"\"Compute agreement between two annotators.\"\"\"\n    total_matches = 0\n    total_entities = len(annotations1) + len(annotations2)\n\n    for ann1 in annotations1:\n        for ann2 in annotations2:\n            if compute_iou(ann1, ann2) &gt;= 0.5 and ann1[\"label\"] == ann2[\"label\"]:\n                total_matches += 1\n                break\n\n    return 2 * total_matches / total_entities if total_entities &gt; 0 else 0.0\n\n# Load multi-annotated tasks\ntasks_multi = [task for task in raw_annotations if len(task.get(\"annotations\", [])) &gt;= 2]\n\n# Compute pairwise agreement\nagreements = []\nfor task in tasks_multi:\n    anns = task[\"annotations\"]\n    for ann1, ann2 in combinations(anns, 2):\n        entities1 = [s[\"value\"] for s in ann1[\"result\"]]\n        entities2 = [s[\"value\"] for s in ann2[\"result\"]]\n        agreement = compute_agreement(entities1, entities2)\n        agreements.append(agreement)\n\navg_agreement = sum(agreements) / len(agreements) if agreements else 0.0\nprint(f\"Average inter-annotator agreement (IOU \u2265 0.5): {avg_agreement:.2%}\")\n</code></pre>"},{"location":"user-guide/annotation/#integrity-tests","title":"Integrity Tests","text":"<pre><code>def validate_gold_annotations(gold_path):\n    \"\"\"Validate gold annotation integrity.\"\"\"\n    errors = []\n\n    with open(gold_path) as f:\n        for i, line in enumerate(f, start=1):\n            doc = json.loads(line)\n\n            # Check required fields\n            if \"text\" not in doc or \"entities\" not in doc:\n                errors.append(f\"Line {i}: Missing required fields\")\n                continue\n\n            text = doc[\"text\"]\n\n            # Check entity integrity\n            for j, entity in enumerate(doc[\"entities\"]):\n                # Check bounds\n                if entity[\"start\"] &lt; 0 or entity[\"end\"] &gt; len(text):\n                    errors.append(f\"Line {i}, entity {j}: Out of bounds\")\n\n                # Check text slice\n                expected_text = text[entity[\"start\"]:entity[\"end\"]]\n                if entity[\"text\"] != expected_text:\n                    errors.append(f\"Line {i}, entity {j}: Text mismatch\")\n\n                # Check label\n                if entity[\"label\"] not in [\"SYMPTOM\", \"PRODUCT\"]:\n                    errors.append(f\"Line {i}, entity {j}: Invalid label\")\n\n    if errors:\n        print(f\"Found {len(errors)} errors:\")\n        for error in errors[:10]:  # Show first 10\n            print(f\"  - {error}\")\n    else:\n        print(\"\u2713 All annotations valid\")\n\nvalidate_gold_annotations(\"data/annotation/exports/gold_annotations.jsonl\")\n</code></pre>"},{"location":"user-guide/annotation/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"user-guide/annotation/#consensus-annotation","title":"Consensus Annotation","text":"<p>For high-quality gold labels, use multiple annotators + consensus:</p> <pre><code>def consensus_annotation(task_annotations):\n    \"\"\"\n    Build consensus from multiple annotations.\n\n    Args:\n        task_annotations: List of annotation dicts\n\n    Returns:\n        Consensus entities (majority vote)\n    \"\"\"\n    from collections import defaultdict\n\n    # Group overlapping spans\n    span_groups = defaultdict(list)\n    for ann in task_annotations:\n        for entity in ann[\"result\"]:\n            value = entity[\"value\"]\n            key = (value[\"start\"], value[\"end\"], value[\"labels\"][0])\n            span_groups[key].append(value)\n\n    # Majority vote (\u226550% agreement)\n    consensus = []\n    min_agree = len(task_annotations) // 2 + 1\n    for key, spans in span_groups.items():\n        if len(spans) &gt;= min_agree:\n            consensus.append({\n                \"start\": key[0],\n                \"end\": key[1],\n                \"label\": key[2],\n                \"text\": spans[0][\"text\"],\n                \"votes\": len(spans)\n            })\n\n    return consensus\n\n# Apply to multi-annotated tasks\nfor task in tasks_multi:\n    consensus = consensus_annotation(task[\"annotations\"])\n    print(f\"Task {task['id']}: {len(consensus)} consensus spans\")\n</code></pre>"},{"location":"user-guide/annotation/#active-learning","title":"Active Learning","text":"<p>Prioritize uncertain examples for annotation:</p> <pre><code>from src.pipeline import simple_inference\n\n# Generate weak labels with confidence\nresults = simple_inference(texts)\n\n# Find low-confidence cases\nuncertain = [\n    result for result in results\n    if any(0.65 &lt;= e[\"confidence\"] &lt; 0.80 for e in result[\"entities\"])\n]\n\n# Export for annotation\nexport_to_label_studio(uncertain, \"data/annotation/exports/uncertain_batch.jsonl\")\n</code></pre>"},{"location":"user-guide/annotation/#annotation-drift-detection","title":"Annotation Drift Detection","text":"<p>Monitor annotation consistency over time:</p> <pre><code>import pandas as pd\n\n# Load annotations with timestamps\ngold = pd.read_json(\"data/annotation/exports/gold_annotations.jsonl\", lines=True)\ngold[\"timestamp\"] = pd.to_datetime(gold[\"entities\"].apply(lambda e: e[0][\"timestamp\"] if e else None))\n\n# Group by week\ngold[\"week\"] = gold[\"timestamp\"].dt.isocalendar().week\n\n# Compute weekly stats\nweekly = gold.groupby(\"week\").agg({\n    \"entities\": lambda x: sum(len(e) for e in x),  # Total entities\n}).reset_index()\n\nweekly[\"symptom_rate\"] = gold.groupby(\"week\")[\"entities\"].apply(\n    lambda x: sum(sum(1 for e in entities if e[\"label\"] == \"SYMPTOM\") for entities in x) / max(1, sum(len(e) for e in x))\n).values\n\nprint(weekly)\n\n# Flag drift: sudden &gt;20% change in symptom_rate\nfor i in range(1, len(weekly)):\n    change = abs(weekly.loc[i, \"symptom_rate\"] - weekly.loc[i-1, \"symptom_rate\"])\n    if change &gt; 0.20:\n        print(f\"\u26a0 Drift detected in week {weekly.loc[i, 'week']}: {change:.1%} change\")\n</code></pre>"},{"location":"user-guide/annotation/#best-practices","title":"Best Practices","text":"<ol> <li>Calibration round - annotate 50-100 samples, discuss disagreements, update guidelines</li> <li>Regular breaks - avoid fatigue (max 2 hours continuous annotation)</li> <li>Randomize order - prevent ordering bias</li> <li>Double annotation - 10-20% overlap for agreement monitoring</li> <li>Version guidelines - update docs as edge cases emerge</li> <li>Track provenance - record annotator, timestamp, batch ID</li> <li>Audit regularly - check for drift, errors, inconsistencies</li> </ol>"},{"location":"user-guide/annotation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/annotation/#issue-label-studio-connection-refused","title":"Issue: Label Studio connection refused","text":"<p>Solutions: <pre><code># Check if server running\nnetstat -an | findstr \"8080\"\n\n# Restart server\nlabel-studio start --host localhost --port 8080\n\n# Check firewall\n# Ensure localhost:8080 not blocked\n</code></pre></p>"},{"location":"user-guide/annotation/#issue-import-fails","title":"Issue: Import fails","text":"<p>Solutions: - Check JSONL format (one valid JSON per line) - Validate JSON: <code>python -m json.tool weak_labels.jsonl</code> - Ensure <code>data.text</code> field present - Check predictions structure matches label config</p>"},{"location":"user-guide/annotation/#issue-low-inter-annotator-agreement","title":"Issue: Low inter-annotator agreement","text":"<p>Solutions: - Review guidelines with annotators - Conduct calibration session - Add more examples to guidelines - Clarify ambiguous cases (anatomy, negation) - Consider majority vote consensus</p>"},{"location":"user-guide/annotation/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Understand weak labels</li> <li>Pipeline Guide - Generate annotations</li> <li>Configuration - Tune weak labeling</li> <li>Development: Testing - Test annotation pipeline</li> </ul>"},{"location":"user-guide/negation/","title":"Negation Detection Guide","text":"<p>Comprehensive guide to negation detection in SpanForge.</p>"},{"location":"user-guide/negation/#overview","title":"Overview","text":"<p>Negation detection identifies spans that are mentioned but explicitly denied or ruled out by the text. Critical for:</p> <ul> <li>Clinical accuracy - distinguish present vs. absent symptoms</li> <li>Adverse event classification - separate actual AEs from negative history</li> <li>Model training - teach models negation scope</li> <li>Quality control - flag potential annotation errors</li> </ul>"},{"location":"user-guide/negation/#architecture","title":"Architecture","text":"<p>SpanForge implements bidirectional negation scope detection with configurable windows.</p> <pre><code>graph LR\n    A[Text] --&gt; B[Detect Negation Cues]\n    B --&gt; C[Forward Scope]\n    B --&gt; D[Backward Scope]\n    C --&gt; E[Mark Spans]\n    D --&gt; E\n    E --&gt; F[Negated Spans]</code></pre>"},{"location":"user-guide/negation/#negation-tokens","title":"Negation Tokens","text":""},{"location":"user-guide/negation/#standard-negation-cues","title":"Standard Negation Cues","text":"<pre><code>NEGATION_TOKENS = {\n    # Direct negation\n    \"no\", \"not\", \"none\", \"never\", \"neither\", \"nor\",\n\n    # Clinical negation\n    \"denies\", \"denied\", \"negative\", \"absent\", \"absence\", \"absence of\",\n    \"free\", \"free of\", \"without\", \"fails to\", \"failed to\",\n\n    # Rule-out language\n    \"rule out\", \"ruled out\", \"r/o\", \"ruling out\",\n\n    # Medical descriptors\n    \"unremarkable\", \"non\", \"non-\",\n\n    # Temporal negation\n    \"no longer\", \"no more\", \"ceased\"\n}\n</code></pre>"},{"location":"user-guide/negation/#negation-categories","title":"Negation Categories","text":"Category Examples Use Case Direct no, not, never General negation Clinical denies, absent, negative Medical reports Rule-out r/o, rule out Differential diagnosis Descriptor unremarkable, non- Test results Temporal no longer, ceased Status changes"},{"location":"user-guide/negation/#detection-algorithm","title":"Detection Algorithm","text":""},{"location":"user-guide/negation/#bidirectional-scope","title":"Bidirectional Scope","text":"<pre><code>def is_negated(text: str, span_start: int, span_end: int, window: int = 5) -&gt; bool:\n    \"\"\"\n    Detect if span is negated (bidirectional).\n\n    Args:\n        text: Full text\n        span_start: Span start character offset\n        span_end: Span end character offset\n        window: Scope in tokens (default: 5)\n\n    Returns:\n        True if negated\n    \"\"\"\n    tokens = text.split()\n    span_text = text[span_start:span_end]\n    span_tokens = span_text.split()\n\n    # Locate span in token stream\n    span_token_start = len(text[:span_start].split())\n    span_token_end = span_token_start + len(span_tokens)\n\n    # Forward: negation cue BEFORE span\n    forward_start = max(0, span_token_start - window)\n    for i in range(forward_start, span_token_start):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    # Backward: negation cue AFTER span\n    backward_end = min(len(tokens), span_token_end + window)\n    for i in range(span_token_end, backward_end):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    return False\n</code></pre>"},{"location":"user-guide/negation/#window-mechanics","title":"Window Mechanics","text":"<p>Forward Negation: <pre><code>Token indices: [0,   1,       2,  3,      4,       5]\nText:          \"No  history  of  severe  itching  today\"\n               ^^^           ^^  ^^^^^^^^^^^^^^\n               cue           |   span [3:5]\n               |             |\n               |&lt;--window---&gt;|\n               [0:3]\n</code></pre> - Span tokens: [3, 5) = \"severe itching\" - Forward window: [0, 3) = \"No history of\" - Negation cue \"No\" at index 0 \u2208 [0, 3) \u2192 NEGATED</p> <p>Backward Negation: <pre><code>Token indices: [0,      1,   2,      3,  4]\nText:          \"Itching  was  denied  by  patient\"\n               ^^^^^^^^      ^^^^^^\n               span [0:1]    cue\n                        |&lt;-window-&gt;|\n                        [1:4]\n</code></pre> - Span tokens: [0, 1) = \"Itching\" - Backward window: [1, 6) = \"was denied by patient\" - Negation cue \"denied\" at index 2 \u2208 [1, 6) \u2192 NEGATED</p>"},{"location":"user-guide/negation/#examples","title":"Examples","text":""},{"location":"user-guide/negation/#forward-negation","title":"Forward Negation","text":"<pre><code># Simple forward negation\ntext = \"No itching reported\"\nspan = {\"text\": \"itching\", \"start\": 3, \"end\": 10}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (cue \"No\" at token 0, span at token 1, distance=1 \u2264 window)\n\n# Multi-word forward negation\ntext = \"Patient denies severe burning sensation\"\nspan = {\"text\": \"severe burning sensation\", \"start\": 15, \"end\": 39}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (cue \"denies\" at token 1, span at tokens 2-4, distance=1 \u2264 window)\n\n# Out-of-scope forward\ntext = \"Patient denies fever but reports itching\"\nspan = {\"text\": \"itching\", \"start\": 37, \"end\": 44}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: False (cue \"denies\" at token 1, span at token 6, distance=5 &gt; window)\n</code></pre>"},{"location":"user-guide/negation/#backward-negation","title":"Backward Negation","text":"<pre><code># Simple backward negation\ntext = \"Itching was denied\"\nspan = {\"text\": \"Itching\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (span at token 0, cue \"denied\" at token 2, distance=2 \u2264 window)\n\n# Clinical backward negation\ntext = \"Redness and swelling were unremarkable\"\nspan = {\"text\": \"Redness\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (span at token 0, cue \"unremarkable\" at token 4, distance=4 \u2264 window)\n\n# Out-of-scope backward\ntext = \"Redness present but unrelated to drug. Denied taking medication.\"\nspan = {\"text\": \"Redness\", \"start\": 0, \"end\": 7}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: False (span at token 0, cue \"Denied\" at token 9, distance=9 &gt; window)\n</code></pre>"},{"location":"user-guide/negation/#complex-patterns","title":"Complex Patterns","text":"<pre><code># Conjunction scope\ntext = \"No history of itching or redness\"\nspan_itching = {\"text\": \"itching\", \"start\": 14, \"end\": 21}\nspan_redness = {\"text\": \"redness\", \"start\": 25, \"end\": 32}\nis_negated(text, span_itching[\"start\"], span_itching[\"end\"], window=5)\n# Result: True (forward negation)\nis_negated(text, span_redness[\"start\"], span_redness[\"end\"], window=5)\n# Result: True (forward negation, scope extends through \"or\")\n\n# Negation + affirmation\ntext = \"No burning but does have itching\"\nspan_burning = {\"text\": \"burning\", \"start\": 3, \"end\": 10}\nspan_itching = {\"text\": \"itching\", \"start\": 25, \"end\": 32}\nis_negated(text, span_burning[\"start\"], span_burning[\"end\"], window=5)\n# Result: True (forward negation)\nis_negated(text, span_itching[\"start\"], span_itching[\"end\"], window=5)\n# Result: False (affirmation \"does have\" breaks negation scope)\n\n# Rule-out language\ntext = \"Rule out severe allergic reaction\"\nspan = {\"text\": \"severe allergic reaction\", \"start\": 9, \"end\": 33}\nis_negated(text, span[\"start\"], span[\"end\"], window=5)\n# Result: True (forward negation, multi-word cue \"rule out\")\n</code></pre>"},{"location":"user-guide/negation/#window-tuning","title":"Window Tuning","text":""},{"location":"user-guide/negation/#precision-vs-recall-trade-off","title":"Precision vs. Recall Trade-off","text":"Window Size Precision Recall False Positives False Negatives Use Case 1-2 Very High Low Few Many Conservative, short sentences 3-4 High Medium Some Some Balanced, standard grammar 5 (default) Balanced Balanced Moderate Moderate General use 6-7 Medium High Many Few Long sentences, complex syntax 8-10 Low Very High Very Many Very Few Exploratory, over-mark"},{"location":"user-guide/negation/#tuning-recommendations","title":"Tuning Recommendations","text":"<pre><code>from src.config import AppConfig\n\n# Conservative: short-range negation only\nconfig_conservative = AppConfig(negation_window=3)\n\n# Standard: balanced precision/recall (recommended)\nconfig_standard = AppConfig(negation_window=5)\n\n# Aggressive: long-range negation\nconfig_aggressive = AppConfig(negation_window=7)\n\n# Exploratory: catch all possible negations\nconfig_exploratory = AppConfig(negation_window=10)\n</code></pre>"},{"location":"user-guide/negation/#empirical-tuning","title":"Empirical Tuning","text":"<pre><code>import pandas as pd\nfrom src.weak_label import match_symptoms\n\n# Load gold annotations\ngold = pd.read_csv(\"gold_annotations.csv\")\n\n# Evaluate different windows\nfor window in [3, 5, 7, 10]:\n    config = AppConfig(negation_window=window)\n\n    # Run weak labeling\n    predictions = []\n    for text in gold[\"text\"]:\n        spans = match_symptoms(text, lexicon)\n        predictions.extend(spans)\n\n    # Compute metrics\n    metrics = evaluate_negation(gold, predictions)\n    print(f\"Window={window}: P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, F1={metrics['f1']:.2f}\")\n\n# Expected output:\n# Window=3: P=0.92, R=0.78, F1=0.84\n# Window=5: P=0.88, R=0.85, F1=0.86  \u2190 Best F1\n# Window=7: P=0.82, R=0.89, F1=0.85\n# Window=10: P=0.74, R=0.93, F1=0.82\n</code></pre>"},{"location":"user-guide/negation/#edge-cases","title":"Edge Cases","text":""},{"location":"user-guide/negation/#affirmative-overrides","title":"Affirmative Overrides","text":"<pre><code># Affirmation breaks negation scope\ntext = \"No fever but does have itching\"\n# \"itching\" is NOT negated (affirmation \"does have\" resets scope)\n\n# Implementation: Track affirmative cues\nAFFIRMATIVE_CUES = {\"does have\", \"has\", \"reports\", \"complains of\", \"presents with\"}\n# TODO: Not yet implemented; planned for future release\n</code></pre>"},{"location":"user-guide/negation/#double-negation","title":"Double Negation","text":"<pre><code># Double negation = affirmation\ntext = \"Patient does not deny itching\"\n# \"itching\" is AFFIRMED (logically)\n# Current system: Marks as NEGATED (structural negation only)\n# TODO: Semantic negation resolution planned\n</code></pre>"},{"location":"user-guide/negation/#negation-boundaries","title":"Negation Boundaries","text":"<pre><code># Sentence boundaries reset scope\ntext = \"No fever. Patient has itching.\"\n# \"itching\" is NOT negated (new sentence resets scope)\n\n# Implementation: Sentence splitting\nsentences = text.split('. ')\nfor sent in sentences:\n    # Process each sentence independently\n    spans = match_symptoms(sent, lexicon)\n</code></pre>"},{"location":"user-guide/negation/#integration-with-pipeline","title":"Integration with Pipeline","text":""},{"location":"user-guide/negation/#weak-labeling","title":"Weak Labeling","text":"<pre><code>from src.weak_label import match_symptoms\nfrom src.config import AppConfig\n\nconfig = AppConfig(negation_window=5)\ntext = \"No history of itching or redness\"\n\n# Detect symptoms\nspans = match_symptoms(text, lexicon)\n\n# Filter by negation status\npositive_symptoms = [s for s in spans if not s.get(\"negated\", False)]\nnegated_symptoms = [s for s in spans if s.get(\"negated\", False)]\n\nprint(f\"Positive: {positive_symptoms}\")  # []\nprint(f\"Negated: {negated_symptoms}\")    # [\"itching\", \"redness\"]\n</code></pre>"},{"location":"user-guide/negation/#pipeline-inference","title":"Pipeline Inference","text":"<pre><code>from src.pipeline import simple_inference\n\ntext = \"Patient denies burning but reports redness\"\n\n# Run pipeline (includes negation detection)\nresults = simple_inference([text])\n\n# Check negation flags\nfor span in results[0][\"entities\"]:\n    print(f\"{span['text']}: negated={span.get('negated', False)}\")\n\n# Output:\n# burning: negated=True\n# redness: negated=False\n</code></pre>"},{"location":"user-guide/negation/#annotation-export","title":"Annotation Export","text":"<pre><code>from src.weak_label import match_symptoms\nimport json\n\n# Generate weak labels with negation\ntexts = [\"No itching reported\", \"Patient has redness\"]\nresults = []\n\nfor text in texts:\n    spans = match_symptoms(text, lexicon)\n    results.append({\n        \"text\": text,\n        \"entities\": [\n            {\n                \"text\": s[\"text\"],\n                \"start\": s[\"start\"],\n                \"end\": s[\"end\"],\n                \"label\": s[\"label\"],\n                \"negated\": s.get(\"negated\", False),\n                \"confidence\": s[\"confidence\"]\n            }\n            for s in spans\n        ]\n    })\n\n# Save for Label Studio\nwith open(\"weak_labels_with_negation.jsonl\", \"w\") as f:\n    for result in results:\n        f.write(json.dumps(result) + \"\\n\")\n</code></pre>"},{"location":"user-guide/negation/#best-practices","title":"Best Practices","text":"<ol> <li>Always export negation flags - critical for downstream models</li> <li>Use default window (5) - well-calibrated for most cases</li> <li>Tune on evaluation set - measure P/R on gold negations</li> <li>Handle sentence boundaries - split long texts</li> <li>Document custom negation tokens - track domain-specific cues</li> <li>Validate with clinicians - ensure negation patterns match domain</li> <li>Track negation statistics - monitor % negated spans</li> </ol>"},{"location":"user-guide/negation/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/negation/#issue-over-marking-negations","title":"Issue: Over-marking negations","text":"<p>Symptoms: - Many false positive negations - Affirmative spans marked as negated</p> <p>Solutions: <pre><code># Reduce window\nconfig = AppConfig(negation_window=3)\n\n# Audit NEGATION_TOKENS for overly broad terms\n# Remove: \"non\" (captures \"non-prescription\", \"non-stop\")\n\n# Add sentence splitting\nsentences = text.split('. ')\n</code></pre></p>"},{"location":"user-guide/negation/#issue-under-marking-negations","title":"Issue: Under-marking negations","text":"<p>Symptoms: - Negated spans not detected - Complex negation patterns missed</p> <p>Solutions: <pre><code># Increase window\nconfig = AppConfig(negation_window=7)\n\n# Add domain-specific negation tokens\nNEGATION_TOKENS.update([\"absence of\", \"free of\", \"no evidence of\"])\n\n# Enable backward negation (already default)\n</code></pre></p>"},{"location":"user-guide/negation/#issue-negation-crosses-sentence-boundaries","title":"Issue: Negation crosses sentence boundaries","text":"<p>Symptoms: - Negation scope bleeds into next sentence</p> <p>Solutions: <pre><code># Sentence splitting before negation detection\ndef split_sentences(text):\n    \"\"\"Split on sentence boundaries.\"\"\"\n    return text.replace('! ', '!|').replace('? ', '?|').replace('. ', '.|').split('|')\n\nsentences = split_sentences(text)\nall_spans = []\noffset = 0\nfor sent in sentences:\n    spans = match_symptoms(sent, lexicon)\n    # Adjust offsets\n    for span in spans:\n        span[\"start\"] += offset\n        span[\"end\"] += offset\n    all_spans.extend(spans)\n    offset += len(sent) + 1  # +1 for delimiter\n</code></pre></p>"},{"location":"user-guide/negation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/negation/#negation-specific-metrics","title":"Negation-Specific Metrics","text":"<pre><code>def evaluate_negation(gold_annotations, predicted_spans):\n    \"\"\"\n    Evaluate negation detection performance.\n\n    Args:\n        gold_annotations: Gold spans with negation flags\n        predicted_spans: Predicted spans with negation flags\n\n    Returns:\n        Dict with negation precision/recall/F1\n    \"\"\"\n    true_pos_neg = 0   # Correctly marked as negated\n    false_pos_neg = 0  # Incorrectly marked as negated\n    false_neg_neg = 0  # Should be negated, but not marked\n\n    for pred in predicted_spans:\n        # Find matching gold span (IOU \u2265 0.5)\n        gold_match = find_matching_gold(pred, gold_annotations)\n        if gold_match:\n            if pred.get(\"negated\", False) and gold_match.get(\"negated\", False):\n                true_pos_neg += 1\n            elif pred.get(\"negated\", False) and not gold_match.get(\"negated\", False):\n                false_pos_neg += 1\n            elif not pred.get(\"negated\", False) and gold_match.get(\"negated\", False):\n                false_neg_neg += 1\n\n    precision = true_pos_neg / (true_pos_neg + false_pos_neg)\n    recall = true_pos_neg / (true_pos_neg + false_neg_neg)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n</code></pre>"},{"location":"user-guide/negation/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Full weak labeling system</li> <li>Configuration - Tune negation window</li> <li>API Reference - Function documentation</li> </ul>"},{"location":"user-guide/pipeline/","title":"Pipeline Guide","text":"<p>Comprehensive guide to SpanForge's end-to-end inference pipeline.</p>"},{"location":"user-guide/pipeline/#overview","title":"Overview","text":"<p>The pipeline combines BioBERT contextual embeddings with lexicon-based weak labeling for biomedical NER. Processes raw text through tokenization, entity detection, and optional persistence.</p> <pre><code>graph TB\n    A[Raw Text] --&gt; B[Tokenization]\n    B --&gt; C[BioBERT Encoding]\n    C --&gt; D[Weak Label Matching]\n    D --&gt; E[Negation Detection]\n    E --&gt; F[Confidence Scoring]\n    F --&gt; G[Entity Spans]\n    G --&gt; H[Optional JSONL Export]</code></pre>"},{"location":"user-guide/pipeline/#architecture","title":"Architecture","text":""},{"location":"user-guide/pipeline/#components","title":"Components","text":"Component Module Function Purpose Tokenizer <code>src.model</code> <code>get_tokenizer()</code> Load BioBERT tokenizer Model <code>src.model</code> <code>get_model()</code> Load BioBERT encoder Encoder <code>src.model</code> <code>encode_text()</code> Generate token embeddings Weak Labeler <code>src.weak_label</code> <code>match_symptoms()</code>, <code>match_products()</code> Lexicon-based entity detection Negation <code>src.weak_label</code> <code>is_negated()</code> Bidirectional negation scope Pipeline <code>src.pipeline</code> <code>simple_inference()</code> Orchestrate end-to-end"},{"location":"user-guide/pipeline/#data-flow","title":"Data Flow","text":"<pre><code>text = \"Patient reports severe itching\"\n\n# 1. Tokenization\ntokens = tokenizer(text)\n# {'input_ids': [101, 5317, 3756, 5729, 24501, 102], ...}\n\n# 2. BioBERT Encoding\nencodings = model(**tokens)\n# {last_hidden_state: tensor([...]), ...}\n\n# 3. Weak Labeling\nsymptoms = match_symptoms(text, symptom_lexicon)\nproducts = match_products(text, product_lexicon)\n# [{'text': 'severe itching', 'start': 16, 'end': 30, ...}]\n\n# 4. Negation Detection\nfor span in symptoms:\n    span['negated'] = is_negated(text, span['start'], span['end'])\n\n# 5. Output\nresult = {\n    'text': text,\n    'entities': symptoms + products\n}\n</code></pre>"},{"location":"user-guide/pipeline/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/pipeline/#basic-inference","title":"Basic Inference","text":"<pre><code>from src.pipeline import simple_inference\n\n# Single text\ntext = \"Patient experienced burning sensation after using Product X\"\nresults = simple_inference([text])\n\n# Access entities\nfor entity in results[0][\"entities\"]:\n    print(f\"{entity['label']}: {entity['text']} (confidence: {entity['confidence']:.2f})\")\n\n# Output:\n# SYMPTOM: burning sensation (confidence: 0.91)\n# PRODUCT: Product X (confidence: 0.95)\n</code></pre>"},{"location":"user-guide/pipeline/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.pipeline import simple_inference\n\n# Multiple texts\ntexts = [\n    \"Patient has severe redness\",\n    \"No itching reported\",\n    \"Used Lotion Y with no adverse effects\"\n]\n\n# Process batch\nresults = simple_inference(texts)\n\n# Iterate results\nfor i, result in enumerate(results):\n    print(f\"\\nText {i+1}: {result['text']}\")\n    for entity in result[\"entities\"]:\n        negated = \" (negated)\" if entity.get(\"negated\", False) else \"\"\n        print(f\"  - {entity['label']}: {entity['text']}{negated}\")\n\n# Output:\n# Text 1: Patient has severe redness\n#   - SYMPTOM: severe redness\n# Text 2: No itching reported\n#   - SYMPTOM: itching (negated)\n# Text 3: Used Lotion Y with no adverse effects\n#   - PRODUCT: Lotion Y\n</code></pre>"},{"location":"user-guide/pipeline/#jsonl-persistence","title":"JSONL Persistence","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = load_texts(\"complaints.csv\")\n\n# Export to JSONL\noutput_path = \"data/output/entities.jsonl\"\nresults = simple_inference(texts, output_jsonl=output_path)\n\nprint(f\"Saved {len(results)} results to {output_path}\")\n\n# Read back\nimport json\nwith open(output_path) as f:\n    for line in f:\n        doc = json.loads(line)\n        print(f\"Text: {doc['text']}\")\n        print(f\"Entities: {len(doc['entities'])}\")\n</code></pre>"},{"location":"user-guide/pipeline/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/pipeline/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from src.config import AppConfig\nfrom src.pipeline import simple_inference\n\n# Custom config\nconfig = AppConfig(\n    negation_window=7,\n    fuzzy_scorer=\"jaccard\",\n    max_seq_len=512,\n    device=\"cuda\"\n)\n\n# Use in pipeline (config automatically loaded via get_config())\nresults = simple_inference(texts)\n</code></pre>"},{"location":"user-guide/pipeline/#filtering-entities","title":"Filtering Entities","text":"<pre><code># High-confidence entities only\nhigh_conf = [\n    entity for entity in result[\"entities\"]\n    if entity[\"confidence\"] &gt;= 0.85\n]\n\n# Non-negated symptoms\npositive_symptoms = [\n    entity for entity in result[\"entities\"]\n    if entity[\"label\"] == \"SYMPTOM\" and not entity.get(\"negated\", False)\n]\n\n# Products with symptoms (co-occurrence)\nhas_both = any(e[\"label\"] == \"SYMPTOM\" for e in result[\"entities\"]) and \\\n           any(e[\"label\"] == \"PRODUCT\" for e in result[\"entities\"])\n</code></pre>"},{"location":"user-guide/pipeline/#entity-grouping","title":"Entity Grouping","text":"<pre><code>from collections import defaultdict\n\n# Group by label\ngrouped = defaultdict(list)\nfor entity in result[\"entities\"]:\n    grouped[entity[\"label\"]].append(entity)\n\nprint(f\"Symptoms: {len(grouped['SYMPTOM'])}\")\nprint(f\"Products: {len(grouped['PRODUCT'])}\")\n\n# Group by canonical form\ncanonical_groups = defaultdict(list)\nfor entity in result[\"entities\"]:\n    canonical_groups[entity[\"canonical\"]].append(entity)\n\n# Find duplicates\nduplicates = {k: v for k, v in canonical_groups.items() if len(v) &gt; 1}\n</code></pre>"},{"location":"user-guide/pipeline/#confidence-histograms","title":"Confidence Histograms","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Collect confidences\nconfidences = [e[\"confidence\"] for result in results for e in result[\"entities\"]]\n\n# Plot distribution\nplt.hist(confidences, bins=20, range=(0, 1.0), edgecolor='black')\nplt.xlabel(\"Confidence Score\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Entity Confidence Distribution\")\nplt.axvline(x=0.85, color='r', linestyle='--', label='Threshold')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/pipeline/#pipeline-components","title":"Pipeline Components","text":""},{"location":"user-guide/pipeline/#1-tokenization","title":"1. Tokenization","text":"<pre><code>from src.model import get_tokenizer, encode_text\n\ntokenizer = get_tokenizer()\ntext = \"Patient has severe burning sensation\"\n\n# Tokenize\ntokens = tokenizer(\n    text,\n    padding=\"max_length\",\n    truncation=True,\n    max_length=256,\n    return_tensors=\"pt\"\n)\n\nprint(tokens[\"input_ids\"])\n# tensor([[  101,  5317,  2038,  5729, 10566,  8006,   102,     0,     0, ...]])\n\n# Token IDs to text\ndecoded = tokenizer.decode(tokens[\"input_ids\"][0])\nprint(decoded)\n# [CLS] Patient has severe burning sensation [SEP] [PAD] [PAD] ...\n</code></pre>"},{"location":"user-guide/pipeline/#2-biobert-encoding","title":"2. BioBERT Encoding","text":"<pre><code>from src.model import get_model, encode_text\n\nmodel = get_model()\ntext = \"Patient has severe burning sensation\"\n\n# Encode\nencoding = encode_text(text)\n\nprint(encoding.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\nprint(encoding[\"input_ids\"].shape)\n# torch.Size([1, 256])\n\n# Get embeddings (requires model forward pass)\nwith torch.no_grad():\n    outputs = model(**encoding)\n    embeddings = outputs.last_hidden_state  # [1, 256, 768]\n\nprint(f\"Embedding shape: {embeddings.shape}\")\n# Embedding shape: torch.Size([1, 256, 768])\n</code></pre>"},{"location":"user-guide/pipeline/#3-weak-labeling","title":"3. Weak Labeling","text":"<pre><code>from src.weak_label import match_symptoms, match_products\nimport pandas as pd\n\n# Load lexicons\nsymptoms = pd.read_csv(\"data/lexicon/symptoms.csv\")[\"symptom\"].tolist()\nproducts = pd.read_csv(\"data/lexicon/products.csv\")[\"product\"].tolist()\n\ntext = \"Patient used Lotion X and experienced severe itching\"\n\n# Detect entities\nsymptom_spans = match_symptoms(text, symptoms)\nproduct_spans = match_products(text, products)\n\nprint(f\"Symptoms: {[s['text'] for s in symptom_spans]}\")\nprint(f\"Products: {[p['text'] for p in product_spans]}\")\n\n# Output:\n# Symptoms: ['severe itching']\n# Products: ['Lotion X']\n</code></pre>"},{"location":"user-guide/pipeline/#4-negation-detection","title":"4. Negation Detection","text":"<pre><code>from src.weak_label import match_symptoms\nfrom src.config import AppConfig\n\nconfig = AppConfig(negation_window=5)\ntext = \"No history of itching or redness\"\n\n# Detect symptoms\nspans = match_symptoms(text, symptoms)\n\n# Check negation\nfor span in spans:\n    negated = span.get(\"negated\", False)\n    print(f\"{span['text']}: negated={negated}\")\n\n# Output:\n# itching: negated=True\n# redness: negated=True\n</code></pre>"},{"location":"user-guide/pipeline/#5-postprocessing","title":"5. Postprocessing","text":"<pre><code>from src.pipeline import postprocess_predictions\n\n# Predictions (mock)\npredictions = [\n    {\"text\": \"itching\", \"start\": 10, \"end\": 17, \"label\": \"SYMPTOM\", \"confidence\": 0.92},\n    {\"text\": \"itching\", \"start\": 10, \"end\": 17, \"label\": \"SYMPTOM\", \"confidence\": 0.88},  # duplicate\n    {\"text\": \"redness\", \"start\": 22, \"end\": 29, \"label\": \"SYMPTOM\", \"confidence\": 0.75},\n]\n\n# Deduplicate &amp; filter\nprocessed = postprocess_predictions(predictions, min_confidence=0.80)\n\nprint(f\"Original: {len(predictions)} spans\")\nprint(f\"Processed: {len(processed)} spans\")\n\n# Output:\n# Original: 3 spans\n# Processed: 1 spans (deduplicated, filtered by confidence)\n</code></pre>"},{"location":"user-guide/pipeline/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/pipeline/#cpu-optimization","title":"CPU Optimization","text":"<pre><code>from src.config import AppConfig\n\n# CPU-friendly config\nconfig = AppConfig(\n    device=\"cpu\",\n    max_seq_len=128,  # Shorter sequences\n)\n\n# Process in smaller batches\nbatch_size = 8\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i+batch_size]\n    results = simple_inference(batch)\n</code></pre>"},{"location":"user-guide/pipeline/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>from src.config import AppConfig\nimport torch\n\n# GPU config\nconfig = AppConfig(\n    device=\"cuda\",\n    max_seq_len=512,\n)\n\n# Enable cuDNN autotuner\ntorch.backends.cudnn.benchmark = True\n\n# Larger batches\nbatch_size = 64\nresults = simple_inference(texts[:batch_size])\n</code></pre>"},{"location":"user-guide/pipeline/#memory-management","title":"Memory Management","text":"<pre><code>import torch\n\n# Process very large datasets\nresults = []\nfor i, text in enumerate(texts):\n    result = simple_inference([text])\n    results.append(result[0])\n\n    # Clear cache every 100 texts\n    if i % 100 == 0:\n        torch.cuda.empty_cache()\n        print(f\"Processed {i}/{len(texts)}\")\n</code></pre>"},{"location":"user-guide/pipeline/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/pipeline/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from src.pipeline import simple_inference\n\ntexts = [\"Valid text\", \"\", None, \"Another valid text\"]\n\n# Handle errors\nresults = []\nfor text in texts:\n    try:\n        if not text:\n            raise ValueError(\"Empty text\")\n        result = simple_inference([text])\n        results.append(result[0])\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        results.append({\"text\": text, \"entities\": [], \"error\": str(e)})\n</code></pre>"},{"location":"user-guide/pipeline/#input-validation","title":"Input Validation","text":"<pre><code>def validate_input(text):\n    \"\"\"Validate input text.\"\"\"\n    if not isinstance(text, str):\n        raise TypeError(f\"Expected str, got {type(text)}\")\n    if not text.strip():\n        raise ValueError(\"Empty text\")\n    if len(text) &gt; 10000:\n        raise ValueError(\"Text too long (&gt;10,000 chars)\")\n    return text.strip()\n\n# Safe inference\nvalidated_texts = [validate_input(t) for t in texts]\nresults = simple_inference(validated_texts)\n</code></pre>"},{"location":"user-guide/pipeline/#integration-patterns","title":"Integration Patterns","text":""},{"location":"user-guide/pipeline/#stream-processing","title":"Stream Processing","text":"<pre><code>from src.pipeline import simple_inference\n\ndef process_stream(input_stream, output_stream, batch_size=32):\n    \"\"\"Process streaming data.\"\"\"\n    batch = []\n    for text in input_stream:\n        batch.append(text)\n        if len(batch) &gt;= batch_size:\n            results = simple_inference(batch)\n            for result in results:\n                output_stream.write(json.dumps(result) + \"\\n\")\n            batch = []\n\n    # Process remaining\n    if batch:\n        results = simple_inference(batch)\n        for result in results:\n            output_stream.write(json.dumps(result) + \"\\n\")\n\n# Usage\nwith open(\"input.txt\") as infile, open(\"output.jsonl\", \"w\") as outfile:\n    process_stream(infile, outfile)\n</code></pre>"},{"location":"user-guide/pipeline/#database-integration","title":"Database Integration","text":"<pre><code>import sqlite3\nfrom src.pipeline import simple_inference\n\n# Read from database\nconn = sqlite3.connect(\"complaints.db\")\ncursor = conn.execute(\"SELECT id, text FROM complaints WHERE processed = 0 LIMIT 1000\")\n\n# Process\nresults = []\nfor row_id, text in cursor:\n    result = simple_inference([text])[0]\n    results.append((row_id, json.dumps(result)))\n\n# Write back\nconn.executemany(\n    \"UPDATE complaints SET entities = ?, processed = 1 WHERE id = ?\",\n    [(entities, row_id) for row_id, entities in results]\n)\nconn.commit()\n</code></pre>"},{"location":"user-guide/pipeline/#rest-api","title":"REST API","text":"<pre><code>from flask import Flask, request, jsonify\nfrom src.pipeline import simple_inference\n\napp = Flask(__name__)\n\n@app.route(\"/extract\", methods=[\"POST\"])\ndef extract_entities():\n    \"\"\"Entity extraction endpoint.\"\"\"\n    data = request.get_json()\n    text = data.get(\"text\", \"\")\n\n    if not text:\n        return jsonify({\"error\": \"Missing text\"}), 400\n\n    # Extract entities\n    results = simple_inference([text])\n\n    return jsonify(results[0])\n\n# Run server\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000)\n\n# Test:\n# curl -X POST http://localhost:5000/extract \\\n#   -H \"Content-Type: application/json\" \\\n#   -d '{\"text\": \"Patient has severe itching\"}'\n</code></pre>"},{"location":"user-guide/pipeline/#testing","title":"Testing","text":""},{"location":"user-guide/pipeline/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom src.pipeline import simple_inference\n\ndef test_pipeline_basic():\n    \"\"\"Test basic pipeline inference.\"\"\"\n    text = \"Patient has severe itching\"\n    results = simple_inference([text])\n\n    assert len(results) == 1\n    assert results[0][\"text\"] == text\n    assert len(results[0][\"entities\"]) &gt; 0\n\ndef test_pipeline_empty():\n    \"\"\"Test empty input.\"\"\"\n    results = simple_inference([])\n    assert results == []\n\ndef test_pipeline_negation():\n    \"\"\"Test negation detection.\"\"\"\n    text = \"No itching reported\"\n    results = simple_inference([text])\n\n    entities = results[0][\"entities\"]\n    assert any(e[\"text\"] == \"itching\" and e.get(\"negated\", False) for e in entities)\n</code></pre>"},{"location":"user-guide/pipeline/#integration-tests","title":"Integration Tests","text":"<pre><code>import tempfile\nfrom src.pipeline import simple_inference\n\ndef test_pipeline_jsonl_export():\n    \"\"\"Test JSONL export.\"\"\"\n    texts = [\"Text 1\", \"Text 2\"]\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n        output_path = f.name\n\n    # Export\n    results = simple_inference(texts, output_jsonl=output_path)\n\n    # Read back\n    with open(output_path) as f:\n        lines = f.readlines()\n\n    assert len(lines) == len(texts)\n\n    import json\n    parsed = [json.loads(line) for line in lines]\n    assert all(\"text\" in doc and \"entities\" in doc for doc in parsed)\n</code></pre>"},{"location":"user-guide/pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Batch processing - process multiple texts at once for efficiency</li> <li>Error handling - wrap inference in try-except for production</li> <li>Input validation - check text length, encoding, emptiness</li> <li>Memory management - clear GPU cache periodically for large datasets</li> <li>Confidence filtering - set minimum thresholds for downstream use</li> <li>JSONL persistence - use for audit trails and reproducibility</li> <li>Monitoring - track processing time, entity counts, confidence distribution</li> </ol>"},{"location":"user-guide/pipeline/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/pipeline/#issue-slow-processing","title":"Issue: Slow processing","text":"<p>Solutions: <pre><code># Use GPU\nconfig = AppConfig(device=\"cuda\")\n\n# Reduce sequence length\nconfig = AppConfig(max_seq_len=128)\n\n# Process in batches\nbatch_size = 32\nresults = simple_inference(texts[:batch_size])\n</code></pre></p>"},{"location":"user-guide/pipeline/#issue-cuda-out-of-memory","title":"Issue: CUDA out of memory","text":"<p>Solutions: <pre><code># Force CPU\nconfig = AppConfig(device=\"cpu\")\n\n# Reduce batch size\nbatch_size = 8\n\n# Clear cache\nimport torch\ntorch.cuda.empty_cache()\n</code></pre></p>"},{"location":"user-guide/pipeline/#issue-low-entity-recall","title":"Issue: Low entity recall","text":"<p>Solutions: <pre><code># Lower fuzzy threshold\nspans = match_symptoms(text, lexicon, fuzzy_threshold=82.0)\n\n# Extend negation window\nconfig = AppConfig(negation_window=7)\n\n# Audit lexicon coverage\nmissing_terms = find_missing_lexicon_terms(gold_annotations, lexicon)\n</code></pre></p>"},{"location":"user-guide/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Weak Labeling Guide - Advanced techniques</li> <li>Negation Guide - Negation patterns</li> <li>Configuration - Tune parameters</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"user-guide/weak-labeling/","title":"Weak Labeling Guide","text":"<p>Comprehensive guide to SpanForge's weak labeling system for biomedical entity recognition.</p>"},{"location":"user-guide/weak-labeling/#overview","title":"Overview","text":"<p>Weak labeling uses lexicon-based fuzzy matching with rule-based filters to automatically annotate symptoms and product mentions without manual labels. Produces high-recall, moderate-precision spans suitable for:</p> <ol> <li>Bootstrapping annotation - seed Label Studio with candidate spans</li> <li>Active learning - prioritize uncertain examples</li> <li>Evaluation baselines - compare supervised models</li> </ol>"},{"location":"user-guide/weak-labeling/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Input Text] --&gt; B[Fuzzy Matching]\n    B --&gt; C[Jaccard Filter]\n    C --&gt; D[Last-Token Alignment]\n    D --&gt; E[Anatomy Filter]\n    E --&gt; F[Negation Detection]\n    F --&gt; G[Confidence Scoring]\n    G --&gt; H[Spans]</code></pre>"},{"location":"user-guide/weak-labeling/#core-components","title":"Core Components","text":""},{"location":"user-guide/weak-labeling/#1-lexicon-matching","title":"1. Lexicon Matching","text":""},{"location":"user-guide/weak-labeling/#fuzzy-matching-wratio","title":"Fuzzy Matching (WRatio)","text":"<p>Uses RapidFuzz WRatio for typo-tolerant matching.</p> <p>Algorithm: <pre><code>1. Tokenize text into n-grams (1-6 tokens)\n2. For each n-gram:\n   a. Compute WRatio against lexicon entries\n   b. If score \u2265 threshold (default: 88.0):\n      - Record match\n</code></pre></p> <p>Example: <pre><code>from src.weak_label import match_symptoms\n\ntext = \"Patient experienced seveer itching\"  # typo: \"seveer\"\nlexicon = [\"severe itching\", \"itching\", \"redness\"]\n\nspans = match_symptoms(text, lexicon, fuzzy_threshold=88.0)\n# Matches \"seveer itching\" \u2192 \"severe itching\" (WRatio: 94.7)\n</code></pre></p> <p>WRatio Characteristics: - Handles typos: \"seveer\" \u2192 \"severe\" - Handles word order: \"itching severe\" \u2192 \"severe itching\" - Handles partial matches: \"severe burning itching\" \u2192 \"severe itching\"</p>"},{"location":"user-guide/weak-labeling/#jaccard-token-set-filter","title":"Jaccard Token-Set Filter","text":"<p>Filters out low-quality fuzzy matches using token overlap.</p> <p>Algorithm: <pre><code>1. Tokenize span and lexicon entry\n2. Compute Jaccard similarity: |A \u2229 B| / |A \u222a B| * 100\n3. Accept if Jaccard \u2265 threshold (default: 40.0)\n</code></pre></p> <p>Example: <pre><code># Good match: high fuzzy + high Jaccard\nspan = \"burning sensation\"\nentry = \"burning\"\n# WRatio: 90.0, Jaccard: 50.0 \u2192 ACCEPT\n\n# Bad match: high fuzzy + low Jaccard (coincidental similarity)\nspan = \"patient history\"\nentry = \"burning\"\n# WRatio: 89.0, Jaccard: 0.0 \u2192 REJECT\n</code></pre></p> <p>Why Jaccard? - Prevents false positives from short common words - Ensures semantic relevance - Complements fuzzy matching</p>"},{"location":"user-guide/weak-labeling/#2-rule-based-filters","title":"2. Rule-Based Filters","text":""},{"location":"user-guide/weak-labeling/#last-token-alignment","title":"Last-Token Alignment","text":"<p>Requires multi-token fuzzy matches to end at token boundaries.</p> <p>Rationale: Prevents partial-word matches.</p> <pre><code># ACCEPT: \"severe itching\" ends at token boundary\ntext = \"Patient has severe itching today\"\nmatch = \"severe itching\"  # \u2713 ends after \"itching\"\n\n# REJECT: \"severe itch\" doesn't end at boundary\ntext = \"Patient has severe itching today\"\nmatch = \"severe itch\"  # \u2717 ends mid-word \"itching\"\n</code></pre> <p>Implementation: <pre><code>def is_last_token_aligned(text, start, end):\n    \"\"\"Check if span ends at token boundary.\"\"\"\n    if end &gt;= len(text):\n        return True\n    next_char = text[end]\n    return next_char in [' ', '.', ',', '!', '?', '\\n']\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#anatomy-filter","title":"Anatomy Filter","text":"<p>Rejects single-token anatomy mentions unless co-occurring with symptom keywords.</p> <p>Rationale: \"skin\" alone is too generic; \"skin redness\" is a symptom.</p> <pre><code>ANATOMY_TOKENS = {\"skin\", \"eye\", \"face\", \"hand\", \"arm\", ...}\nSYMPTOM_KEYWORDS = {\"burning\", \"itching\", \"redness\", \"pain\", ...}\n\n# REJECT: standalone anatomy\ntext = \"Apply to skin twice daily\"\nmatch = \"skin\"  # \u2717 no symptom co-occurrence\n\n# ACCEPT: anatomy + symptom\ntext = \"Patient reported skin burning\"\nmatch = \"skin\"  # \u2713 co-occurs with \"burning\"\n</code></pre> <p>List of Anatomy Tokens: <pre><code>skin, eye, eyes, face, hand, hands, arm, arms, leg, legs, \nfoot, feet, scalp, chest, back, neck, finger, fingers, \ntoe, toes, nail, nails, lip, lips, mouth, tongue, throat, \nstomach, abdomen, head\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#3-negation-detection","title":"3. Negation Detection","text":"<p>Bidirectional negation scope detection with configurable window.</p> <p>Forward Negation (standard): <pre><code>Negation cue \u2192 [window tokens] \u2192 span\n         \"no\"     [history of]    itching\n</code></pre></p> <p>Backward Negation (new): <pre><code>   span   \u2190 [window tokens] \u2190 Negation cue\nitching     [was denied by]       patient\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#negation-tokens","title":"Negation Tokens","text":"<pre><code>NEGATION_TOKENS = {\n    \"no\", \"not\", \"none\", \"never\", \"without\", \"denies\", \n    \"denied\", \"negative\", \"free\", \"absent\", \"rule out\", \n    \"ruled out\", \"r/o\", \"unremarkable\", \"non\", \"free of\",\n    \"absence\", \"absence of\", \"fails to\", \"failed to\"\n}\n</code></pre>"},{"location":"user-guide/weak-labeling/#algorithm","title":"Algorithm","text":"<pre><code>def is_negated(text, span_start, span_end, window=5):\n    \"\"\"\n    Check if span is negated (forward or backward).\n\n    Args:\n        text: Full text\n        span_start: Span start character offset\n        span_end: Span end character offset\n        window: Negation scope in tokens (default: 5)\n\n    Returns:\n        True if negated\n    \"\"\"\n    tokens = text.split()\n    span_tokens = text[span_start:span_end].split()\n\n    # Find span token indices\n    span_token_start = len(text[:span_start].split())\n    span_token_end = span_token_start + len(span_tokens)\n\n    # Forward: negation cue before span\n    forward_start = max(0, span_token_start - window)\n    for i in range(forward_start, span_token_start):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    # Backward: negation cue after span\n    backward_end = min(len(tokens), span_token_end + window)\n    for i in range(span_token_end, backward_end):\n        if tokens[i].lower() in NEGATION_TOKENS:\n            return True\n\n    return False\n</code></pre> <p>Examples:</p> <pre><code># Forward negation\ntext = \"No history of itching or redness\"\n# \"itching\" (tokens 3-3) negated by \"No\" (token 0) [window=5]\nis_negated(text, ..., window=5)  # True\n\n# Backward negation\ntext = \"Itching was denied by patient\"\n# \"Itching\" (token 0) negated by \"denied\" (token 2) [window=5]\nis_negated(text, ..., window=5)  # True\n\n# Out of scope\ntext = \"Patient denies fever but reports itching\"\n# \"itching\" (token 6) NOT negated by \"denies\" (token 1) [window=5, gap=5]\nis_negated(text, ..., window=5)  # False\n</code></pre>"},{"location":"user-guide/weak-labeling/#tuning-negation-window","title":"Tuning Negation Window","text":"Window Precision Recall Use Case 3 High Low Conservative, short sentences 5 Balanced Balanced Default recommendation 7 Medium High Long sentences, complex grammar 10 Low Very High Exploratory, accept over-marking <pre><code>from src.config import AppConfig\n\n# Conservative negation\nconfig = AppConfig(negation_window=3)\n\n# Aggressive negation\nconfig = AppConfig(negation_window=10)\n</code></pre>"},{"location":"user-guide/weak-labeling/#4-confidence-scoring","title":"4. Confidence Scoring","text":"<p>Weighted combination of fuzzy and Jaccard scores.</p> <p>Formula: <pre><code>confidence = 0.8 \u00d7 fuzzy_score + 0.2 \u00d7 jaccard_score\nconfidence = min(confidence, 1.0)  # clamp\n</code></pre></p> <p>Rationale: - Fuzzy score (80% weight) - primary signal - Jaccard score (20% weight) - quality gate - Clamping prevents impossible &gt;1.0 values</p> <p>Example: <pre><code>span = \"burning sensation\"\nlexicon_entry = \"burning\"\n\nfuzzy_score = 90.0  # high similarity\njaccard_score = 50.0  # moderate overlap\n\nconfidence = 0.8 * 90.0 + 0.2 * 50.0\nconfidence = 72.0 + 10.0 = 82.0\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/weak-labeling/#custom-lexicons","title":"Custom Lexicons","text":"<pre><code>from src.weak_label import match_symptoms\n\n# Load custom lexicon\ncustom_lexicon = [\n    \"proprietary syndrome X\",\n    \"brand-specific reaction\",\n    \"custom symptom term\"\n]\n\nspans = match_symptoms(\n    text=\"Patient had brand-specific reaction\",\n    lexicon=custom_lexicon,\n    fuzzy_threshold=88.0\n)\n</code></pre>"},{"location":"user-guide/weak-labeling/#batch-processing","title":"Batch Processing","text":"<pre><code>from src.weak_label import match_symptoms, match_products\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"complaints.csv\")\n\n# Load lexicons\nsymptoms = pd.read_csv(\"data/lexicon/symptoms.csv\")[\"symptom\"].tolist()\nproducts = pd.read_csv(\"data/lexicon/products.csv\")[\"product\"].tolist()\n\n# Process batch\nresults = []\nfor text in df[\"complaint_text\"]:\n    symptom_spans = match_symptoms(text, symptoms)\n    product_spans = match_products(text, products)\n    results.append({\n        \"text\": text,\n        \"symptoms\": symptom_spans,\n        \"products\": product_spans\n    })\n</code></pre>"},{"location":"user-guide/weak-labeling/#confidence-filtering","title":"Confidence Filtering","text":"<pre><code># High-confidence spans only\nspans = match_symptoms(text, lexicon)\nhigh_conf = [s for s in spans if s[\"confidence\"] &gt;= 0.85]\n\n# Low-confidence spans (for review)\nreview_queue = [s for s in spans if 0.65 &lt;= s[\"confidence\"] &lt; 0.85]\n</code></pre>"},{"location":"user-guide/weak-labeling/#negation-aware-filtering","title":"Negation-Aware Filtering","text":"<pre><code># Exclude negated spans\npositive_spans = [s for s in spans if not s.get(\"negated\", False)]\n\n# Negated spans only (for training negation classifier)\nnegated_spans = [s for s in spans if s.get(\"negated\", False)]\n</code></pre>"},{"location":"user-guide/weak-labeling/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/weak-labeling/#cpu-optimization","title":"CPU Optimization","text":"<pre><code># Use exact matching for small lexicons\ndef exact_match(text, lexicon):\n    \"\"\"Exact substring matching (fast).\"\"\"\n    spans = []\n    for term in lexicon:\n        start = 0\n        while True:\n            idx = text.lower().find(term.lower(), start)\n            if idx == -1:\n                break\n            spans.append({\n                \"text\": text[idx:idx+len(term)],\n                \"start\": idx,\n                \"end\": idx + len(term),\n                \"label\": \"SYMPTOM\",\n                \"confidence\": 1.0,\n                \"canonical\": term\n            })\n            start = idx + 1\n    return spans\n</code></pre>"},{"location":"user-guide/weak-labeling/#threshold-tuning","title":"Threshold Tuning","text":"<pre><code># Higher thresholds = higher precision, lower recall\nstrict_spans = match_symptoms(text, lexicon, fuzzy_threshold=92.0)\n\n# Lower thresholds = higher recall, lower precision\nlenient_spans = match_symptoms(text, lexicon, fuzzy_threshold=82.0)\n</code></pre>"},{"location":"user-guide/weak-labeling/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/weak-labeling/#precisionrecall","title":"Precision/Recall","text":"<pre><code>def evaluate_weak_labels(gold_annotations, predicted_spans):\n    \"\"\"\n    Evaluate weak labeling performance.\n\n    Args:\n        gold_annotations: List of gold spans\n        predicted_spans: List of predicted spans\n\n    Returns:\n        Dict with precision, recall, F1\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for pred in predicted_spans:\n        matched = False\n        for gold in gold_annotations:\n            # IOU overlap \u2265 0.5\n            overlap = compute_iou(pred, gold)\n            if overlap &gt;= 0.5 and pred[\"label\"] == gold[\"label\"]:\n                true_positives += 1\n                matched = True\n                break\n        if not matched:\n            false_positives += 1\n\n    false_negatives = len(gold_annotations) - true_positives\n\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n</code></pre>"},{"location":"user-guide/weak-labeling/#confidence-calibration","title":"Confidence Calibration","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Plot confidence distribution\nconfidences = [s[\"confidence\"] for s in spans]\nplt.hist(confidences, bins=20, range=(0, 100))\nplt.xlabel(\"Confidence Score\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Weak Label Confidence Distribution\")\nplt.show()\n</code></pre>"},{"location":"user-guide/weak-labeling/#best-practices","title":"Best Practices","text":"<ol> <li>Start with high thresholds (fuzzy=90, Jaccard=45) - prioritize precision</li> <li>Tune on evaluation set - measure P/R/F1 on gold annotations</li> <li>Use bidirectional negation - captures more negation patterns</li> <li>Filter anatomy singletons - reduces false positives</li> <li>Require last-token alignment - prevents partial-word matches</li> <li>Cache lexicon lookups - speeds up batch processing</li> <li>Version lexicons - track changes for reproducibility</li> </ol>"},{"location":"user-guide/weak-labeling/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"user-guide/weak-labeling/#issue-high-false-positive-rate","title":"Issue: High false positive rate","text":"<p>Causes: - Fuzzy threshold too low - Missing anatomy filter - Lexicon contains generic terms</p> <p>Solutions: <pre><code># Increase thresholds\nspans = match_symptoms(text, lexicon, fuzzy_threshold=92.0)\n\n# Add anatomy filter (already default)\n\n# Audit lexicon for generic terms\ngeneric_terms = [\"skin\", \"patient\", \"product\"]  # remove these\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#issue-missing-multi-word-symptoms","title":"Issue: Missing multi-word symptoms","text":"<p>Causes: - Lexicon only has single-word entries - Last-token alignment too strict</p> <p>Solutions: <pre><code># Add multi-word entries to lexicon\nlexicon = [\n    \"burning sensation\",  # not just \"burning\"\n    \"severe itching\",     # not just \"itching\"\n    \"dry skin\"            # not just \"dry\"\n]\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#issue-over-aggressive-negation","title":"Issue: Over-aggressive negation","text":"<p>Causes: - Negation window too large - Negation token list too broad</p> <p>Solutions: <pre><code># Reduce window\nconfig = AppConfig(negation_window=3)\n\n# Audit NEGATION_TOKENS for overly broad terms\n</code></pre></p>"},{"location":"user-guide/weak-labeling/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Tune thresholds</li> <li>Negation Guide - Advanced negation patterns</li> <li>API Reference - Full function documentation</li> </ul>"}]}