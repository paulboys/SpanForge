{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43677483",
   "metadata": {},
   "source": [
    "# Adverse Event NER: Complete Annotation Workflow\n",
    "\n",
    "This notebook walks you through the **entire annotation pipeline** from raw consumer complaints to Label Studio-ready tasks and quality-checked gold standard data.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup & Text Ingestion** - Load sample complaint texts\n",
    "2. **Weak Labeling** - Auto-generate initial symptom/product spans using heuristics\n",
    "3. **Export for Annotation** - Convert weak labels to Label Studio task format\n",
    "4. **Mock Human Curation** - Simulate annotator corrections (Label Studio would be used in production)\n",
    "5. **Gold Conversion** - Transform annotated data to normalized training format\n",
    "6. **Quality Metrics** - Compute annotator agreement, label distribution, conflicts\n",
    "7. **Comparison Analysis** - Evaluate weak labeling precision/recall against gold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b34e25",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Text Ingestion\n",
    "\n",
    "First, configure Python path and load our NER modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec5ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Project root: c:\\Users\\User\\Documents\\NER\n",
      "‚úì Working directory: c:\\Users\\User\\Documents\\NER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniforge3\\envs\\NER\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"scripts\" else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Import NER modules\n",
    "from src.weak_label import load_symptom_lexicon, load_product_lexicon, weak_label_batch\n",
    "from src.pipeline import simple_inference\n",
    "\n",
    "print(\"‚úì Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90330361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 5 sample complaints\n",
      "\n",
      "1. I got a severe rash and headache from the hydra boost cream\n",
      "2. No irritation from the face wash, just mild dryness around lips\n",
      "3. The moisturizer caused redness and itching on my skin\n",
      "4. I experienced nausea after using the vitamin serum\n",
      "5. The exfoliating scrub caused stinging and left me feeling dry\n"
     ]
    }
   ],
   "source": [
    "# Sample complaint texts (realistic consumer reports with symptoms & products)\n",
    "complaints = [\n",
    "    \"I got a severe rash and headache from the hydra boost cream\",\n",
    "    \"No irritation from the face wash, just mild dryness around lips\",\n",
    "    \"The moisturizer caused redness and itching on my skin\",\n",
    "    \"I experienced nausea after using the vitamin serum\",\n",
    "    \"The exfoliating scrub caused stinging and left me feeling dry\"\n",
    "]\n",
    "\n",
    "print(f\"‚úì Loaded {len(complaints)} sample complaints\\n\")\n",
    "for i, text in enumerate(complaints, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bc259",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Weak Labeling (Automated Span Detection)\n",
    "\n",
    "Use lexicon-based heuristics to automatically detect symptom and product mentions. This generates our initial \"weak\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95d10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 161 symptom terms\n",
      "‚úì Loaded 3 product terms\n",
      "\n",
      "üìä Weak Labeling Results:\n",
      "\n",
      "Complaint 1: \"I got a severe rash and headache from the hydra boost cream...\"\n",
      "  ‚Üí Found 3 spans\n",
      "    ‚Ä¢ SYMPTOM  | \"severe\" ‚Üí Headache (conf: 0.82)\n",
      "    ‚Ä¢ SYMPTOM  | \"rash\" ‚Üí Rash\n",
      "    ‚Ä¢ SYMPTOM  | \"headache\" ‚Üí Headache\n",
      "\n",
      "Complaint 2: \"No irritation from the face wash, just mild dryness around l...\"\n",
      "  ‚Üí Found 2 spans\n",
      "    ‚Ä¢ SYMPTOM  | \"mild\" ‚Üí Headache (conf: 0.82)\n",
      "    ‚Ä¢ PRODUCT  | \"face wash\" ‚Üí Gentle Daily Cleanser\n",
      "\n",
      "Complaint 3: \"The moisturizer caused redness and itching on my skin...\"\n",
      "  ‚Üí Found 2 spans\n",
      "    ‚Ä¢ SYMPTOM  | \"redness\" ‚Üí Erythema\n",
      "    ‚Ä¢ SYMPTOM  | \"itching\" ‚Üí Pruritus\n",
      "\n",
      "Complaint 4: \"I experienced nausea after using the vitamin serum...\"\n",
      "  ‚Üí Found 2 spans\n",
      "    ‚Ä¢ SYMPTOM  | \"nausea\" ‚Üí Nausea\n",
      "    ‚Ä¢ PRODUCT  | \"serum\" ‚Üí Radiance Vitamin C Serum\n",
      "\n",
      "Complaint 5: \"The exfoliating scrub caused stinging and left me feeling dr...\"\n",
      "  ‚Üí Found 1 spans\n",
      "    ‚Ä¢ SYMPTOM  | \"feeling\" ‚Üí Nausea (conf: 0.82)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load lexicons\n",
    "symptom_lex = load_symptom_lexicon(Path(\"data/lexicon/symptoms.csv\"))\n",
    "product_lex = load_product_lexicon(Path(\"data/lexicon/products.csv\"))\n",
    "\n",
    "print(f\"‚úì Loaded {len(symptom_lex)} symptom terms\")\n",
    "print(f\"‚úì Loaded {len(product_lex)} product terms\")\n",
    "\n",
    "# Run weak labeling\n",
    "spans_batch = weak_label_batch(complaints, symptom_lex, product_lex, negation_window=5, scorer=\"wratio\")\n",
    "\n",
    "print(f\"\\nüìä Weak Labeling Results:\\n\")\n",
    "for i, (text, spans) in enumerate(zip(complaints, spans_batch), 1):\n",
    "    print(f\"Complaint {i}: \\\"{text[:60]}...\\\"\")\n",
    "    print(f\"  ‚Üí Found {len(spans)} spans\")\n",
    "    for span in spans:\n",
    "        neg_flag = \" [NEGATED]\" if span.negated else \"\"\n",
    "        conf_str = f\" (conf: {span.confidence:.2f})\" if span.confidence < 1.0 else \"\"\n",
    "        print(f\"    ‚Ä¢ {span.label:8} | \\\"{span.text}\\\" ‚Üí {span.canonical}{conf_str}{neg_flag}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799ac56",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Export Weak Labels to JSONL\n",
    "\n",
    "Persist the weak labels in a structured format for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc641b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 5 weak label records to:\n",
      "  data\\output\\workflow_demo_weak.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Prepare weak label JSONL\n",
    "weak_jsonl_path = Path(\"data/output/workflow_demo_weak.jsonl\")\n",
    "weak_jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "weak_records = []\n",
    "for idx, (text, spans) in enumerate(zip(complaints, spans_batch)):\n",
    "    record = {\n",
    "        \"id\": idx,\n",
    "        \"text\": text,\n",
    "        \"weak_spans\": [\n",
    "            {\n",
    "                \"start\": s.start,\n",
    "                \"end\": s.end,\n",
    "                \"label\": s.label,\n",
    "                \"text\": s.text,\n",
    "                \"canonical\": s.canonical,\n",
    "                \"confidence\": s.confidence,\n",
    "                \"negated\": s.negated,\n",
    "            }\n",
    "            for s in spans\n",
    "        ]\n",
    "    }\n",
    "    weak_records.append(record)\n",
    "\n",
    "# Write JSONL\n",
    "with weak_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in weak_records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Saved {len(weak_records)} weak label records to:\")\n",
    "print(f\"  {weak_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcecba0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Convert to Label Studio Task Format\n",
    "\n",
    "Transform weak labels into the JSON format Label Studio expects for import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350ad9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 5 Label Studio tasks\n",
      "  Saved to: data\\annotation\\exports\\workflow_demo_tasks.json\n",
      "\n",
      "üí° In production, you would:\n",
      "   1. Start Label Studio: label-studio start\n",
      "   2. Create project with config from: data/annotation/config/label_config.xml\n",
      "   3. Import tasks from: data\\annotation\\exports\\workflow_demo_tasks.json\n",
      "   4. Annotators correct/refine spans in UI\n",
      "   5. Export completed annotations\n"
     ]
    }
   ],
   "source": [
    "# Convert weak labels to Label Studio tasks (with optional pre-annotations)\n",
    "ls_tasks = []\n",
    "for rec in weak_records:\n",
    "    task = {\"data\": {\"text\": rec[\"text\"]}}\n",
    "    \n",
    "    # Optional: include weak spans as \"predictions\" (can bias annotators; use cautiously)\n",
    "    # Uncomment to include pre-annotations:\n",
    "    # task[\"predictions\"] = [{\n",
    "    #     \"model_version\": \"weak_v1\",\n",
    "    #     \"result\": [\n",
    "    #         {\n",
    "    #             \"value\": {\n",
    "    #                 \"start\": s[\"start\"],\n",
    "    #                 \"end\": s[\"end\"],\n",
    "    #                 \"text\": s[\"text\"],\n",
    "    #                 \"labels\": [s[\"label\"]],\n",
    "    #             },\n",
    "    #             \"from_name\": \"label\",\n",
    "    #             \"to_name\": \"text\",\n",
    "    #             \"type\": \"labels\",\n",
    "    #         }\n",
    "    #         for s in rec[\"weak_spans\"]\n",
    "    #     ]\n",
    "    # }]\n",
    "    \n",
    "    ls_tasks.append(task)\n",
    "\n",
    "# Save tasks JSON\n",
    "ls_tasks_path = Path(\"data/annotation/exports/workflow_demo_tasks.json\")\n",
    "ls_tasks_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ls_tasks_path.write_text(json.dumps(ls_tasks, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úì Created {len(ls_tasks)} Label Studio tasks\")\n",
    "print(f\"  Saved to: {ls_tasks_path}\")\n",
    "print(f\"\\nüí° In production, you would:\")\n",
    "print(f\"   1. Start Label Studio: label-studio start\")\n",
    "print(f\"   2. Create project with config from: data/annotation/config/label_config.xml\")\n",
    "print(f\"   3. Import tasks from: {ls_tasks_path}\")\n",
    "print(f\"   4. Annotators correct/refine spans in UI\")\n",
    "print(f\"   5. Export completed annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d3fe4",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Mock Human Curation (Simulated Label Studio Export)\n",
    "\n",
    "Since we don't have Label Studio running, we'll simulate a human annotator correcting the weak labels. In production, this would be the JSON export from Label Studio after annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83129af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created mock Label Studio export with 5 annotated tasks\n",
      "  Saved to: data\\annotation\\raw\\workflow_demo_ls_export.json\n",
      "\n",
      "üìù Summary of 'human' corrections:\n",
      "   Task 1: 3 entities annotated\n",
      "   Task 2: 3 entities annotated\n",
      "   Task 3: 3 entities annotated\n",
      "   Task 4: 2 entities annotated\n",
      "   Task 5: 3 entities annotated\n"
     ]
    }
   ],
   "source": [
    "# Simulate Label Studio export (normally exported from Label Studio UI)\n",
    "# This mimics what a human annotator would produce after reviewing weak labels\n",
    "\n",
    "mock_ls_export = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"data\": {\"text\": complaints[0]},\n",
    "        \"annotations\": [{\n",
    "            \"result\": [\n",
    "                {\"value\": {\"start\": 8, \"end\": 19, \"text\": \"severe rash\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 24, \"end\": 32, \"text\": \"headache\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 42, \"end\": 59, \"text\": \"hydra boost cream\", \"labels\": [\"PRODUCT\"]}}\n",
    "            ]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"data\": {\"text\": complaints[1]},\n",
    "        \"annotations\": [{\n",
    "            \"result\": [\n",
    "                {\"value\": {\"start\": 3, \"end\": 13, \"text\": \"irritation\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 23, \"end\": 32, \"text\": \"face wash\", \"labels\": [\"PRODUCT\"]}},\n",
    "                {\"value\": {\"start\": 44, \"end\": 51, \"text\": \"dryness\", \"labels\": [\"SYMPTOM\"]}}\n",
    "            ]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"data\": {\"text\": complaints[2]},\n",
    "        \"annotations\": [{\n",
    "            \"result\": [\n",
    "                {\"value\": {\"start\": 4, \"end\": 15, \"text\": \"moisturizer\", \"labels\": [\"PRODUCT\"]}},\n",
    "                {\"value\": {\"start\": 23, \"end\": 30, \"text\": \"redness\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 35, \"end\": 42, \"text\": \"itching\", \"labels\": [\"SYMPTOM\"]}}\n",
    "            ]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"data\": {\"text\": complaints[3]},\n",
    "        \"annotations\": [{\n",
    "            \"result\": [\n",
    "                {\"value\": {\"start\": 14, \"end\": 20, \"text\": \"nausea\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 37, \"end\": 50, \"text\": \"vitamin serum\", \"labels\": [\"PRODUCT\"]}}\n",
    "            ]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"data\": {\"text\": complaints[4]},\n",
    "        \"annotations\": [{\n",
    "            \"result\": [\n",
    "                {\"value\": {\"start\": 4, \"end\": 21, \"text\": \"exfoliating scrub\", \"labels\": [\"PRODUCT\"]}},\n",
    "                {\"value\": {\"start\": 29, \"end\": 37, \"text\": \"stinging\", \"labels\": [\"SYMPTOM\"]}},\n",
    "                {\"value\": {\"start\": 63, \"end\": 66, \"text\": \"dry\", \"labels\": [\"SYMPTOM\"]}}\n",
    "            ]\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save mock export\n",
    "mock_export_path = Path(\"data/annotation/raw/workflow_demo_ls_export.json\")\n",
    "mock_export_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "mock_export_path.write_text(json.dumps(mock_ls_export, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úì Created mock Label Studio export with {len(mock_ls_export)} annotated tasks\")\n",
    "print(f\"  Saved to: {mock_export_path}\")\n",
    "print(f\"\\nüìù Summary of 'human' corrections:\")\n",
    "for task in mock_ls_export:\n",
    "    n_spans = len(task[\"annotations\"][0][\"result\"])\n",
    "    print(f\"   Task {task['id']}: {n_spans} entities annotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867598c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Convert to Gold Standard Format (with Provenance)\n",
    "\n",
    "Transform the Label Studio export into our normalized gold JSONL format with provenance tracking and canonical mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c1d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5 tasks to data\\annotation\\exports\\workflow_demo_gold.jsonl (source=workflow_demo_batch, annotator=demo_user)\n",
      "\n",
      "\n",
      "üìä Gold Standard Summary:\n",
      "  Total records: 5\n",
      "\n",
      "Sample gold record (task 1):\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"I got a severe rash and headache from the hydra boost cream\",\n",
      "  \"source\": \"workflow_demo_batch\",\n",
      "  \"annotator\": \"demo_user\",\n",
      "  \"revision\": 1,\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"start\": 8,\n",
      "      \"end\": 19,\n",
      "      \"label\": \"SYMPTOM\",\n",
      "      \"text\": \"severe rash\",\n",
      "      \"canonical\": \"severe rash\",\n",
      "      \"concept_id\": \"SYMPTOM:severe_rash\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 24,\n",
      "      \"end\": 32,\n",
      "      \"label\": \"SYMPTOM\",\n",
      "      \"text\": \"headache\",\n",
      "      \"canonical\": \"headache\",\n",
      "      \"concept_id\": \"SYMPTOM:headache\"\n",
      "    },\n",
      "    {\n",
      "      \"start\": 42,\n",
      "      \"end\": 59,\n",
      "      \"label\": \"PRODUCT\",\n",
      "      \"text\": \"hydra boost cream\",\n",
      "      \"canonical\": \"hydra boost cream\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the conversion script programmatically\n",
    "import subprocess\n",
    "\n",
    "conversion_cmd = [\n",
    "    \"python\", \"scripts/annotation/convert_labelstudio.py\",\n",
    "    \"--input\", str(mock_export_path),\n",
    "    \"--output\", \"data/annotation/exports/workflow_demo_gold.jsonl\",\n",
    "    \"--source\", \"workflow_demo_batch\",\n",
    "    \"--annotator\", \"demo_user\",\n",
    "    \"--revision\", \"1\",\n",
    "    \"--symptom-lexicon\", \"data/lexicon/symptoms.csv\",\n",
    "    \"--product-lexicon\", \"data/lexicon/products.csv\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(conversion_cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(\"Error:\", result.stderr)\n",
    "\n",
    "# Load and display the gold standard\n",
    "gold_path = Path(\"data/annotation/exports/workflow_demo_gold.jsonl\")\n",
    "gold_records = []\n",
    "with gold_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            gold_records.append(json.loads(line))\n",
    "\n",
    "print(f\"\\nüìä Gold Standard Summary:\")\n",
    "print(f\"  Total records: {len(gold_records)}\")\n",
    "print(f\"\\nSample gold record (task 1):\")\n",
    "print(json.dumps(gold_records[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5d9cd",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Quality Metrics & Annotator Agreement\n",
    "\n",
    "Analyze the gold standard data to compute quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91dc4465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality report written to data\\annotation\\reports\\workflow_demo_quality.json\n",
      "\n",
      "\n",
      "üìà Quality Report:\n",
      "\n",
      "Total Tasks: 5\n",
      "Mean Spans per Task: 2.8\n",
      "\n",
      "Label Distribution:\n",
      "  SYMPTOM: 9\n",
      "  PRODUCT: 5\n",
      "\n",
      "Conflicts (overlapping spans with different labels): 0\n",
      "\n",
      "Annotator Span Counts:\n",
      "  demo_user: 14\n"
     ]
    }
   ],
   "source": [
    "# Run quality report\n",
    "quality_cmd = [\n",
    "    \"python\", \"scripts/annotation/quality_report.py\",\n",
    "    \"--gold\", \"data/annotation/exports/workflow_demo_gold.jsonl\",\n",
    "    \"--out\", \"data/annotation/reports/workflow_demo_quality.json\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(quality_cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Load and display quality report\n",
    "quality_path = Path(\"data/annotation/reports/workflow_demo_quality.json\")\n",
    "quality_report = json.loads(quality_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(f\"\\nüìà Quality Report:\\n\")\n",
    "print(f\"Total Tasks: {quality_report['n_tasks']}\")\n",
    "print(f\"Mean Spans per Task: {quality_report['mean_spans_per_task']:.1f}\")\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "for label, count in quality_report['label_counts'].items():\n",
    "    print(f\"  {label}: {count}\")\n",
    "print(f\"\\nConflicts (overlapping spans with different labels): {quality_report['conflicts']}\")\n",
    "print(f\"\\nAnnotator Span Counts:\")\n",
    "for annotator, count in quality_report['annotator_counts'].items():\n",
    "    print(f\"  {annotator}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ec844",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Compare Weak vs Gold (Precision/Recall Analysis)\n",
    "\n",
    "Evaluate how well our automated weak labeling performed compared to human-curated gold standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison script\n",
    "comparison_cmd = [\n",
    "    \"python\", \"scripts/annotation/compare_weak_vs_gold.py\",\n",
    "    \"--weak\", \"data/output/workflow_demo_weak.jsonl\",\n",
    "    \"--gold\", \"data/annotation/exports/workflow_demo_gold.jsonl\",\n",
    "    \"--output\", \"data/annotation/exports/workflow_demo_comparison.txt\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(comparison_cmd, capture_output=True, text=True, env={**os.environ, \"PYTHONPATH\": str(project_root)})\n",
    "print(result.stdout)\n",
    "\n",
    "# Load and display comparison report\n",
    "comparison_path = Path(\"data/annotation/exports/workflow_demo_comparison.txt\")\n",
    "comparison_report = json.loads(comparison_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(f\"\\nüîç Weak vs Gold Comparison:\\n\")\n",
    "print(f\"Overall Metrics:\")\n",
    "print(f\"  Precision: {comparison_report['overall']['precision']:.1%}\")\n",
    "print(f\"  Recall: {comparison_report['overall']['recall']:.1%}\")\n",
    "print(f\"  F1 Score: {comparison_report['overall']['f1']:.1%}\")\n",
    "print(f\"  True Positives: {comparison_report['overall']['tp']}\")\n",
    "print(f\"  False Positives: {comparison_report['overall']['fp']}\")\n",
    "print(f\"  False Negatives: {comparison_report['overall']['fn']}\")\n",
    "\n",
    "print(f\"\\nPer-Label Breakdown:\")\n",
    "for label, metrics in comparison_report['labels'].items():\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.1%}\")\n",
    "    print(f\"    Recall: {metrics['recall']:.1%}\")\n",
    "    print(f\"    F1: {metrics['f1']:.1%}\")\n",
    "\n",
    "print(f\"\\nüí° Suggestions:\")\n",
    "for suggestion in comparison_report.get('suggestions', []):\n",
    "    print(f\"  ‚Ä¢ {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dfe59",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Register Batch (Provenance Tracking)\n",
    "\n",
    "Record this annotation batch in the registry for audit trail and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb745fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered batch workflow_demo_batch (5 tasks) -> data\\annotation\\registry.csv\n",
      "\n",
      "\n",
      "üìã Provenance Registry:\n",
      "timestamp,batch_id,gold_file,n_tasks,annotators,revision,notes\n",
      "2025-11-16T10:16:28,workflow_demo_batch,data\\annotation\\exports\\workflow_demo_gold.jsonl,5,demo_user,1,Interactive notebook demo of complete annotation workflow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the batch in provenance registry\n",
    "register_cmd = [\n",
    "    \"python\", \"scripts/annotation/register_batch.py\",\n",
    "    \"--batch-id\", \"workflow_demo_batch\",\n",
    "    \"--gold\", \"data/annotation/exports/workflow_demo_gold.jsonl\",\n",
    "    \"--annotators\", \"demo_user\",\n",
    "    \"--revision\", \"1\",\n",
    "    \"--notes\", \"Interactive notebook demo of complete annotation workflow\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(register_cmd, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Display registry\n",
    "registry_path = Path(\"data/annotation/registry.csv\")\n",
    "if registry_path.exists():\n",
    "    print(f\"\\nüìã Provenance Registry:\")\n",
    "    print(registry_path.read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f1721",
   "metadata": {},
   "source": [
    "## üéØ Summary & Next Steps\n",
    "\n",
    "You've just walked through the complete annotation pipeline! Here's what happened:\n",
    "\n",
    "1. ‚úÖ **Raw Text** ‚Üí Started with 5 consumer complaints\n",
    "2. ‚úÖ **Weak Labeling** ‚Üí Auto-detected symptoms/products using lexicons\n",
    "3. ‚úÖ **Export** ‚Üí Persisted weak labels to JSONL\n",
    "4. ‚úÖ **Label Studio Format** ‚Üí Converted to task import format\n",
    "5. ‚úÖ **Human Curation** ‚Üí (Simulated) annotator corrections\n",
    "6. ‚úÖ **Gold Standard** ‚Üí Normalized format with provenance & canonical mapping\n",
    "7. ‚úÖ **Quality Metrics** ‚Üí Analyzed label distribution, conflicts, annotator stats\n",
    "8. ‚úÖ **Precision/Recall** ‚Üí Compared weak vs gold performance\n",
    "9. ‚úÖ **Registry** ‚Üí Tracked batch in provenance audit trail\n",
    "\n",
    "### Real-World Production Workflow\n",
    "\n",
    "```bash\n",
    "# 1. Start Label Studio\n",
    "label-studio start --no-browser\n",
    "\n",
    "# 2. Next: obtain API key from Label Studio UI (Account Settings after activating legacy token in Organiation tab)\n",
    "curl.exe -X POST http://localhost:8080/api/projects -H \"Authorization: Token <REPLACE_WITH_YOUR_TOKEN>\" -H \"Content-Type: application/json\" --data \"@data/annotation/config/project_bootstrap.json\"\n",
    "\n",
    "# 3. Bootstrap project\n",
    "python scripts/annotation/init_label_studio_project.py --name \"Adverse Event NER\"\n",
    "\n",
    "# 4. Import weak labels (optional pre-annotations)\n",
    "python scripts/annotation/cli.py import-weak `\n",
    "    --weak data/output/notebook_test.jsonl `\n",
    "    --out data/annotation/exports/tasks.json `\n",
    "    --push --project-id 1\n",
    "\n",
    "# 5. Annotators work in Label Studio UI\n",
    "\n",
    "# 6. Export from Label Studio ‚Üí data/annotation/raw/export.json\n",
    "\n",
    "# 7. Convert to gold with provenance\n",
    "python scripts/annotation/convert_labelstudio.py \\\n",
    "    --input data/annotation/raw/export.json \\\n",
    "    --output data/annotation/exports/gold.jsonl \\\n",
    "    --source batch_2025Q4 \\\n",
    "    --annotator alice \\\n",
    "    --symptom-lexicon data/lexicon/symptoms.csv \\\n",
    "    --product-lexicon data/lexicon/products.csv\n",
    "\n",
    "# 8. Quality check\n",
    "python scripts/annotation/cli.py quality \\\n",
    "    --gold data/annotation/exports/gold.jsonl \\\n",
    "    --out data/annotation/reports/quality.json\n",
    "\n",
    "# 9. Register batch\n",
    "python scripts/annotation/cli.py register \\\n",
    "    --batch-id batch_2025Q4 \\\n",
    "    --gold data/annotation/exports/gold.jsonl \\\n",
    "    --annotators alice,bob \\\n",
    "    --revision 1\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Train/Dev/Test Splits**: Partition gold data for model training\n",
    "- **Token Classification**: Fine-tune BioBERT with BIO tags from gold spans\n",
    "- **Active Learning**: Use model predictions to prioritize next annotation batch\n",
    "- **Threshold Tuning**: Adjust fuzzy/Jaccard cutoffs based on comparison metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
